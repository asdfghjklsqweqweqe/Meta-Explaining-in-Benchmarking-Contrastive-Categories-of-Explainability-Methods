{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPfjYD0r5YiKIQfvXRyBcGR"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import torch\n","import statistics\n","from statistics import mean\n","import csv\n","from sklearn import metrics\n","from copy import deepcopy\n","import numpy as np\n","from torch_geometric.nn import MessagePassing\n","from torch.nn.parameter import Parameter"],"metadata":{"id":"H_hSPIVb2Cm3"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lXDwLWJGTj77"},"outputs":[],"source":["class evalaution_of_xmethods_fidelity_plus(object):\n","    def __init__(self, a_trained_model, test_data):\n","        super(evalaution_of_xmethods_fidelity_plus, self).__init__()\n","        self.a_trained_model = a_trained_model\n","        self.test_data = test_data\n","\n","    def is_salient(self, score, importance_threshold):\n","        if importance_threshold == score == 0:\n","            return True\n","        if importance_threshold == score == 1:\n","            return False\n","        if importance_threshold < score:\n","            return True\n","        else:\n","            return False\n","\n","    def Compute_ROC_AUC(self, your_model, your_dataset):\n","        preds = []\n","        reals = []\n","\n","        your_model.eval()\n","        for batched_data in your_dataset:\n","            if your_model.__class__.__name__ == \"GCN_plus_GAP\":\n","                Output_of_Hidden_Layers, pooling_layer_output, ffn_output, prediction = your_model(batched_data)\n","            elif your_model.__class__.__name__ == \"DGCNN_Model\":\n","                final_GNN_layer_output, sortpooled_embedings, output_conv1d_1, maxpooled_output_conv1d_1, output_conv1d_2, to_dense, output_h1, dropout_output_h1, output_h2, prediction = your_model(batched_data, None)\n","            elif your_model.__class__.__name__ == \"DIFFPOOL_Model\":\n","                concatination_list_of_poolings, prediction_output_not_softed, prediction = your_model(batched_data, None)\n","            elif your_model.__class__.__name__ == \"GIN_Model\":\n","                mlps_output_embeds, mlp_outputs_globalSUMpooled, lin1_output, lin1_output_dropouted, lin2_output, prediction = your_model(batched_data, None)\n","            else:\n","                print(\"Model Name is not valid.\")\n","\n","            # preds.extend(torch.argmax(prediction, dim=1).tolist())\n","            preds.extend(prediction.tolist())\n","            reals.extend(batched_data.y.tolist())\n","\n","        is_binary = len(np.unique(reals)) == 2\n","        if is_binary:\n","            preds = (np.array(preds)[:, 1] > 0.5).astype(int)\n","\n","        auc_roc = metrics.roc_auc_score(np.array(reals), np.array(preds), multi_class=\"ovr\", average=\"micro\")\n","\n","        return auc_roc\n","\n","    def drop_important_node_features(self, your_dataset, importance_threshold, attribution_scores):\n","        occluded_GNNgraph_list = []\n","        for i in range(len(attribution_scores)):\n","            sample_graph = deepcopy(your_dataset[i])\n","            for j in range(len(attribution_scores[i])):\n","                for k in range(len(attribution_scores[i][j])):\n","                    if self.is_salient(attribution_scores[i][j][k], importance_threshold):\n","                        sample_graph.x[j][k] = 0\n","            occluded_GNNgraph_list.append(sample_graph)\n","        return occluded_GNNgraph_list\n","\n","    def drop_important_nodes(self, your_dataset, importance_threshold, attribution_scores):\n","        occluded_GNNgraph_list = []\n","        for i in range(len(attribution_scores)):\n","            sample_graph = deepcopy(your_dataset[i])\n","            for j in range(len(attribution_scores[i])):\n","                if self.is_salient(attribution_scores[i][j], importance_threshold):\n","                    sample_graph.x[j][:] = 0\n","            occluded_GNNgraph_list.append(sample_graph)\n","        return occluded_GNNgraph_list\n","\n","    def Fidelity_node_features(self, your_dataset, generated_saliency_maps, importance_threshold):\n","\n","        auc_roc_before_droping_important_nodes = self.Compute_ROC_AUC(self.a_trained_model, your_dataset)\n","        new_graph_dataset = self.drop_important_node_features(your_dataset, importance_threshold, generated_saliency_maps)\n","        auc_roc_after_droping_important_nodes = self.Compute_ROC_AUC(self.a_trained_model, new_graph_dataset)\n","\n","        return auc_roc_before_droping_important_nodes - auc_roc_after_droping_important_nodes\n","\n","    def Fidelity_node(self, your_dataset, generated_saliency_maps, importance_threshold):\n","\n","        auc_roc_before_droping_important_nodes = self.Compute_ROC_AUC(self.a_trained_model, your_dataset)\n","        new_graph_dataset = self.drop_important_nodes(your_dataset, importance_threshold, generated_saliency_maps)\n","        auc_roc_after_droping_important_nodes = self.Compute_ROC_AUC(self.a_trained_model, new_graph_dataset)\n","\n","        return auc_roc_before_droping_important_nodes - auc_roc_after_droping_important_nodes\n","\n","    def drop_important_edges(self, your_dataset, importance_threshold, Edge_Masks_Dropped):\n","        attribution_scores = []\n","        Edge_Masks_original = []\n","        Edge_Masks_Dropped_copy = []\n","        for tensor in Edge_Masks_Dropped:\n","            Edge_Masks_original.append(deepcopy(tensor.detach()))\n","            Edge_Masks_Dropped_copy.append(deepcopy(tensor.detach()))\n","\n","        for edge_mask in Edge_Masks_Dropped_copy:\n","\n","            importance_indices = edge_mask > importance_threshold * (max(edge_mask)-min(edge_mask))\n","            edge_mask[importance_indices] = 0\n","\n","        return Edge_Masks_original, Edge_Masks_Dropped_copy\n","\n","    def Fidelity_edge(self, your_dataset, generated_saliency_maps, importance_threshold):\n","\n","        auc_roc_before_droping_important_nodes = self.Compute_ROC_AUC_edge(self.a_trained_model, your_dataset, False)\n","        Edge_Masks_original, Edge_Masks_Dropped = self.drop_important_edges(your_dataset, importance_threshold, generated_saliency_maps)\n","        auc_roc_after_droping_important_nodes = self.Compute_ROC_AUC_edge(self.a_trained_model, your_dataset, Edge_Masks_Dropped)\n","\n","        return auc_roc_before_droping_important_nodes - auc_roc_after_droping_important_nodes\n","\n","    def normalize_saliency_node_features_based(self, sal_maps):\n","        Graphs_new_gradients = []\n","        for graph_grads in sal_maps:\n","            new_gradients = []\n","            for node_grads in graph_grads:\n","                node_gradients = []\n","                for dim in node_grads:\n","                    node_gradients.append((dim-min(node_grads))/(max(node_grads)-min(node_grads)))\n","                new_gradients.append(node_gradients)\n","            Graphs_new_gradients.append(new_gradients)\n","        return Graphs_new_gradients\n","\n","    def normalize_saliency_node_based(self, sal_maps):\n","        Graphs_new_gradients = []\n","        for graph_grads in sal_maps:\n","            new_gradients = []\n","            for node_grads in graph_grads:\n","                val = (node_grads-min(graph_grads))/(max(graph_grads)-min(graph_grads)) if (max(graph_grads)-min(graph_grads)) != 0 else 0\n","                new_gradients.append(val)\n","            Graphs_new_gradients.append(new_gradients)\n","        return Graphs_new_gradients\n","\n","    def clear_masks(self, model):\n","\n","        for module in model.modules():\n","            if isinstance(module, MessagePassing):\n","                module.explain = False\n","                module._edge_mask = None\n","                module._loop_mask = None\n","                module._apply_sigmoid = True\n","        return module\n","\n","    def apply_masks(self, model, mask, edge_index, apply_sigmoid):\n","        loop_mask = edge_index[0] != edge_index[1]\n","\n","        for module in model.modules():\n","            if isinstance(module, MessagePassing):\n","\n","                if (not isinstance(mask, Parameter)\n","                        and '_edge_mask' in module._parameters):\n","                    mask = Parameter(mask)\n","\n","                module.explain = True\n","                module._edge_mask = mask\n","                module._loop_mask = loop_mask\n","                module._apply_sigmoid = apply_sigmoid\n","\n","    def Compute_ROC_AUC_edge(self, your_model, your_dataset, masked):\n","        preds = []\n","        reals = []\n","\n","        if masked == False:\n","            your_model.eval()\n","            for batched_data in your_dataset:\n","                reals.extend(batched_data.y.tolist())\n","                if your_model.__class__.__name__ == \"GCN_plus_GAP\":\n","                    Output_of_Hidden_Layers, pooling_layer_output, ffn_output, soft = your_model(batched_data)\n","                    preds.extend(soft.tolist())\n","                elif your_model.__class__.__name__ == \"DGCNN_Model\":\n","                    final_GNN_layer_output, sortpooled_embedings, output_conv1d_1, maxpooled_output_conv1d_1, output_conv1d_2, to_dense, output_h1, dropout_output_h1, output_h2, soft = your_model(batched_data, None)\n","                    preds.extend(soft.tolist())\n","                elif your_model.__class__.__name__ == \"DIFFPOOL_Model\":\n","                    concatination_list_of_poolings, prediction_output_not_softed, soft = your_model(batched_data, None)\n","                    preds.extend(soft.tolist())\n","                elif your_model.__class__.__name__ == \"GIN_Model\":\n","                    mlps_output_embeds, mlp_outputs_globalSUMpooled, lin1_output, lin1_output_dropouted, lin2_output, soft = your_model(batched_data, None)\n","                    preds.extend(soft.tolist())\n","        else:\n","            your_model.eval()\n","            for batched_data, edge_mask in zip(your_dataset, masked):\n","                reals.extend(batched_data.y.tolist())\n","                if your_model.__class__.__name__ == \"GCN_plus_GAP\":\n","                    self.apply_masks(your_model, edge_mask, batched_data.edge_index, apply_sigmoid=True)\n","                    Output_of_Hidden_Layers, pooling_layer_output, ffn_output, soft = your_model(batched_data)\n","                    preds.extend(soft.tolist())\n","                    self.clear_masks(your_model)\n","                elif your_model.__class__.__name__ == \"DGCNN_Model\":\n","                    final_GNN_layer_output, sortpooled_embedings, output_conv1d_1, maxpooled_output_conv1d_1, output_conv1d_2, to_dense, output_h1, dropout_output_h1, output_h2, soft = your_model(batched_data, edge_mask.tolist())\n","                    preds.extend(soft.tolist())\n","                elif your_model.__class__.__name__ == \"DIFFPOOL_Model\":\n","                    concatination_list_of_poolings, prediction_output_not_softed, soft = your_model(batched_data, edge_mask.tolist())\n","                    preds.extend(soft.tolist())\n","                elif your_model.__class__.__name__ == \"GIN_Model\":\n","                    mlps_output_embeds, mlp_outputs_globalSUMpooled, lin1_output, lin1_output_dropouted, lin2_output, soft = your_model(batched_data, edge_mask.tolist())\n","                    preds.extend(soft.tolist())\n","\n","\n","        # preds = torch.cat(preds)\n","        # preds, max_idxs = torch.max(preds[:], dim=1)\n","        # print(\"preds: \", preds)\n","        is_binary = len(np.unique(reals)) == 2\n","        if is_binary:\n","            preds = (np.array(preds)[:, 1] > 0.5).astype(int)\n","        roc_auc = metrics.roc_auc_score(np.array(reals), np.array(preds), multi_class=\"ovr\", average=\"micro\")\n","        return roc_auc\n","\n","    def normalize_saliency_edge_based(self, saliency_maps):\n","        Graphs_new_gradients = []\n","\n","        for graph_grads in saliency_maps:\n","            for edge_grads in graph_grads:\n","                edge_grads = (edge_grads-torch.min(graph_grads))/(torch.max(graph_grads)-torch.min(graph_grads)) if (torch.max(graph_grads)-torch.min(graph_grads)) != 0 else 0\n","            Graphs_new_gradients.append(graph_grads)\n","\n","        return Graphs_new_gradients\n","\n","    def my_fidelity(self, saliencies_for_multiple_classes, importance_threshold, style):\n","        if style == \"Node Feature\":\n","            try:\n","                fid_scores = []\n","                for key, value in saliencies_for_multiple_classes.items():\n","                    saliency_map = self.normalize_saliency_node_features_based(value)\n","                    fid_score = self.Fidelity_node_features(self.test_data, saliency_map, importance_threshold)\n","                    fid_scores.append(fid_score)\n","\n","                return mean(fid_scores)\n","            except:\n","                print(\"attributions are not in appropriate shape\")\n","\n","        elif style == \"Node\":\n","            try:\n","                fid_scores = []\n","                for key, value in saliencies_for_multiple_classes.items():\n","                    saliency_map = self.normalize_saliency_node_based(value)\n","                    fid_score = self.Fidelity_node(self.test_data, saliency_map, importance_threshold)\n","                    fid_scores.append(fid_score)\n","                return mean(fid_scores)\n","            except:\n","                print(\"attributions are not in appropriate shape\")\n","\n","        elif style ==\"Edge\":\n","            try:\n","                fid_scores = []\n","                for key, value in saliencies_for_multiple_classes.items():\n","                    saliency_map = self.normalize_saliency_edge_based(value)\n","                    fid_score = self.Fidelity_edge(self.test_data, saliency_map, importance_threshold)\n","                    fid_scores.append(fid_score)\n","                return mean(fid_scores)\n","            except:\n","                print(\"attributions are not in appropriate shape\")\n","\n","        else:\n","            print(\"node based not covered yet\")"]}]}