{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM/e+BEWfeKCPGXrYdhuSFq"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import torch\n","import statistics\n","from statistics import mean\n","import csv\n","from sklearn import metrics\n","from copy import deepcopy\n","from torch_geometric.nn import MessagePassing\n","from torch.nn.parameter import Parameter"],"metadata":{"id":"H_hSPIVb2Cm3","colab":{"base_uri":"https://localhost:8080/","height":361},"executionInfo":{"status":"error","timestamp":1711115119009,"user_tz":-60,"elapsed":8290,"user":{"displayName":"Ehsan Mobaraki","userId":"06314958514195310048"}},"outputId":"eab988db-fb32-4bd7-8f15-471dd1cac272"},"execution_count":null,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"No module named 'torch_geometric'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-e3cce180b38d>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcopy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMessagePassing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameter\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mParameter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch_geometric'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lXDwLWJGTj77"},"outputs":[],"source":["class evalaution_of_xmethods_stability(object):\n","    def __init__(self, a_trained_model, test_data):\n","        \"\"\"\n","            Perturb Node Features for Entire Dataset\n","            Explain perturbed dataset\n","            Compute their distance\n","        \"\"\"\n","        super(evalaution_of_xmethods_stability, self).__init__()\n","\n","        self.a_trained_model = a_trained_model\n","        self.test_data = test_data\n","\n","\n","    def normalize_saliency_node_features_based(self, sal_maps):\n","        Graphs_new_gradients = []\n","        for graph_grads in sal_maps:\n","            new_gradients = []\n","            for node_grads in graph_grads:\n","                node_gradients = []\n","                for dim in node_grads:\n","                    node_gradients.append((dim-min(node_grads))/(max(node_grads)-min(node_grads)) if (max(node_grads)-min(node_grads)) != 0 else 0)\n","                new_gradients.append(node_gradients)\n","            Graphs_new_gradients.append(new_gradients)\n","\n","        return Graphs_new_gradients\n","\n","    def normalize_saliency_node_based(self, sal_maps):\n","        Graphs_new_gradients = []\n","        for graph_grads in sal_maps:\n","            new_gradients = []\n","            for node_grads in graph_grads:\n","                val = (node_grads-min(graph_grads))/(max(graph_grads)-min(graph_grads)) if (max(graph_grads)-min(graph_grads)) != 0 else 0\n","                new_gradients.append(val)\n","            Graphs_new_gradients.append(new_gradients)\n","\n","        return Graphs_new_gradients\n","\n","    def normalize_stability_distance(self, distance_list):\n","        normalized_distance_list = []\n","        for exp_diff in distance_list:\n","            val = (exp_diff-min(distance_list))/(max(distance_list)-min(distance_list)) if (max(distance_list)-min(distance_list)) != 0 else 0\n","            normalized_distance_list.append(val)\n","        return normalized_distance_list\n","\n","    def perturb_node_features_of_dataset(self, dataset, mean, std):\n","        dataset_perturbed = deepcopy(dataset)\n","        for graph in dataset_perturbed:\n","            continuous_noise = torch.ones(dataset[0].x.size()[1]).normal_(mean, std)\n","            graph.x += continuous_noise\n","        return dataset_perturbed\n","\n","    def dist_explanation(self, normal_saliency, perturbed_saliency, top_k_features):\n","        distance_list = []\n","        for normal_graph, perturbed_graph in zip(normal_saliency, perturbed_saliency):\n","            perturbed_graph = np.array(perturbed_graph)\n","            perturbed_graph = torch.from_numpy(perturbed_graph)\n","            normal_graph = np.array(normal_graph)\n","            normal_graph = torch.from_numpy(normal_graph)\n","\n","            normal_graph_mask = torch.where(normal_graph >= normal_graph.topk(top_k_features)[0][-1].item(),\n","                                            torch.ones_like(normal_graph), torch.zeros_like(normal_graph))\n","            perturbed_graph_mask = torch.where(perturbed_graph >= perturbed_graph.topk(top_k_features)[0][-1].item(),\n","                                                torch.ones_like(perturbed_graph), torch.zeros_like(perturbed_graph))\n","\n","            distance = F.pairwise_distance(normal_graph_mask.unsqueeze(dim=0), perturbed_graph_mask.unsqueeze(dim=0), p=1) / normal_graph_mask.size()[0]\n","            distance_list.append(distance.tolist()[0])\n","        return distance_list\n","\n","    def one_graph_all_together(self, normal_saliency, perturbed_saliency):\n","        normal_saliency_list = []\n","        perturbed_saliency_list = []\n","        for normal_graph, perturbed_graph in zip(normal_saliency, perturbed_saliency):\n","            normal_saliency_list.append([node_feat for node in normal_graph for node_feat in node])\n","            perturbed_saliency_list.append([node_feat for node in perturbed_graph for node_feat in node])\n","        return perturbed_saliency_list, perturbed_saliency_list\n","\n","    def distance_thresholding_node_features(self, distance_list, importance_threshold):\n","        stability_list = []\n","        for graph_score in distance_list:\n","            if importance_threshold == graph_score == 0:\n","                stability_list.append(1)\n","            if importance_threshold == graph_score == 1:\n","                stability_list.append(0)\n","            if importance_threshold < graph_score:\n","                stability_list.append(1 - graph_score)\n","            else:\n","                stability_list.append(0)\n","        return stability_list\n","\n","    def normalize_saliency_edge_based(self, saliency_maps):\n","        Graphs_new_gradients = []\n","\n","        for graph_grads in saliency_maps:\n","            #new_gradients = []\n","            for edge_grads in graph_grads:\n","                edge_grads = (edge_grads-min(graph_grads))/(max(graph_grads)-min(graph_grads)) if (max(graph_grads)-min(graph_grads)) != 0 else 0\n","                #new_gradients.append(val)\n","            Graphs_new_gradients.append(graph_grads)\n","\n","        return Graphs_new_gradients\n","\n","    def my_stability(self, normal_saliencies_for_multiple_classes, perturbed_saliencies_for_multiple_classes,\n","                     top_k_features, importance_threshold, style):\n","        if style == \"Node Feature\":\n","            try:\n","                normal_saliency_classes_listed = {}\n","                perturbed_saliency_classes_listed = {}\n","                distances_class_based = {}\n","                for key, values in normal_saliencies_for_multiple_classes.items():\n","                    normal_saliencies_for_multiple_classes[key] = self.normalize_saliency_node_features_based(normal_saliencies_for_multiple_classes[key])\n","                    perturbed_saliencies_for_multiple_classes[key] = self.normalize_saliency_node_features_based(perturbed_saliencies_for_multiple_classes[key])\n","\n","                    normal_saliency_classes_listed[key], perturbed_saliency_classes_listed[key] = self.one_graph_all_together(normal_saliency_class0, perturbed_saliency_class0)\n","                    distances_class_based[key] = self.dist_explanation(normal_saliency_classes_listed[key], perturbed_saliency_classes_listed[key], top_k_features)\n","                    distances_class_based[key] = self.normalize_stability_distance(distances_class_based[key])\n","                    distances_class_based[key] = self.distance_thresholding_node_features(distances_class_based[key], importance_threshold)\n","\n","                distance_list = []\n","                for key, value in distances_class_based.items():\n","                    distance_list.append(mean(value))\n","\n","                return mean(distance_list)\n","            except:\n","                print(\"attributions are not in appropriate shape\")\n","\n","        elif style == \"Node\":\n","            try:\n","                normal_saliency_classes_listed = {}\n","                perturbed_saliency_classes_listed = {}\n","                distances_class_based = {}\n","                for key, values in normal_saliencies_for_multiple_classes.items():\n","                    normal_saliencies_for_multiple_classes[key] = self.normalize_saliency_node_based(normal_saliencies_for_multiple_classes[key])\n","                    perturbed_saliencies_for_multiple_classes[key] = self.normalize_saliency_node_based(perturbed_saliencies_for_multiple_classes[key])\n","\n","                    distances_class_based[key] = self.dist_explanation(normal_saliencies_for_multiple_classes[key], perturbed_saliencies_for_multiple_classes[key], top_k_features)\n","                    distances_class_based[key] = self.normalize_stability_distance(distances_class_based[key])\n","                    distances_class_based[key] = self.distance_thresholding_node_features(distances_class_based[key], importance_threshold)\n","\n","                distance_list = []\n","                for key, value in distances_class_based.items():\n","                    distance_list.append(mean(value))\n","\n","                return mean(distance_list)\n","            except:\n","                print(\"attributions are not in appropriate shape\")\n","\n","        elif style ==\"Edge\":\n","            try:\n","                normal_saliency_classes_listed = {}\n","                perturbed_saliency_classes_listed = {}\n","                distances_class_based = {}\n","                for key, values in normal_saliencies_for_multiple_classes.items():\n","                    normal_saliencies_for_multiple_classes[key] = self.normalize_saliency_edge_based(normal_saliencies_for_multiple_classes[key])\n","                    perturbed_saliencies_for_multiple_classes[key] = self.normalize_saliency_edge_based(perturbed_saliencies_for_multiple_classes[key])\n","\n","                    distances_class_based[key] = self.dist_explanation(normal_saliencies_for_multiple_classes[key], perturbed_saliencies_for_multiple_classes[key], top_k_features)\n","                    distances_class_based[key] = self.normalize_stability_distance(distances_class_based[key])\n","                    distances_class_based[key] = self.distance_thresholding_node_features(distances_class_based[key], importance_threshold)\n","\n","                distance_list = []\n","                for key, value in distances_class_based.items():\n","                    distance_list.append(mean(value))\n","\n","                return mean(distance_list)\n","            except:\n","                print(\"attributions are not in appropriate shape\")\n","        else:\n","            print(\"Only node_feat, node, and edge are covered.\")"]},{"cell_type":"code","source":["#stability = evalaution_of_xmethods_stability(GNN_Model, test_dataset)"],"metadata":{"id":"u8AA-nht3vlo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#perturbed_dataset = stability.perturb_node_features_of_dataset(test_dataset, 0.1, 0.1)"],"metadata":{"id":"DFkLbBj43yW1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#stability.my_stability(normal_saliency=importance_levels_for_feature_of_nodes_one_node_based,\n","#                       perturbed_saliency=importance_levels_for_feature_of_nodes_one_node_based_perturbed,\n","#                       top_k_features=10, importance_threshold=0.0000000001, style=\"Node\")"],"metadata":{"id":"vCr68Sk531KL"},"execution_count":null,"outputs":[]}]}