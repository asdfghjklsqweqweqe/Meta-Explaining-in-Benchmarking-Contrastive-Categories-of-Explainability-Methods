# -*- coding: utf-8 -*-
"""GNNExplainer on Graph Classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1E-P177CTC_4D27v6DksYtBgl3BIpna60

## ***GNNExplainer***


> Moduled: Accpeting the four GNNs (GCN+GAP, DGCNN, DIFFPOOL, and GIN)


---
"""



import argparse
import os
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
import numpy as np
from math import sqrt
from statistics import mean
import torch_geometric
from torch_geometric.datasets import TUDataset
import torch
import torch.nn as nn
from torch.nn.parameter import Parameter
from torch_geometric.nn import GCNConv
import torch.nn.functional as F
from torch.nn import Linear
from sklearn import metrics
from scipy.spatial.distance import hamming
import statistics
import pandas
import csv
from time import perf_counter
from torch_geometric.nn import GCNConv, global_mean_pool
from torch_geometric.loader import DataLoader
import torch_geometric.nn as gnn
from torch.autograd import graph
from typing import Any, Dict, Optional, Union
from IPython.core.display import deepcopy
from torch_geometric.nn import MessagePassing
import copy
loss_fn = F.binary_cross_entropy_with_logits


class GNNExplainer:
    coeffs = {
        'edge_size': 0.005,
        'edge_reduction': 'sum',
        'node_feat_size': 1.0,
        'node_feat_reduction': 'mean',
        'edge_ent': 1.0,
        'node_feat_ent': 0.1,
        'EPS': 1e-15,
        }
    def __init__(self, GNN_Model, Exp_Epoch, Exp_lr):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.GNN_Model = GNN_Model.to(self.device)
        self.loss_fn = F.binary_cross_entropy_with_logits

        self.explainer_epochs = Exp_Epoch
        self.explainer_lr = Exp_lr
        self.node_mask = self.edge_mask = None
        self.softmax = nn.Softmax(dim=-1)

        if self.GNN_Model.__class__.__name__ == "GCN_plus_GAP_Model":
            pass
        elif self.GNN_Model.__class__.__name__ == "DGCNN_Model":
            pass
        elif self.GNN_Model.__class__.__name__ == "DIFFPOOL_Model":
            pass
        elif self.GNN_Model.__class__.__name__ == "GIN_Model":
            pass
        else:
            raise Exception("We cover GCN_plus_GAP_Model, DGCNN, DIFFPOOL, and GIN.")


    def initialize_masks(self, graph_to_explain):
        graph_to_explain = graph_to_explain.to(self.device)
        (N, F), E = (1, len(graph_to_explain.x[0])), len(graph_to_explain.edge_index[1])
        std = 0.1
        self.node_mask = Parameter(torch.randn(N, F, device=self.device) * std, requires_grad=True).to(self.device)
        std = (torch.nn.init.calculate_gain('relu') * sqrt(2.0 / (2 * N)))
        self.edge_mask = Parameter(torch.randn(E, device=self.device) * std, requires_grad=True).to(self.device)

        # if self.GNN_Model.__class__.__name__ == "GCN_plus_GAP_Model":
        #     for module in self.GNN_Model.modules():
        #         if isinstance(module, MessagePassing):
        #             module.__explain__ = True
        #             module.__edge_mask__ = self.edge_mask
        #             module._apply_sigmoid = True

    def explainer_loss(self, By_Perturbation_predicted_label, predicted_label):
        By_Perturbation_predicted_label = By_Perturbation_predicted_label.to(torch.float32).to(self.device)
        predicted_label = predicted_label.to(torch.float32).to(self.device)

        loss_in_explainer_stage = self.loss_fn(By_Perturbation_predicted_label, predicted_label)

        m = self.edge_mask.sigmoid().to(self.device)
        edge_reduce = getattr(torch, self.coeffs['edge_reduction'])                                                     ######.         MARGINALIZE Over All Feature Subsets
        loss_in_explainer_stage = loss_in_explainer_stage + self.coeffs['edge_size'] * edge_reduce(m)
        ent = -m * torch.log(m + self.coeffs['EPS']) - (1 - m) * torch.log(1 - m + self.coeffs['EPS'])
        loss_in_explainer_stage = loss_in_explainer_stage + self.coeffs['edge_ent'] * ent.mean()

        m = self.node_mask.sigmoid().to(self.device)                                                                    ######.         Element-wise Entropy for structural and node feature masks to be discrete
        node_feat_reduce = getattr(torch, self.coeffs['node_feat_reduction'])
        loss_in_explainer_stage = loss_in_explainer_stage + self.coeffs['node_feat_size'] * node_feat_reduce(m)
        ent = -m * torch.log(m + self.coeffs['EPS']) - (1 - m) * torch.log(1 - m + self.coeffs['EPS'])
        loss_in_explainer_stage = loss_in_explainer_stage + self.coeffs['node_feat_ent'] * ent.mean()

        return loss_in_explainer_stage

    def explainer_train_step(self, graph_to_explain, predicted_label):
        graph_to_explain = graph_to_explain.to(self.device)
        new_graph_by_masks = graph_to_explain
        new_graph_by_masks = new_graph_by_masks.to(self.device)


        parameters = [self.node_mask, self.edge_mask]

        explainer_optimizer = torch.optim.Adam(parameters, lr=self.explainer_lr)
        for i in range(self.explainer_epochs):
            explainer_optimizer.zero_grad()
            h_node = graph_to_explain.x * self.node_mask.sigmoid()

            new_graph_by_masks.x = h_node

            if self.GNN_Model.__class__.__name__ == "GCN_plus_GAP_Model":
                Output_of_Hidden_Layers, pooling_layer_output, ffn_output, soft = self.GNN_Model(new_graph_by_masks, self.edge_mask)
            elif self.GNN_Model.__class__.__name__ == "DGCNN_Model":
                final_GNN_layer_output, sortpooled_embedings, output_conv1d_1, maxpooled_output_conv1d_1, output_conv1d_2, to_dense, output_h1, dropout_output_h1, output_h2, soft = self.GNN_Model(new_graph_by_masks, self.edge_mask)
            elif self.GNN_Model.__class__.__name__ == "DIFFPOOL_Model":
                concatination_list_of_poolings, without_soft, soft = self.GNN_Model(new_graph_by_masks, self.edge_mask)
            elif self.GNN_Model.__class__.__name__ == "GIN_Model":
                mlps_output_embeds, mlp_outputs_globalSUMpooled, lin1_output, lin1_output_dropouted, lin2_output, soft = self.GNN_Model(new_graph_by_masks, self.edge_mask)

            By_Perturbation_predicted_label = soft.argmax(dim=-1)

            loss = self.explainer_loss(By_Perturbation_predicted_label, predicted_label)

            loss.backward()
            explainer_optimizer.step()

    def post_process_mask(self, mask, apply_soft):
        if apply_soft:
            mask = mask.detach()
            mask = self.softmax(mask)
            return mask
        else:
            return mask

    def node_post_process_mask(self, node_mask, graph):
        attributed_graph = []
        attributed_graph_normalized = []
        attributed_graph_standardized = []
        # for node in graph.x:
        #     attributed_graph.append(sum(np.multiply(node.tolist(), node_mask.tolist()[0])))
        for node in graph.x:
            attributed_graph.append(torch.sum(node * node_mask[0]))

        for score in attributed_graph:
            if (max(attributed_graph)-min(attributed_graph)) != 0:
                attributed_graph_normalized.append(abs(((score-min(attributed_graph))*100/(max(attributed_graph)-min(attributed_graph)))).tolist())
            else:
                attributed_graph_normalized.append(0)

        for score in attributed_graph_normalized:
            if max(attributed_graph_normalized) != 0:
                attributed_graph_standardized.append(abs(score / max(attributed_graph_normalized)))
            else:
                attributed_graph_standardized.append(0)

        return attributed_graph_standardized


    def train_explainer(self, main_graph_to_explain, graph_to_explain, class_index):

        self.GNN_Model.eval()
        if self.GNN_Model.__class__.__name__ == "GCN_plus_GAP_Model":
            Output_of_Hidden_Layers, pooling_layer_output, ffn_output, soft = self.GNN_Model(graph_to_explain, None)
        elif self.GNN_Model.__class__.__name__ == "DGCNN_Model":
            final_GNN_layer_output, sortpooled_embedings, output_conv1d_1, maxpooled_output_conv1d_1, output_conv1d_2, to_dense, output_h1, dropout_output_h1, output_h2, soft = self.GNN_Model(graph_to_explain, None)
        elif self.GNN_Model.__class__.__name__ == "DIFFPOOL_Model":
            concatination_list_of_poolings, without_soft, soft = self.GNN_Model(graph_to_explain, None)
        elif self.GNN_Model.__class__.__name__ == "GIN_Model":
            mlps_output_embeds, mlp_outputs_globalSUMpooled, lin1_output, lin1_output_dropouted, lin2_output, soft = self.GNN_Model(graph_to_explain, None)

        predicted_label = soft.detach()[:, class_index]


        self.initialize_masks(graph_to_explain)
        self.explainer_train_step(graph_to_explain, predicted_label)

        node_mask = self.node_post_process_mask(self.node_mask, main_graph_to_explain)

        edge_mask = self.post_process_mask(self.edge_mask, True)
        return node_mask, edge_mask


    def __call__(self, graph_to_explain, class_index):
        graph_to_explain = graph_to_explain.to(self.device)

        new_graph_by_masks = deepcopy(graph_to_explain.detach()).to(self.device)
        node_mask, edge_mask = self.train_explainer(graph_to_explain, new_graph_by_masks, class_index)

        return node_mask, edge_mask




# times = []
# explanations_nodes = []
# class_index = 0
# for graph in test_dataset:
#    t1 = perf_counter()
#    EXP = GNNExplainer(GNN_Model, 1, 0.001)
#    correct_node_mask, correct_edge_mask = EXP(graph, class_index)
#    times.append(perf_counter()-t1)
#    explanations_nodes.append(correct_node_mask)
#    print("correct_node_mask: ", correct_node_mask)

# predicted_label = soft.detach()[:,class_index]