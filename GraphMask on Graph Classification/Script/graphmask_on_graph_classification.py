# -*- coding: utf-8 -*-
"""GraphMask on Graph Classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BVMP3R1_gqrt-5oBNktVhsCaabRzwWkn

## ***GraphMask***


> Moduled: Accpeting the four GNNs (GCN+GAP, DGCNN, DIFFPOOL, and GIN)


---
"""


import argparse
import os
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
import numpy as np
from math import sqrt
import math
from torch_geometric.datasets import TUDataset
import torch as th
import torch
import torch.nn as nn
from torch.nn.parameter import Parameter
from torch_geometric.nn import GCNConv
import torch.nn.functional as F
from torch.nn import Linear, LayerNorm
from sklearn import metrics
from scipy.spatial.distance import hamming
import statistics
import pandas
from time import perf_counter
from IPython.core.display import deepcopy
from torch_geometric.nn import MessagePassing
import copy
from torch.nn import ReLU, Sequential
from torch import sigmoid
from itertools import chain
from time import perf_counter
import csv
from torch_geometric.nn import GCNConv, global_mean_pool
from torch_geometric.loader import DataLoader
import torch_geometric.nn as gnn




class GraphMask:
    def __init__(self, GNN_Model, explainer_save_index, Exp_Epoch, Exp_lr, explainer_hid_dim, explainer_input_dim,
                 dataset_name):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

        ######################              MAIN
        self.beta = 1 / 3
        self.gamma = -0.2
        self.zeta = 1.0
        self.fix_temp = True
        self.loc_bias = 0.3
        self.penalty_scaling = 1
        self.allowance = 0.03
        self.max_allowed_performance_diff = 0.05
        self.temp = self.beta if self.fix_temp else Parameter(torch.zeros(1).fill_(self.beta))
        self.gamma_zeta_ratio = math.log(-self.gamma / self.zeta)
        ###########
        self.GNN_Model = GNN_Model
        self.explainer_input_dim = explainer_input_dim
        self.explainer_lr = Exp_lr
        self.explainer_hid_dim = explainer_hid_dim
        self.graphmask_mlp = Sequential(Linear(in_features=self.explainer_input_dim * 2, out_features=self.explainer_hid_dim),
                                        LayerNorm(self.explainer_hid_dim), ReLU(),
                                        Linear(in_features=self.explainer_hid_dim, out_features=1)).to(self.device)
        self.graphmask_mlp_optimizer = torch.optim.Adam(self.graphmask_mlp.parameters(), lr=self.explainer_lr)
        self.explainer_epochs = Exp_Epoch
        #self.criterion = torch.nn.CrossEntropyLoss()
        #self.criterion = F.binary_cross_entropy_with_logits
        self.criterion = nn.L1Loss().to(self.device)
        self.explainer_save_index = explainer_save_index
        self.Explainability_name = 'GraphMask'
        self.Task_name = 'Graph Classification'
        self.Model_Name = GNN_Model.__class__.__name__
        self.dataset_name = dataset_name

        self.baseline = torch.FloatTensor(1).to(self.device)
        bl_stdv = 1. / math.sqrt(1)
        self.baseline.uniform_(-bl_stdv, bl_stdv)
        self.baseline = torch.nn.Parameter(self.baseline, requires_grad=True).to(self.device)

        ###############.      Lagrangian Optimization
        self.min_alpha = -2
        self.max_alpha = 30
        self.update_counter = 0
        self.init_alpha = 0.55
        self.alpha_optimizer_lr = 1e-2
        self.alpha = torch.tensor(self.init_alpha, requires_grad=True)#.to(self.device)
        self.optimizer_alpha = torch.optim.RMSprop([self.alpha], lr=self.alpha_optimizer_lr, centered=True)
        self.update_counter = 0


    def explainer_loss(self, By_Perturbation_predicted_label, predicted_label):
        loss_per_epoch = self.criterion(By_Perturbation_predicted_label.to(self.device), predicted_label.to(self.device))
        return loss_per_epoch

    def binary_concrete(self, explaier_outputs, temperature, summarize_penalty=True):
        explaier_outputs = explaier_outputs + self.loc_bias
        if self.ExTrain_or_ExTest == 'train':
            u = torch.empty_like(explaier_outputs).uniform_(1e-6, 1.0-1e-6).to(self.device)
            s = sigmoid((torch.log(u) - torch.log(1 - u) + explaier_outputs) / temperature)
            penalty = sigmoid(explaier_outputs - temperature * self.gamma_zeta_ratio)
        else:
            s = sigmoid(explaier_outputs)
            penalty = torch.zeros_like(explaier_outputs).to(self.device)

        if summarize_penalty:
            penalty = penalty.mean()

        s = s * (self.zeta - self.gamma) + self.gamma
        clipped_s = s.clamp(min=0, max=1)

        hard_concrete = (clipped_s > 0.5).float()
        clipped_s = clipped_s + (hard_concrete - clipped_s).detach()
        clipped_s = clipped_s.squeeze(dim=-1)

        return clipped_s, penalty

    def lagrangian_optimization_update(self, f, g, batch_size_multiplier):

        if batch_size_multiplier is not None and batch_size_multiplier > 1:
            if self.update_counter % batch_size_multiplier == 0:
                self.graphmask_mlp_optimizer.zero_grad()
                self.optimizer_alpha.zero_grad()

            self.update_counter += 1
        else:
            self.graphmask_mlp_optimizer.zero_grad()
            self.optimizer_alpha.zero_grad()

        loss = f + torch.nn.functional.softplus(self.alpha) * g
        loss.backward(retain_graph=True)

        if batch_size_multiplier is not None and batch_size_multiplier > 1:
            if self.update_counter % batch_size_multiplier == 0:
                self.graphmask_mlp_optimizer.step()
                self.alpha.grad *= -1
                self.optimizer_alpha.step()
        else:
            self.graphmask_mlp_optimizer.step()
            self.alpha.grad *= -1
            self.optimizer_alpha.step()

        if self.alpha.item() < -2:
            self.alpha.data = torch.full_like(self.alpha.data, -2)
        elif self.alpha.item() > 30:
            self.alpha.data = torch.full_like(self.alpha.data, 30)

    def train_step_explainer(self, merged_embeddings_list_batchs, GNN_Model, your_dataset, GNN_Model_preds_NOT_MASKED, target_class):

        self.graphmask_mlp.train()
        self.graphmask_mlp.zero_grad()
        for batched_merged_embeddings, batched_preds_NOT_MASKED, batched_graphs in zip(merged_embeddings_list_batchs, GNN_Model_preds_NOT_MASKED, your_dataset):
            batched_merged_embeddings = batched_merged_embeddings.to(self.device)
            explaier_outputs = self.graphmask_mlp(batched_merged_embeddings).view(-1)
            batched_graphs = batched_graphs.to(self.device)

            edge_mask, sparsity_penalty = self.binary_concrete(explaier_outputs, self.temp)

            importance_indices = edge_mask == 0
            edge_mask[importance_indices] = self.baseline

            # if self.GNN_Model.__class__.__name__ == "GCN_plus_GAP_Model":
            #     self.apply_masks(GNN_Model, edge_mask, batched_graphs.edge_index, apply_sigmoid=True)

            if GNN_Model.__class__.__name__ == "GCN_plus_GAP_Model":
                Output_of_Hidden_Layers, pooling_layer_output, ffn_output, soft_MASKED = GNN_Model(batched_graphs, edge_mask)

            elif GNN_Model.__class__.__name__ == "DGCNN_Model":
                final_GNN_layer_output, sortpooled_embedings, output_conv1d_1, maxpooled_output_conv1d_1, output_conv1d_2, to_dense, output_h1, dropout_output_h1, output_h2, soft_MASKED = GNN_Model(batched_graphs, edge_mask)

            elif GNN_Model.__class__.__name__ == "DIFFPOOL_Model":
                concatination_list_of_poolings, prediction_output_not_softed, soft_MASKED = GNN_Model(batched_graphs, edge_mask)

            elif GNN_Model.__class__.__name__ == "GIN_Model":
                mlps_output_embeds, mlp_outputs_globalSUMpooled, lin1_output, lin1_output_dropouted, lin2_output, soft_MASKED = GNN_Model(batched_graphs, edge_mask)

            batch_loss = self.explainer_loss(soft_MASKED.detach()[:,target_class].to(torch.float32), batched_preds_NOT_MASKED.detach()[:,target_class].to(torch.float32))

            g = torch.relu(batch_loss - self.allowance).mean()
            f = (sparsity_penalty * self.penalty_scaling)

            try:
                batch_size = your_dataset.batch_size
            except:
                batch_size = 1

            self.lagrangian_optimization_update(f=f, g=g, batch_size_multiplier=batch_size)

            batch_loss.requires_grad = True
            batch_loss.backward(retain_graph=True)
            self.graphmask_mlp_optimizer.step()

        return edge_mask

    def train_explainer(self, GNN_Model, your_dataset, target_class):
        edge_masks_per_epoch = []
        your_dataset2 = deepcopy(your_dataset)
        merged_embeddings_list = self.get_merged_embeddings(GNN_Model, your_dataset)

        GNN_Model_preds_NOT_MASKED = []
        for batch_of_graphs in your_dataset:
            batch_of_graphs = batch_of_graphs.to(self.device)
            if self.GNN_Model.__class__.__name__ == "GCN_plus_GAP_Model":
                Output_of_Hidden_Layers, pooling_layer_output, ffn_output, soft_NOT_MASKED = self.GNN_Model(batch_of_graphs, None)
            elif self.GNN_Model.__class__.__name__ == "DGCNN_Model":
                final_GNN_layer_output, sortpooled_embedings, output_conv1d_1, maxpooled_output_conv1d_1, output_conv1d_2, to_dense, output_h1, dropout_output_h1, output_h2, soft_NOT_MASKED = self.GNN_Model(batch_of_graphs, None)
            elif self.GNN_Model.__class__.__name__ == "DIFFPOOL_Model":
                concatination_list_of_poolings, prediction_output_not_softed, soft_NOT_MASKED = self.GNN_Model(batch_of_graphs, None)
            elif self.GNN_Model.__class__.__name__ == "GIN_Model":
                mlps_output_embeds, mlp_outputs_globalSUMpooled, lin1_output, lin1_output_dropouted, lin2_output, soft_NOT_MASKED = self.GNN_Model(batch_of_graphs, None)

            GNN_Model_preds_NOT_MASKED.append(soft_NOT_MASKED)

        for epoch in range(self.explainer_epochs):
            #print("Epoch: ", epoch)
            edge_mask = self.train_step_explainer(merged_embeddings_list, GNN_Model, your_dataset, GNN_Model_preds_NOT_MASKED, target_class)
            edge_masks_per_epoch.append(edge_mask)


            if (epoch + 1) == self.explainer_save_index:
                torch.save({'epoch': epoch+1, 'model_state_dict': self.graphmask_mlp.state_dict(),
                            'optimizer_state_dict': self.graphmask_mlp_optimizer.state_dict(),
                            'baseline_state_dict': self.baseline}, '/data/cs.aau.dk/ey33jw/Explainability_Methods/' +
                           str(self.Explainability_name) + " on " + str(self.Task_name) + "/Model/" +
                           str(self.Model_Name) + "_classifier_GraphMask_MLP_" + str(epoch + 1) + "_epochs_" +
                           str(self.dataset_name) + "_" + str(target_class) + ".pt")

        # if self.GNN_Model.__class__.__name__ == "GCN_plus_GAP_Model":
        #     self.clear_masks(GNN_Model)

    def test_explainer(self, GNN_Model, your_dataset, graphmask_mlp, target_class):
        edge_masks = []
        predicted_labels_MASKED = []
        merged_embeddings_list_batchs = self.get_merged_embeddings(GNN_Model, your_dataset)
        GNN_Model_preds_NOT_MASKED = []
        graphmask_mlp.eval()
        for batch_of_graphs in your_dataset:
            batch_of_graphs = batch_of_graphs.to(self.device)
            if self.GNN_Model.__class__.__name__ == "GCN_plus_GAP_Model":
                Output_of_Hidden_Layers, pooling_layer_output, ffn_output, soft_NOT_MASKED = GNN_Model(batch_of_graphs, None)
            elif self.GNN_Model.__class__.__name__ == "DGCNN_Model":
                final_GNN_layer_output, sortpooled_embedings, output_conv1d_1, maxpooled_output_conv1d_1, output_conv1d_2, to_dense, output_h1, dropout_output_h1, output_h2, soft_NOT_MASKED = GNN_Model(batch_of_graphs, None)
            elif self.GNN_Model.__class__.__name__ == "DIFFPOOL_Model":
                concatination_list_of_poolings, prediction_output_not_softed, soft_NOT_MASKED = GNN_Model(batch_of_graphs, None)
            elif self.GNN_Model.__class__.__name__ == "GIN_Model":
                mlps_output_embeds, mlp_outputs_globalSUMpooled, lin1_output, lin1_output_dropouted, lin2_output, soft_NOT_MASKED = GNN_Model(batch_of_graphs, None)

            GNN_Model_preds_NOT_MASKED.append(soft_NOT_MASKED)

        for batched_merged_embeddings, batched_preds_NOT_MASKED, batched_graphs in zip(merged_embeddings_list_batchs, GNN_Model_preds_NOT_MASKED, your_dataset):
            explaier_outputs = graphmask_mlp(batched_merged_embeddings).view(-1)
            batched_graphs = batched_graphs.to(self.device)
            # edge_masks.append(explaier_outputs)

            edge_mask, sparsity_penalty = self.binary_concrete(explaier_outputs, self.temp)

            importance_indices = edge_mask == 0
            edge_mask[importance_indices] = self.baseline
            edge_masks.append(edge_mask)

            # if self.GNN_Model.__class__.__name__ == "GCN_plus_GAP_Model":
            #     self.apply_masks(GNN_Model, edge_mask, batched_graphs.edge_index, apply_sigmoid=True)
            if self.GNN_Model.__class__.__name__ == "GCN_plus_GAP_Model":
                Output_of_Hidden_Layers, pooling_layer_output, ffn_output, soft_MASKED = GNN_Model(batched_graphs, edge_mask)
            elif self.GNN_Model.__class__.__name__ == "DGCNN_Model":
                final_GNN_layer_output, sortpooled_embedings, output_conv1d_1, maxpooled_output_conv1d_1, output_conv1d_2, to_dense, output_h1, dropout_output_h1, output_h2, soft_MASKED = GNN_Model(batched_graphs, edge_mask)
            elif self.GNN_Model.__class__.__name__ == "DIFFPOOL_Model":
                concatination_list_of_poolings, prediction_output_not_softed, soft_MASKED = GNN_Model(batched_graphs, edge_mask)
            elif self.GNN_Model.__class__.__name__ == "GIN_Model":
                mlps_output_embeds, mlp_outputs_globalSUMpooled, lin1_output, lin1_output_dropouted, lin2_output, soft_MASKED = GNN_Model(batched_graphs, edge_mask)

            predicted_labels_MASKED.append(torch.squeeze(soft_MASKED.detach()[:,target_class].to(torch.float32)).tolist())
        # self.clear_masks(GNN_Model)

        # normalized_scores = self.normalize_scores(edge_masks)

        # standardized_scores = self.standardize_scores(normalized_scores)

        return predicted_labels_MASKED, edge_masks#standardized_scores

    def normalize_scores(self, Edge_Masks_original):
        graphs = []
        for graph_masks in Edge_Masks_original:
            graph_masks = torch.tensor(graph_masks, device=graph_masks.device) if not isinstance(graph_masks, torch.Tensor) else graph_masks

            min_val = graph_masks.min()
            max_val = graph_masks.max()

            normalized_masks = (graph_masks - min_val) * 100 / (max_val - min_val) if (max_val-min_val) != 0 else 0
            graphs.append(normalized_masks)

        return graphs

    def standardize_scores(self, Edge_Masks_original):
        graphs = []
        for graph_masks in Edge_Masks_original:
            graph_masks = torch.tensor(graph_masks, device=graph_masks.device) if not isinstance(graph_masks, torch.Tensor) else graph_masks
            max_val = graph_masks.max()
            if max_val == 0:
                standardized_masks = torch.zeros_like(graph_masks)
            else:
                standardized_masks = graph_masks / max_val

            graphs.append(standardized_masks)

        return graphs

    def apply_masks(self, model, mask, edge_index, apply_sigmoid):
        loop_mask = edge_index[0] != edge_index[1]

        for module in model.modules():
            if isinstance(module, MessagePassing):

                if (not isinstance(mask, Parameter)
                        and '_edge_mask' in module._parameters):
                    mask = Parameter(mask)

                module.explain = True
                module._edge_mask = mask
                module._loop_mask = loop_mask
                module._apply_sigmoid = apply_sigmoid
                #print(module._edge_mask)

    def clear_masks(self, model):

        for module in model.modules():
            if isinstance(module, MessagePassing):
                module.explain = False
                module._edge_mask = None
                module._loop_mask = None
                module._apply_sigmoid = True
        return module

    def batch_handler_for_embeddings(self, batched_graphs, new_embeddings):
        unpaded_data = []
        #if batched_graphs.batch is not None:
        #    graph_sizes = [batched_graphs[i].x.size()[1] for i in range(len(batched_graphs))]
        #else:
        #    graph_sizes = [len(batched_graphs.x)]

        #for i, graph in enumerate(new_embeddings):
        #    unpaded_data.append(graph[:graph_sizes[i]])
        #new_graph_embeddings_merged = torch.cat(unpaded_data, dim=0)
        for i, graph in enumerate(new_embeddings):
            unpaded_data.append(graph)
        new_graph_embeddings_merged = torch.cat(unpaded_data, dim=0)
        return new_graph_embeddings_merged

    def get_merged_embeddings(self, GNN_Model, your_dataset):
        new_embeddings_list = []
        edge_embeddings_list = []
        for batched_graphs in your_dataset:
            new_graph_by_masks = deepcopy(batched_graphs.detach())

            new_embeddings, GNN_Model_explain_predicted_labels = self.get_hopped_embeddings(GNN_Model, new_graph_by_masks)

            new_embeddings_list.append(new_embeddings)
            if len(new_embeddings.size()) == 3: #and new_embeddings.size()[0] > 1:
                new_graph_embeddings_updated = self.batch_handler_for_embeddings(batched_graphs, new_embeddings)
                edge_embeddings = self.edge_embeddings_generator(new_graph_embeddings_updated, deepcopy(batched_graphs.edge_index), deepcopy(batched_graphs.batch))
            else:
                edge_embeddings = self.edge_embeddings_generator(new_embeddings, deepcopy(batched_graphs.edge_index), deepcopy(batched_graphs.batch))

            edge_embeddings_list.append(edge_embeddings)

        return edge_embeddings_list


    def edge_embeddings_generator(self, embedding, edge_index, batch_tensor):
        Zs = []
        if batch_tensor is None:
            Zs = [embedding[edge_index[0]], embedding[edge_index[1]]]
            edge_embds = torch.cat(Zs, dim=-1)
            return edge_embds

        for i in range(batch_tensor.max().item() + 1):
            edge_mask = (batch_tensor[edge_index[0]] == i) & (batch_tensor[edge_index[1]] == i)
            local_edge_index = edge_index[:, edge_mask]
            unique_nodes = torch.unique(local_edge_index)
            graph_embeddings = embedding[unique_nodes]
            node_to_local_index = {node.item(): idx for idx, node in enumerate(unique_nodes)}
            local_edge_index_mapped = torch.tensor([[node_to_local_index[node.item()] for node in local_edge_index[0]],
                                                    [node_to_local_index[node.item()] for node in local_edge_index[1]]],
                                                   device=embedding.device)
            Zs.append(torch.cat([graph_embeddings[local_edge_index_mapped[0]],
                                 graph_embeddings[local_edge_index_mapped[1]]], dim=-1))
        edge_embds = torch.cat(Zs, dim=0)

        return edge_embds

    def get_hopped_embeddings(self, GNN_Model, batch_of_graphs):
        batch_of_graphs = batch_of_graphs.to(self.device)
        if GNN_Model.__class__.__name__ == "GCN_plus_GAP_Model":
            Output_of_Hidden_Layers, pooling_layer_output, ffn_output, soft = GNN_Model(batch_of_graphs, None)
            return Output_of_Hidden_Layers[-1], soft.argmax(dim=1)

        elif GNN_Model.__class__.__name__ == "DGCNN_Model":
            final_GNN_layer_output, sortpooled_embedings, output_conv1d_1, maxpooled_output_conv1d_1, output_conv1d_2, to_dense, output_h1, dropout_output_h1, output_h2, soft = GNN_Model(batch_of_graphs, None)
            return final_GNN_layer_output, soft.argmax(dim=1)

        elif GNN_Model.__class__.__name__ == "DIFFPOOL_Model":
            concatination_list_of_poolings, prediction_output_not_softed, soft = GNN_Model(batch_of_graphs, None)
            return batch_of_graphs.x, soft.argmax(dim=1)

        elif GNN_Model.__class__.__name__ == "GIN_Model":
            mlps_output_embeds, mlp_outputs_globalSUMpooled, lin1_output, lin1_output_dropouted, lin2_output, soft = GNN_Model(batch_of_graphs, None)
            return torch.squeeze(mlps_output_embeds[-1], dim=0), soft.argmax(dim=1)


    def load_explainer_mlp(self, Exp_Load_index, target_class):
        graphmask_mlp = Sequential(Linear(in_features=self.explainer_input_dim * 2, out_features=self.explainer_hid_dim),
                                   LayerNorm(self.explainer_hid_dim), ReLU(),
                                   Linear(in_features=self.explainer_hid_dim, out_features=1))
        graphmask_mlp_optimizer = torch.optim.Adam(self.graphmask_mlp.parameters(), lr=self.explainer_lr)
        checkpoint = torch.load(str(self.Explainability_name) + " on " + str(self.Task_name) + "/Model/" +
                                str(self.Model_Name) + "_classifier_GraphMask_MLP_" + str(Exp_Load_index) + "_epochs_" +
                                str(self.dataset_name) + "_" + str(target_class) + ".pt")
        graphmask_mlp.load_state_dict(checkpoint['model_state_dict'])
        graphmask_mlp_optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        baseline = checkpoint['baseline_state_dict']
        #print("BaseLine: ", target_class, "   ", baseline)




        return graphmask_mlp, graphmask_mlp_optimizer, baseline

    def __call__(self, ExTrain_or_ExTest,  Exp_Load_index, your_dataset, target_class):
        self.ExTrain_or_ExTest = ExTrain_or_ExTest
        if ExTrain_or_ExTest == "train":
            #t1 = perf_counter()
            self.train_explainer(self.GNN_Model, your_dataset, target_class)
            #t2 = perf_counter()
            #print((t2 - t1) / len(your_dataset))
        elif ExTrain_or_ExTest == "test":
            graphmask_mlp, graphmask_mlp_optimizer, baseline = self.load_explainer_mlp(Exp_Load_index=Exp_Load_index,
                                                                                       target_class=target_class)
            graphmask_mlp = graphmask_mlp.to(self.device)
            predicted_labels, edge_masks = self.test_explainer(self.GNN_Model, your_dataset, graphmask_mlp, target_class)
            #print("Class: ", target_class, " predicted_labels: ", predicted_labels)
            #print("masks: ", len(edge_masks), edge_masks[0])
            return edge_masks
        else:
            print("recheck")



#target_class = 0
#ExTrain_or_ExTest = 'test'


#t1_start = perf_counter()
#EXP = GraphMask(GNN_Model=GNN_Model, explainer_save_index=10, Exp_Epoch=11, Exp_lr=0.001, explainer_hid_dim=13, explainer_input_dim=13)
#EXP(ExTrain_or_ExTest=ExTrain_or_ExTest, Exp_Load_index=10, your_dataset=[fake_test_dataset[-1]], target_class=target_class)
#t2_start = perf_counter()
#attribution_time = (t2_start - t1_start)/len(fake_train_dataset if ExTrain_or_ExTest == 'train' else fake_test_dataset)
#print("attribution_time: ", attribution_time)


# test on one sample in a e sample in a list: [fake_test_dataset[-1]], train could be done by both sample-based and batch-based.