# -*- coding: utf-8 -*-
"""Generalized HyperParameter Optimization of GNNs on Graph Classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LOvSa7YLhJLfn5HTixprWZUZxYirGBoq

## ***GNN HyperParameter Optimization***


> Moduled: Accpeting the four GNNs (GCN+GAP, DGCNN, DIFFPOOL, and GIN).

Configurations of the gnn models for different datasets should be settled manually.


---
"""


from torch_geometric.datasets import Planetoid
from torch_geometric.transforms import NormalizeFeatures
from colorit import *
from torch._C import dtype
import torch
import csv
import torch.nn as nn
import matplotlib.pyplot as plt
from torch_geometric.nn import GCNConv
import torch_geometric.nn as gnn
import torch.nn.functional as F
from torch.nn import Linear
from sklearn.model_selection import train_test_split
import numpy as np
from torch_geometric.loader import DataLoader
from torch_geometric.nn import global_mean_pool
from torch_geometric.datasets import TUDataset
from torch_geometric.data.batch import Batch
from torch import Tensor
from torch_geometric.typing import OptPairTensor, Adj, OptTensor, Size
from typing import Callable, Union, Tuple
# from torch_sparse import SparseTensor
import itertools
# from torchsummary import summary
from torch.autograd import Variable
# from keras import backend as K
from statistics import mean
from operator import add
# import tensorflow as tf
from sklearn.model_selection import KFold
from sklearn import metrics
from bayes_opt import BayesianOptimization, UtilityFunction
from time import perf_counter
import pickle


class GNN_Models_HyperParameter_Optimization:

    def __init__(self, GNN_Model_index, meta_learning_epochs, input_dataset, dataset_name, act_fun, weight_decay):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.GNN_Models_Name = dict([(1, "GCN_plus_GAP_Model"), (2, "DGCNN_Model"), (3, "DIFFPOOL_Model"),
                                     (4, "GIN_Model")])
        self.GNN_Model_name = self.GNN_Models_Name[GNN_Model_index]
        self.meta_learning_epochs = meta_learning_epochs
        self.input_dataset = input_dataset
        self.dataset_name = dataset_name
        self.act_fun = act_fun
        self.LOSS_Visualization_Parameter = 100
        self.Model_Saving_Parameter = 100
        self.Explainability_name = "HyperParameter Optimization of GNNs"
        self.Task_name = "Graph Classification"
        self.default_path = '/data/cs.aau.dk/ey33jw/Explainability_Methods/'
        os.chdir(self.default_path)
        self.csv_files_path = "/data/cs.aau.dk/ey33jw/Optimized_HyperParameters_for_GNN_Models/"
        current_directory = os.getcwd()
        # print("Current Working Directory:", current_directory)

        #---------------------------------------  LOSSes
        self.CrossEnt_criterion = torch.nn.CrossEntropyLoss()
        self.MAE_criterion = torch.nn.L1Loss()
        self.MSE_criterion = torch.nn.MSELoss()

        #---------------------------------------  HyperParameters Setup
        num_folds = [2, 6.999999]
        lr = [0.0001, 0.01]
        EPOCHS = [1, 5.999999]   #.  * self.epoch_factor
        self.epoch_factor = 100
        DropOut_Rate = [0.25, 0.75]
        Bias = [0, 1]
        Batch = [1, 4.999999] #. Batch_count_by_16
        Loss_Function = [1, 3.999999]   #.   "CrossEntropyLoss" add more for now
        Weight_Initializer = [1, 3.999999] # 1. Xavier Uniform.  2. Kaiming.  3. uniform 0,0.1std
        self.weight_decay = weight_decay

        self.utility = UtilityFunction(kind="ucb", kappa=1.96, xi=0.01)
        param_grid = dict(num_folds=num_folds, lr=lr, EPOCHS=EPOCHS, DropOut_Rate=DropOut_Rate, Bias=Bias,
                          Batch_count_by_16=Batch, Loss_Function=Loss_Function, Weight_Initializer=Weight_Initializer)
        self.meta_optimizer = BayesianOptimization(f=None, pbounds=param_grid, verbose=3, random_state=4)

    def __call__(self):
        self.call_me_to_start()


    def call_me_to_start(self):
        for i in range(self.meta_learning_epochs):

            hyp_list = self.meta_optimizer.suggest(self.utility)
            hyp_list["num_folds"] = int(hyp_list["num_folds"])
            hyp_list["lr"] = float(hyp_list["lr"])
            hyp_list["EPOCHS"] = int(hyp_list["EPOCHS"]) * self.epoch_factor
            hyp_list["DropOut_Rate"] = float(hyp_list["DropOut_Rate"])
            hyp_list["Bias"] = bool(round(hyp_list["Bias"]))
            hyp_list["Batch_count_by_16"] = int(hyp_list["Batch_count_by_16"])
            hyp_list["Loss_Function"] = int(hyp_list["Loss_Function"])
            hyp_list["Weight_Initializer"] = int(hyp_list["Weight_Initializer"])

            print(hyp_list)

            # the new parameter values.
            #hyp_list = {'Batch_count_by_16': 2, 'Bias': True, 'Dataset_name': 1, 'DropOut_Rate': 0.6831446492908493, 'EPOCHS': 100, 'Loss_Function': 1, 'Weight_Initializer': 2, 'load_index': 0, 'lr': 0.007360463658354643, 'num_folds': 2}

            target = self.Central_Room(act_fun=self.act_fun, load_index=0, weight_decay=self.weight_decay, **hyp_list)
            try:
                self.meta_optimizer.register(params=hyp_list, target=target)
            except:
                pass
            print(color('===========================================================================================================', Colors.purple))
            print(color(f'iTeration: {i+1:03d} is Passed!', Colors.red))
        self.Final_OutComes_by_fig()

    def Central_Room(self, act_fun, load_index, weight_decay, num_folds, lr, EPOCHS, DropOut_Rate, Bias,
                     Batch_count_by_16, Loss_Function, Weight_Initializer):

        self.num_classes = self.find_num_classes_graph_labels(self.input_dataset)
        Train_Datasets, Test_Datasets = self.get_your_dataset(num_folds, self.input_dataset, Batch_count_by_16)
        Models = self.Models_K_Folds(num_folds, self.input_dataset, DropOut_Rate, Bias, Weight_Initializer, act_fun=act_fun)
        Optimizers = self.Optimizers_K_Fold_and_LR(Models, num_folds, lr, weight_decay)

        # if load_index != 0:
        #     GNN_Model, GNN_Model_Optimizer, load_index = loading_model(load_index)

        GNN_Model_training_time_per_epoch_FOLDS = []
        ACC_Probabilistic_list = []
        ACC_Quantitative_list = []
        AUC_ROC_list = []
        AUC_PRC_list = []

        for i in range(num_folds):
            GNN_Model_training_loss_per_epoch, GNN_Model_training_time_per_epoch = self.gnn_train(EPOCHS, load_index,
                                                                                                  Models[i],
                                                                                                  Optimizers[i],
                                                                                                  Train_Datasets[i],
                                                                                                  Loss_Function)
            GNN_Model_training_time_per_epoch_FOLDS.append(mean(GNN_Model_training_time_per_epoch))
            AUC_ROC, AUC_PRC, ACC_Quantitative, ACC_Probabilistic = self.gnn_model_test(Test_Datasets[i], Models[i])
            print("AUC_ROC: ", AUC_ROC, "\nAUC_PRC: ", AUC_PRC, "\nACC_Quantitative: ", ACC_Quantitative,
                  "\nACC_Probabilistic: ", ACC_Probabilistic)
            ACC_Probabilistic_list.append(ACC_Probabilistic)
            ACC_Quantitative_list.append(ACC_Quantitative)
            AUC_ROC_list.append(AUC_ROC)
            AUC_PRC_list.append(AUC_PRC)

        print("======================================================================", "len(AUC_ROC_list): ",
              len(AUC_ROC_list))

        Model_Description = self.describe_trained_model(act_fun=act_fun)
        self.recording_csv_files([self.dataset_name, self.GNN_Model_name, Model_Description, Weight_Initializer,
                                  num_folds, EPOCHS, lr, DropOut_Rate, Bias, Batch_count_by_16*16, Loss_Function,
                                  mean(AUC_ROC_list), mean(AUC_PRC_list), mean(ACC_Probabilistic_list),
                                  mean(GNN_Model_training_time_per_epoch_FOLDS)])

        return mean(AUC_ROC_list)

    def find_num_classes_graph_labels(self, graph_dataset):
        unique_labels = set()
        for graph in graph_dataset:
            unique_labels.add(graph.y.tolist()[0])
        num_classes = len(unique_labels)
        return num_classes

    def pick_your_graph(self, main_dataset, train_index_list, test_index_list):
        train_dataset = []
        test_dataset = []

        for index in train_index_list:
            train_dataset.append(main_dataset[index])
        for index in test_index_list:
            test_dataset.append(main_dataset[index])

        return train_dataset, test_dataset

    def batching_data(self, train_dataset, test_dataset, BATCH_SIZE):
        train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=False)
        test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)
        return train_dataloader, test_dataloader

    def get_your_dataset(self, num_folds, dataset, Batch_count_by_16):
        Train_Datasets_Index = []
        Test_Datasets_Index = []

        kf = KFold(n_splits=num_folds, shuffle=True)
        for train_index, test_index in kf.split(dataset):
            Train_Datasets_Index.append(train_index.tolist())
            Test_Datasets_Index.append(test_index.tolist())

        Train_Datasets_Batched = {}
        Test_Datasets_Batched = {}
        Train_Datasets = {}
        Test_Datasets = {}

        for k in range(num_folds):
            Train_Datasets[k], Test_Datasets[k] = self.pick_your_graph(dataset, Train_Datasets_Index[k],
                                                                       Test_Datasets_Index[k])

        if Batch_count_by_16 != 0:
            for i in range(num_folds):
                Train_Datasets_Batched[i], Test_Datasets_Batched[i] = self.batching_data(Train_Datasets[i],
                                                                                         Test_Datasets[i],
                                                                                         Batch_count_by_16*16)
            return Train_Datasets_Batched, Test_Datasets_Batched
        else:
            return Train_Datasets, Test_Datasets

    def Models_K_Folds(self, num_folds, your_dataset, dropout_rate, Bias, winit, act_fun):
        #num_classes = self.find_num_classes_graph_labels(your_dataset)


        if self.GNN_Model_name == "GCN_plus_GAP_Model":
            sys.path.insert(0, self.default_path+'/Models/Script')
            import GCN_plus_GAP as gcn_plus_gap_model
            Models = {}
            node_feat_size = your_dataset[0].x.size()[-1]
            gcn_input_dim = {'MUTAG': node_feat_size, 'NCI1': node_feat_size, 'ENZYMES': 16, 'Graph-SST5': 64,
                             "PROTEINS": 16, 'IsCyclic': node_feat_size}
            for i in range(num_folds):
                Models[i] = gcn_plus_gap_model.GCN_plus_GAP_Model(model_level="graph", num_classes=self.num_classes,
                                                                  GNN_layers=[node_feat_size,
                                                                              gcn_input_dim[self.dataset_name]],
                                                                  Bias=Bias, Weight_Initializer=winit, act_fun=act_fun,
                                                                  dropout_rate=dropout_rate).to(self.device)

        elif self.GNN_Model_name == "DGCNN_Model":
            sys.path.insert(0, self.default_path+'/Models/Script')
            import DGCNN as dgcnn_model
            Models = {}
            k_dgcnn = {'MUTAG': 17, 'NCI1': 32, 'ENZYMES': 32, 'Graph-SST5': 19, 'PROTEINS': 26, 'IsCyclic': 20}
            node_feat_size = your_dataset[0].x.size()[-1]
            dgcnn_input_dim = {'MUTAG': [32, 32, 32, 32], 'NCI1': [32, 32, 32, 32], 'ENZYMES': [32, 32, 32, 32],
                               'Graph-SST5': [64, 64, 64, 64], 'PROTEINS': [32, 32, 32, 32],
                               'IsCyclic': [32, 32, 32, 32]}
            for i in range(num_folds):

                Models[i] = dgcnn_model.DGCNN_Model(GNN_layers=dgcnn_input_dim[self.dataset_name], Bias=Bias,
                                                    num_classes=self.num_classes, mlp_act_fun="ReLu",
                                                    dgcnn_act_fun="tanh", Weight_Initializer=winit,
                                                    mlp_dropout_rate=dropout_rate, dgcnn_k=k_dgcnn[self.dataset_name],
                                                    node_feat_size=node_feat_size, hid_channels=[16, 32],
                                                    conv1d_kernels=[2, 5], ffn_layer_size=128,
                                                    strides=[2, 1]).to(self.device)



        elif self.GNN_Model_name == "DIFFPOOL_Model":
            sys.path.insert(0, self.default_path+'/Models/Script/Layers/')
            import Batched_GraphSage_Layer as batched_graphsage_layer
            import Batched_DIFFPOOL_Assignment as batched_diffpool_assignment
            import Batched_DIFFPOOL_Embedding as batched_diffpool_embedding
            import Batched_DIFFPOOL_Layer as batched_diffpool_layer
            sys.path.insert(0, self.default_path+'/Models/Script')
            import DIFFPOOL as diffpool_model
            Models = {}
            node_feat_size = your_dataset[0].x.size()[-1]
            for i in range(num_folds):
                Models[i] = diffpool_model.DIFFPOOL_Model(embedding_input_dim=node_feat_size,
                                                          embedding_num_block_layers=1, embedding_hid_dim=64,
                                                          new_feature_size=64, assignment_input_dim=node_feat_size,
                                                          assignment_num_block_layers=1, assignment_hid_dim=64,
                                                          max_number_of_nodes=256, prediction_hid_layers=[50],
                                                          concat_neighborhood=False, num_classes=self.num_classes,
                                                          Weight_Initializer=winit, Bias=Bias, act_fun=act_fun,
                                                          dropout_rate=dropout_rate, normalize_graphsage=False,
                                                          aggregation="mean",  concat_diffpools_outputs=True,
                                                          num_pooling=1, pooling="mean").to(self.device)

        elif self.GNN_Model_name == "GIN_Model":
            sys.path.insert(0, self.default_path+'/Models/Script/Layers/')
            import GIN_MLP_Layers as gin_mlp_layers
            sys.path.insert(0, self.default_path+'/Models/Script')
            import GIN as gin_model
            Models = {}
            node_feat_size = your_dataset[0].x.size()[-1]

            gin_input_dim = {'MUTAG': node_feat_size, 'NCI1': node_feat_size, 'ENZYMES': 16, 'Graph-SST5': 64,
                             'PROTEINS': 16, 'IsCyclic': node_feat_size}
            for i in range(num_folds):
                Models[i] = gin_model.GIN_Model(num_mlp_layers=4, Bias=Bias, num_slp_layers=2, mlp_act_fun=act_fun,
                                                mlp_input_dim=node_feat_size,
                                                mlp_hid_dim=gin_input_dim[self.dataset_name],
                                                mlp_output_dim=self.num_classes, dropout_rate=dropout_rate,
                                                joint_embeddings=False, Weight_Initializer=winit).to(self.device)

        os.chdir(self.default_path)

        return Models

    def Optimizers_K_Fold_and_LR(self, models, num_folds, lr, weight_decay):
        Optimizers = {}
        if self.GNN_Model_name == 'GCN_plus_GAP_Model':
            for i in range(num_folds):
                Optimizers[i] = torch.optim.Adam(models[i].parameters(), lr=lr, weight_decay=weight_decay)

        elif self.GNN_Model_name == "DGCNN_Model":
            for i in range(num_folds):
                Optimizers[i] = torch.optim.Adam(models[i].parameters(), lr=lr, weight_decay=weight_decay)

        elif self.GNN_Model_name == "DIFFPOOL_Model":
            for i in range(num_folds):
                Optimizers[i] = torch.optim.Adam(models[i].parameters(), lr=lr, weight_decay=weight_decay)

        elif self.GNN_Model_name == "GIN_Model":
            for i in range(num_folds):
                Optimizers[i] = torch.optim.Adam(models[i].parameters(), lr=lr, weight_decay=weight_decay)

        return Optimizers

    def gnn_train(self, EPOCHS, load_index, GNN_Model, Optimizer, Data, Loss_Function):
        GNN_Model_training_loss_per_epoch = []
        GNN_Model_training_time_per_epoch = []

        for epoch in range(EPOCHS):
            starting_time = perf_counter()
            GNN_Model_training_loss = self.train_step(GNN_Model, Optimizer, Data, Loss_Function)
            print(f'Epoch: {epoch+1:03d}, Evaluation of {self.GNN_Model_name} Model, Loss: {GNN_Model_training_loss:.4f}')
            GNN_Model_training_loss_per_epoch.append(GNN_Model_training_loss)
            ending_time = perf_counter()
            GNN_Model_training_time_per_epoch.append(ending_time - starting_time)

            if (epoch + load_index + 1) % self.LOSS_Visualization_Parameter == 0 and epoch > 0:
                self.visualize_losses(GNN_Model_training_loss_per_epoch, epoch + load_index + 1)
            if (epoch + load_index + 1) % self.Model_Saving_Parameter == 0 and epoch > 0:
                torch.save({'epoch': epoch + load_index + 1, 'model_state_dict': GNN_Model.state_dict(),
                            'optimizer_state_dict': Optimizer.state_dict(),
                            'loss': GNN_Model_training_loss_per_epoch, },
                           str(self.Explainability_name) + " on " + str(self.Task_name) + "/Model/" + str(
                               self.GNN_Model_name) + '/' + str(self.GNN_Model_name) + self.dataset_name +
                               "__model_classifier" + str(epoch + load_index + 1) + ".pt")

        return GNN_Model_training_loss_per_epoch, GNN_Model_training_time_per_epoch

    def train_step(self, GNN_Model, Optimizer, Data, Loss_Function):
        if self.GNN_Model_name == "GCN_plus_GAP_Model":
            GNN_Model_loss_batch = []
            GNN_Model.train()
            GNN_Model.zero_grad()
            for graph in Data:
                graph = graph.to(self.device)
                Output_of_Hidden_Layers, pooling_layer_output, ffn_output, soft = GNN_Model(graph, None)
                batch_loss = self.loss_calculations(soft, graph.y, Loss_Function)
                GNN_Model_loss_batch.append(batch_loss)
                batch_loss.backward()
                Optimizer.step()
            return torch.mean(torch.tensor(GNN_Model_loss_batch))

        elif self.GNN_Model_name == "DGCNN_Model":
            GNN_Model_loss_batch = []
            GNN_Model.train()
            GNN_Model.zero_grad()
            for graph in Data:
                graph = graph.to(self.device)
                final_GNN_layer_output, sortpooled_embedings, output_conv1d_1, maxpooled_output_conv1d_1, output_conv1d_2, to_dense, ffn_1, dropout_ffn_1, ffn_2, softmaxed_ffn_2 = GNN_Model(graph, None)
                batch_loss = self.loss_calculations(softmaxed_ffn_2, graph.y, Loss_Function)
                GNN_Model_loss_batch.append(batch_loss)
                batch_loss.backward()
                Optimizer.step()
            return torch.mean(torch.tensor(GNN_Model_loss_batch))

        elif self.GNN_Model_name == "DIFFPOOL_Model":
            GNN_Model_loss_batch = []
            GNN_Model.train()
            GNN_Model.zero_grad()
            for graph in Data:
                graph = graph.to(self.device)
                concatination_list_of_poolings, prediction_output_without_softmax, prediction_output = GNN_Model(graph, None)
                batch_loss = self.loss_calculations(prediction_output, graph.y, Loss_Function)
                GNN_Model_loss_batch.append(batch_loss)
                batch_loss.backward()
                Optimizer.step()
            return torch.mean(torch.tensor(GNN_Model_loss_batch))

        elif self.GNN_Model_name == "GIN_Model":
            GNN_Model_loss_batch = []
            GNN_Model.train()
            GNN_Model.zero_grad()
            for graph in Data:
                graph = graph.to(self.device)
                mlps_output_embeds, mlp_outputs_globalSUMpooled, lin1_output, lin1_output_dropouted, lin2_output, lin2_output_softmaxed = GNN_Model(graph, None)
                batch_loss = self.loss_calculations(lin2_output_softmaxed, graph.y, Loss_Function)
                GNN_Model_loss_batch.append(batch_loss)
                batch_loss.backward()
                Optimizer.step()
            return torch.mean(torch.tensor(GNN_Model_loss_batch))

        else:
            print("Model Name is not appropriate")

    def loss_calculations(self, preds, gtruth, Loss_Function):

        new_gtruth = self.multiclass_tensor_converter(gtruth)

        if Loss_Function == 1:
            loss_per_epoch1 = self.CrossEnt_criterion(preds, new_gtruth).float()
            return loss_per_epoch1

        elif Loss_Function == 2:
            loss_per_epoch2 = self.MAE_criterion(preds, new_gtruth).float()
            return loss_per_epoch2

        elif Loss_Function == 3:
            loss_per_epoch3 = self.MSE_criterion(preds, new_gtruth).float()
            return loss_per_epoch3

    def multiclass_tensor_converter(self, mylabels):
        one_hot_labels = F.one_hot(mylabels, num_classes=self.num_classes).to(torch.float32)
        return one_hot_labels

    def gnn_model_test(self, test_dataset, GNN_Model):
        GNN_Outputs = []
        GNN_Preds = []
        Real_Labels = []

        if self.GNN_Model_name == "GCN_plus_GAP_Model":
            GNN_Model.eval()

            for batch_of_graphs in test_dataset:
                batch_of_graphs = batch_of_graphs.to(self.device)
                Output_of_Hidden_Layers_test, pooling_layer_output_test, ffn_output_test, gcn_soft_test = GNN_Model(batch_of_graphs, None)
                GNN_Model_test_pred = gcn_soft_test.argmax(dim=1)
                GNN_Preds.extend(GNN_Model_test_pred.tolist())
                Real_Labels.extend(batch_of_graphs.y.tolist())

            return self.Evaluations(np.array(Real_Labels), np.array(GNN_Preds))

        elif self.GNN_Model_name == "DGCNN_Model":
            GNN_Model.eval()

            for batch_of_graphs in test_dataset:
                batch_of_graphs = batch_of_graphs.to(self.device)
                final_GNN_layer_output, sortpooled_embedings, output_conv1d_1, maxpooled_output_conv1d_1, output_conv1d_2, to_dense, ffn_1, dropout_ffn_1, ffn_2, dgcnn_softmaxed_ffn_2_test = GNN_Model(batch_of_graphs, None)
                GNN_Model_test_pred = dgcnn_softmaxed_ffn_2_test.argmax(dim=1)
                GNN_Preds.extend(GNN_Model_test_pred.tolist())
                Real_Labels.extend(batch_of_graphs.y.tolist())

            return self.Evaluations(np.array(Real_Labels), np.array(GNN_Preds))


        elif self.GNN_Model_name == "DIFFPOOL_Model":
            GNN_Model.eval()

            for batch_of_graphs in test_dataset:
                batch_of_graphs = batch_of_graphs.to(self.device)
                concatination_list_of_poolings, prediction_output_without_soft, prediction_output = GNN_Model(batch_of_graphs, None)
                GNN_Model_test_pred = prediction_output.argmax(dim=1)
                GNN_Preds.extend(GNN_Model_test_pred.tolist())
                Real_Labels.extend(batch_of_graphs.y.tolist())

            return self.Evaluations(np.array(Real_Labels), np.array(GNN_Preds))

        elif self.GNN_Model_name == "GIN_Model":
            GNN_Model.eval()

            for batch_of_graphs in test_dataset:
                batch_of_graphs = batch_of_graphs.to(self.device)
                mlps_output_embeds, mlp_outputs_globalSUMpooled, lin1_output, lin1_output_dropouted, lin2_output, lin2_output_softmaxed = GNN_Model(batch_of_graphs, None)
                GNN_Model_test_pred = lin2_output_softmaxed.argmax(dim=1)
                GNN_Preds.extend(GNN_Model_test_pred.tolist())
                Real_Labels.extend(batch_of_graphs.y.tolist())

            return self.Evaluations(np.array(Real_Labels), np.array(GNN_Preds))

    def Evaluations(self, y, pred):
        print("pred: ", pred)
        print("y: ", y)

        new_pred = self.multiclass_tensor_converter(torch.from_numpy(pred))

        auc_roc_scores = []
        auc_prc_scores = []
        precision_scores = []
        recall_scores = []
        for i in range(self.num_classes):
            aucroc = metrics.roc_auc_score(y_true=y==i, y_score=new_pred[:, i])
            auc_roc_scores.append(aucroc)

            precision, recall, thresholds = metrics.precision_recall_curve(y_true= y == i,  probas_pred=new_pred[:, i])
            auc_prc_scores.append(metrics.auc(recall, precision))

            precision_scores.append(precision)
            recall_scores.append(recall)

        AUC_ROC = mean(auc_roc_scores)
        print("AUC_ROC: ", AUC_ROC)

        AUC_PRC = mean(auc_prc_scores)
        print("AUC_PRC: ", AUC_PRC)

        precision_scores = metrics.precision_score(y_true=y, y_pred=pred, average=None, zero_division=0)
        print("Precision: ", precision_scores)

        ACC_Quantitative = metrics.accuracy_score(y, pred.round(), normalize=False)
        print("ACC_Quantitative: ", ACC_Quantitative)
        ACC_Probabilistic = metrics.accuracy_score(y, pred.round(), normalize=True)
        print("ACC_Probabilistic: ", ACC_Probabilistic)

        return AUC_ROC, AUC_PRC, ACC_Quantitative, ACC_Probabilistic

    def describe_trained_model(self, act_fun):
        if self.GNN_Model_name == "GCN_plus_GAP_Model":
            if act_fun == 'ReLu':
                return str("ReLu(GCN1)+ReLu(GCN2)+GAP+FFN")
            elif act_fun == 'eLu':
                return str("eLu(GCN1)+eLu(GCN2)+GAP+FFN")
        elif self.GNN_Model_name == "DGCNN_Model":
            return str("4 * tanh(DGCNN) + 2 * relu(Conv1d) + Sortpooling + 2 * relu(Linear)")
        elif self.GNN_Model_name == "DIFFPOOL_Model":
            return str("2 * DIFFPOOL + 2 * GRAPHSAGE + 3 * Linear")
        elif self.GNN_Model_name == "GIN_Model":
            return str("5MLPs(1Linear+BNorm+1Linear) + GlobalSUMPooling + 2Linears")
        else:
            return str("Please specify model type")

    def recording_csv_files(self, config_list):
        if config_list[10] == 1:
            config_list[10] = "CrossEntropyLoss"
        elif config_list[10] == 2:
            config_list[10] = "MAE"
        elif config_list[10] == 3:
            config_list[10] = "MSE"

        if config_list[3] == 1:
            config_list[3] = "xavier_normal_"
        elif config_list[3] == 2:
            config_list[3] = "kaiming_normal_"
        elif config_list[3] == 3:
            config_list[3] = "normal_"

        if not os.path.exists(str(self.csv_files_path) + "/" + str(self.Task_name) + "/" + str(self.GNN_Model_name) +
                              "/csv_files/" + str(self.GNN_Model_name) + "_HyperParameters_" + str(self.dataset_name) +
                              "_Dataset.csv"):
            Header = np.array(["Datset", "Model Name", "Model Description", "Weight_Initializer", "K-Fold", "Epochs",
                               "LR", "DropOut Rate", "Bias", "Batch Size", "Loss Function", "AUC-ROC", "AUC-PRC", "Acc",
                               "Avg Runing Time[Epochs]"])
            with open(str(self.csv_files_path) + "/" + str(self.Task_name) + "/" + str(self.GNN_Model_name) +
                      "/csv_files/" + str(self.GNN_Model_name) + "_HyperParameters_" + str(self.dataset_name) +
                      "_Dataset.csv", 'w') as outcsv:
               writer = csv.DictWriter(outcsv, fieldnames=Header)
               writer.writeheader()

        with open(str(self.csv_files_path) + "/" + str(self.Task_name) + "/" + str(self.GNN_Model_name) +
                  "/csv_files/" + str(self.GNN_Model_name) + "_HyperParameters_" + str(self.dataset_name) +
                  "_Dataset.csv", 'a') as outcsv:
            wr = csv.writer(outcsv, dialect='excel', delimiter=',')
            wr.writerow(config_list)

    def visualize_losses(self, gcn_losses, epoch_history):
        gcn_losses_list = torch.stack(gcn_losses).cpu().detach().numpy()

        fig = plt.figure(figsize=(27, 20))

        ax = plt.subplot2grid((3, 1), (0, 0), colspan=1)
        plt.xlabel('Epochs')
        plt.ylabel('Loss')
        plt.title(str(self.GNN_Models_Name) + " Model Loss in Epoch: " + str(epoch_history))
        ax.plot(gcn_losses_list, color='r')

        plt.savefig(str(self.csv_files_path) + "/" + str(self.Task_name) + "/" + str(self.GNN_Model_name) +
                    "/figures/" + str(self.GNN_Model_name) + '__Loss_til_epoch_{:04d}.png'.format(epoch_history))
        # plt.show()
        plt.close()

    def Final_OutComes_by_fig(self):
        print("Best result: {}; f(x) = {:.3f}.".format(self.meta_optimizer.max["params"],
                                                       self.meta_optimizer.max["target"]))
        print()
        print()
        plt.figure(figsize=(15, 5))
        plt.plot(range(1, 1 + len(self.meta_optimizer.space.target)), self.meta_optimizer.space.target, "-o",
                 color='red')
        plt.grid(True)
        plt.xlabel("Iteration", fontsize=14)
        plt.ylabel("AUC-ROC f(x)", fontsize=14)
        plt.xticks(fontsize=14)
        plt.yticks(fontsize=14)
        plt.savefig(self.default_path + str(self.Explainability_name) + ' on ' + str(self.Task_name) +
                    '/Experimental Results/' + str(self.GNN_Model_name) + "/" + str(self.GNN_Model_name) +
                    '__AUC_ROC_BHPO_HyperParameter.png')
        # plt.show()
        plt.close()


DataSet_name = "IsCyclic"
if DataSet_name == "Graph-SST5":
    from dig.xgraph.dataset import SentiGraphDataset
    py_path = '/data/cs.aau.dk/ey33jw/'
    os.chdir(py_path)
    current_directory = os.getcwd()
    entire_dataset = SentiGraphDataset(root='./Datasets_for_Explainability_Methods/', name='Graph-SST5')
    py_path = '/data/cs.aau.dk/ey33jw/Explainability_Methods/'
    os.chdir(py_path)
    # current_directory = os.getcwd()
elif DataSet_name == "IsCyclic":
    with open("/data/cs.aau.dk/ey33jw/Datasets_for_Explainability_Methods/IsCyclic/iscyclic_graphs.pkl", 'rb') as f:
        entire_dataset = pickle.load(f)
else:
    entire_dataset = TUDataset(root='data/TUDataset', name=DataSet_name)


#.  GNN_Models_Name = dict([(1, "GCN_plus_GAP"), (2, "DGCNN"), (3, "DIFFPOOL"), (4, "GIN")])

hyp_opt_gnns = GNN_Models_HyperParameter_Optimization(GNN_Model_index=1, meta_learning_epochs=100, act_fun="ReLu",
                                                      input_dataset=entire_dataset, dataset_name=DataSet_name,
                                                      weight_decay=1e-6)

hyp_opt_gnns()
