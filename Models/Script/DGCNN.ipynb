{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"L4","authorship_tag":"ABX9TyP0PXZhYWXaTjksHwulp8tL"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["# Install required packages.\n","import os\n","import torch\n","os.environ['TORCH'] = torch.__version__\n","print(torch.__version__)\n","\n","!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n","!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n","!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git\n","!pip install torch-cluster -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n","!pip install --upgrade scipy networkx numpy"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H1e1g89hAZOu","executionInfo":{"status":"ok","timestamp":1715952298929,"user_tz":-120,"elapsed":51581,"user":{"displayName":"Ehsan Mobaraki","userId":"06314958514195310048"}},"outputId":"e53d43f3-738f-4716-f18f-dadf0a4ce489"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["2.2.1+cu121\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m81.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m48.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for torch-geometric (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Looking in links: https://data.pyg.org/whl/torch-2.2.1+cu121.html\n","Collecting torch-cluster\n","  Downloading https://data.pyg.org/whl/torch-2.2.0%2Bcu121/torch_cluster-1.6.3%2Bpt22cu121-cp310-cp310-linux_x86_64.whl (3.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m36.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-cluster) (1.11.4)\n","Requirement already satisfied: numpy<1.28.0,>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from scipy->torch-cluster) (1.25.2)\n","Installing collected packages: torch-cluster\n","Successfully installed torch-cluster-1.6.3+pt22cu121\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (1.11.4)\n","Collecting scipy\n","  Downloading scipy-1.13.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m44.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (3.3)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.25.2)\n","Collecting numpy\n","  Downloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m74.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: numpy, scipy\n","  Attempting uninstall: numpy\n","    Found existing installation: numpy 1.25.2\n","    Uninstalling numpy-1.25.2:\n","      Successfully uninstalled numpy-1.25.2\n","  Attempting uninstall: scipy\n","    Found existing installation: scipy 1.11.4\n","    Uninstalling scipy-1.11.4:\n","      Successfully uninstalled scipy-1.11.4\n","Successfully installed numpy-1.26.4 scipy-1.13.0\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EhDbTlTCAhra","executionInfo":{"status":"ok","timestamp":1715952315439,"user_tz":-120,"elapsed":16516,"user":{"displayName":"Ehsan Mobaraki","userId":"06314958514195310048"}},"outputId":"da51a2ee-d846-422c-e5a1-239155dd0a3a"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":3,"metadata":{"id":"1sUoQrYwAR94","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1715952329093,"user_tz":-120,"elapsed":11234,"user":{"displayName":"Ehsan Mobaraki","userId":"06314958514195310048"}},"outputId":"33b08efa-8cff-4c83-c13c-2ae9b8704a4c"},"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading https://www.chrsmrrs.com/graphkerneldatasets/MUTAG.zip\n","Processing...\n","Done!\n"]},{"output_type":"stream","name":"stdout","text":["tanh is Selected.\n","tanh is Selected.\n","lin2_output_softmaxed:  torch.Size([1, 2])\n","tensor([[0.4994, 0.5006]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n"]}],"source":["import torch\n","import torch.nn as nn\n","from torch.autograd import Variable\n","import math\n","import torch.nn.functional as F\n","from torch.nn.parameter import Parameter\n","from torch_geometric.utils.convert import to_scipy_sparse_matrix\n","#from torch_geometric.utils.train_test_split_edges import torch_geometric\n","#torch_geometric.transforms.RandomLinkSplit\n","import torch_geometric\n","import networkx as nx\n","import numpy as np\n","from torch_geometric.nn import GCNConv\n","import sys\n","from torch_geometric.loader import DataLoader\n","from torch_geometric.datasets import TUDataset\n","from scipy.sparse import csr_matrix\n","py_path = '/content/drive/MyDrive/Explainability Methods/Models/Script/Layers/'\n","sys.path.insert(0,py_path)\n","import matrix_util as Mat_Util\n","import DGCNN_layer as dgcnn_layer\n","import DGCNN_GNN_Layers as dgcnn_gnn_layers\n","import SortPooling_Layer as sortpooling_layer\n","import MLP_DGCNN as mlp_dgcnn\n","\n","class GlobalMeanPool(nn.Module):\n","\n","    def __init__(self):\n","        super().__init__()\n","\n","    def forward(self, x, batch):\n","        return gnn.global_mean_pool(x, batch)\n","################################################################################\n","class IdenticalPool(nn.Module):\n","\n","    def __init__(self):\n","        super().__init__()\n","\n","    def forward(self, x, batch):\n","        return x\n","\n","################################################################################\n","class DGCNN_Model(nn.Module):\n","    '''\n","        DGCNN Layers using sparse adjacency matrix\n","    '''\n","    def __init__(self, GNN_layers, mlp_act_fun, dgcnn_act_fun, mlp_dropout_rate, Weight_Initializer, Bias, num_classes, dgcnn_k,\n","                 node_feat_size, hid_channels, conv1d_kernels, ffn_layer_size, strides):\n","\n","        super(DGCNN_Model, self).__init__()\n","        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","        self.GNN_layers = GNN_layers\n","        self.output_dim = GNN_layers[-1]\n","        self.num_GNN_layers = len(GNN_layers)\n","        self.mlp_dropout_rate = mlp_dropout_rate\n","        self.Bias = Bias\n","        self.Weight_Initializer = Weight_Initializer\n","        self.dgcnn_k = dgcnn_k\n","        self.node_feat_size = node_feat_size\n","        self.num_classes = num_classes\n","        self.hid_channels = hid_channels\n","        self.conv1d_kernels = conv1d_kernels\n","        self.ffn_layer_size = ffn_layer_size\n","        self.strides = strides\n","\n","        self.gnn_layers = dgcnn_gnn_layers.dgcnn_gnn_layers(GNN_layers=self.GNN_layers, node_feat_size=self.node_feat_size,\n","                                                            Bias=self.Bias, dgcnn_act_fun=dgcnn_act_fun).to(self.device)\n","\n","        self.sort_pool = sortpooling_layer.SortPooling(self.dgcnn_k, node_feat_size=self.node_feat_size).to(self.device)\n","\n","        self.classic_conv = mlp_dgcnn.MLP_DGCNN(num_class=self.num_classes, last_gnn_layer_dim=self.GNN_layers[-1],\n","                                                mlp_act_fun=mlp_act_fun, dropout_rate=self.mlp_dropout_rate,\n","                                                hid_channels=self.hid_channels, conv1d_kernels=self.conv1d_kernels,\n","                                                dgcnn_k=self.dgcnn_k, ffn_layer_size=self.ffn_layer_size, Bias=self.Bias,\n","                                                strides=self.strides).to(self.device)\n","        if dgcnn_act_fun == 'ReLu':\n","            self.dgcnn_act_fun = F.relu\n","            print('ReLu is Selected.')\n","        elif dgcnn_act_fun == 'eLu':\n","            self.dgcnn_act_fun = nn.functional.elu\n","            print('eLu is Selected.')\n","        elif dgcnn_act_fun == 'tanh':\n","            self.dgcnn_act_fun = torch.tanh\n","            print('tanh is Selected.')\n","\n","\n","\n","        mean = 0\n","        std = 0.1\n","        #self.initialize_weights(Weight_Initializer, Bias, mean, std)\n","\n","\n","    def initialize_weights(model, Weight_Initializer, Bias, mean, std):\n","        # 1. Xavier Normal_.  2. Kaiming Normal_.  3. Uniform (0,0.1std)\n","        if Weight_Initializer == 1:                                             #.      1. Xavier Normal_.\n","            for i,layers in enumerate(model.children()):\n","                if isinstance(layers, dgcnn_gnn_layers.dgcnn_gnn_layers):\n","                    for j, layer in enumerate(layers.modules()):\n","                        if isinstance(layer, nn.Linear):\n","                            torch.nn.init.xavier_normal_(layer.weight.data)\n","                            if Bias:\n","                                layer.bias.data.zero_()\n","                        if isinstance(layer, dgcnn_layer.GNN_DGCNN):\n","                            torch.nn.init.xavier_normal_(layer.conv_params.weight)\n","                            if Bias:\n","                                layer.conv_params.bias.data.zero_()\n","                        else:\n","                            pass\n","                if isinstance(layers, torch.nn.Linear):\n","                    torch.nn.init.xavier_normal_(layers.weight)\n","                    if Bias:\n","                        layers.bias.data.zero_()\n","                if isinstance(layers, (mlp_dgcnn.MLP_DGCNN)):\n","                    torch.nn.init.xavier_normal_(layers.conv1d_1.weight)\n","                    torch.nn.init.xavier_normal_(layers.conv1d_2.weight)\n","                    torch.nn.init.xavier_normal_(layers.linear1.weight)\n","                    torch.nn.init.xavier_normal_(layers.linear2.weight)\n","\n","                elif isinstance(layers, (GlobalMeanPool)):\n","                    pass\n","                elif isinstance(layers, (IdenticalPool)):\n","                    pass\n","\n","        if Weight_Initializer == 2:                                             #.      2. Kaiming Normal_.\n","            for i,layers in enumerate(model.children()):\n","                if isinstance(layers, dgcnn_gnn_layers.dgcnn_gnn_layers):\n","                    for j, layer in enumerate(layers.modules()):\n","                        if isinstance(layer, nn.Linear):\n","                            torch.nn.init.kaiming_normal_(layer.weight.data)\n","                            if Bias:\n","                                layer.bias.data.zero_()\n","                        if isinstance(layer, dgcnn_layer.GNN_DGCNN):\n","                            torch.nn.init.kaiming_normal_(layer.conv_params.weight)\n","                            if Bias:\n","                                layer.conv_params.bias.data.zero_()\n","                        else:\n","                            pass\n","                if isinstance(layers, torch.nn.Linear):\n","                    torch.nn.init.kaiming_normal_(layers.weight)\n","                    if Bias:\n","                        layers.bias.data.zero_()\n","                if isinstance(layers, (mlp_dgcnn.MLP_DGCNN)):\n","                    torch.nn.init.kaiming_normal_(layers.conv1d_1.weight)\n","                    torch.nn.init.kaiming_normal_(layers.conv1d_2.weight)\n","                    torch.nn.init.kaiming_normal_(layers.linear1.weight)\n","                    torch.nn.init.kaiming_normal_(layers.linear2.weight)\n","\n","                elif isinstance(layers, (GlobalMeanPool)):\n","                    pass\n","                elif isinstance(layers, (IdenticalPool)):\n","                    pass\n","\n","        if Weight_Initializer == 3:                                             #.      3. Uniform (0,0.1std)\n","            for i,layers in enumerate(model.children()):\n","                if isinstance(layers, dgcnn_gnn_layers.dgcnn_gnn_layers):\n","                    for j, layer in enumerate(layers.modules()):\n","                        #print(\"here2\")\n","                        if isinstance(layer, nn.Linear):\n","                            torch.nn.init.normal_(layer.weight.data, mean, std)\n","                            if Bias:\n","                                layer.bias.data.zero_()\n","                        if isinstance(layer, dgcnn_layer.GNN_DGCNN):\n","                            torch.nn.init.normal_(layer.conv_params.weight, mean, std)\n","                            if Bias:\n","                                layer.conv_params.bias.data.zero_()\n","                        else:\n","                            pass\n","                if isinstance(layers, torch.nn.Linear):\n","                    torch.nn.init.normal_(layers.weight, mean, std)\n","                    if Bias:\n","                        layers.bias.data.zero_()\n","                if isinstance(layers, (mlp_dgcnn.MLP_DGCNN)):\n","                    torch.nn.init.normal_(layers.conv1d_1.weight, mean, std)\n","                    torch.nn.init.normal_(layers.conv1d_2.weight, mean, std)\n","                    torch.nn.init.normal_(layers.linear1.weight, mean, std)\n","                    torch.nn.init.normal_(layers.linear2.weight, mean, std)\n","                elif isinstance(layers, (GlobalMeanPool)):\n","                    pass\n","                elif isinstance(layers, (IdenticalPool)):\n","                    pass\n","\n","\n","    def forward(self, graph, edge_mask):\n","\n","        if graph.batch is not None:\n","            graph_sizes = [len(graph[i].x) for i in range(len(graph))]\n","        else:\n","            graph_sizes = [len(graph.x)]\n","\n","\n","        Output_of_GNN_Layers = self.gnn_layers(graph, edge_mask).to(self.device)\n","\n","\n","\n","        Output_of_GNN_Layers.retain_grad()\n","\n","\n","        sortpooled_embedings = self.sort_pool(output_of_dgcnn_layer=Output_of_GNN_Layers, batch_graphs=graph)\n","\n","        sortpooled_embedings.retain_grad()\n","\n","\n","        output_conv1d_1, maxpooled_output_conv1d_1, output_conv1d_2, to_dense, ffn_1, dropout_ffn_1, ffn_2, softmaxed_ffn_2 = self.classic_conv(sortpooled_embedings=sortpooled_embedings, graph_sizes=graph_sizes)\n","\n","        return Output_of_GNN_Layers, sortpooled_embedings, output_conv1d_1, maxpooled_output_conv1d_1, output_conv1d_2, to_dense, ffn_1, dropout_ffn_1, ffn_2, softmaxed_ffn_2\n","\n","\n","\n","\n","\n","\n","#node_feat_size = len(dataset[0].x[0])\n","#k=17\n","#dgcnn_model_example = DGCNN_Model(GNN_layers=[32, 32, 32, 7], num_classes=2, mlp_act_fun='ReLu', dgcnn_act_fun='tanh', mlp_dropout_rate=0.5, Weight_Initializer=3, Bias=False, dgcnn_k=k, node_feat_size=node_feat_size, hid_channels=[16,32], conv1d_kernels=[2,5], ffn_layer_size=128, strides=[2,1])\n","\n","#print(dgcnn_model_example)\n","#final_GNN_layer_output, sortpooled_embedings, output_conv1d_1, maxpooled_output_conv1d_1, output_conv1d_2, to_dense, output_h1, dropout_output_h1, output_h2, softmaxed_h2 = dgcnn_model_example(dataset[0], None)\n","#print(softmaxed_h2)\n","\n","dataset = TUDataset(root='data/TUDataset', name='MUTAG')\n","batch_size = 2\n","k = 17\n","node_feat_size = len(dataset[0].x[0])\n","batched_dataset = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n","dgcnn_model_example = DGCNN_Model(GNN_layers=[32, 32, 32, 7], num_classes=2, mlp_act_fun='ReLu', dgcnn_act_fun='tanh', mlp_dropout_rate=0.5, Weight_Initializer=3, Bias=False, dgcnn_k=k, node_feat_size=node_feat_size, hid_channels=[16,32], conv1d_kernels=[2,5], ffn_layer_size=128, strides=[2,1])\n","import random\n","em = [random.uniform(0, 1) for i in range(len(dataset[0].edge_index[0]))]\n","\n","for batched_graphs in dataset:\n","    em = [random.uniform(0, 1) for i in range(len(batched_graphs.edge_index[0]))]\n","    #x, edge_index, batch, y = batched_graphs.x, batched_graphs.edge_index, batched_graphs.batch, batched_graphs.y\n","    final_GNN_layer_output, sortpooled_embedings, output_conv1d_1, maxpooled_output_conv1d_1, output_conv1d_2, to_dense, output_h1, dropout_output_h1, output_h2, softmaxed_h2 = dgcnn_model_example(batched_graphs, None)\n","    print(\"lin2_output_softmaxed: \", softmaxed_h2.size())\n","    print(softmaxed_h2)\n","    break\n","\n","#node_feat_size = len(dataset[0].x[0])\n","#k=20\n","#dgcnn_model_example = DGCNN_Model(GNN_layers=[32, 32, 32, 7], num_classes=2, mlp_act_fun='ReLu', dgcnn_act_fun='tanh', mlp_dropout_rate=0.5, Weight_Initializer=3, Bias=False, dgcnn_k=k, node_feat_size=node_feat_size, hid_channels=[16,32], conv1d_kernels=[2,5], ffn_layer_size=128, strides=[2,1])\n","\n","#print(dgcnn_model_example)\n","#final_GNN_layer_output, sortpooled_embedings, output_conv1d_1, maxpooled_output_conv1d_1, output_conv1d_2, to_dense, output_h1, dropout_output_h1, output_h2, softmaxed_h2 = dgcnn_model_example(dataset[0])\n","#print(softmaxed_h2)"]},{"cell_type":"code","source":["graph_example = dataset[0]\n","em = [random.uniform(0, 1) for i in range(len(graph_example.edge_index[0]))]\n","x = to_scipy_sparse_matrix(graph_example.edge_index).multiply(1)\n","row_A = np.array(graph_example.edge_index[0])\n","col_A = np.array(graph_example.edge_index[1])\n","data_A = np.array(em)\n","\n","csrMatrix_A = csr_matrix((data_A, (row_A, col_A)))\n","print(csrMatrix_A)\n","\n","\n","\n","a = csr_matrix((np.array(em), (np.array(graph_example.edge_index[0]), np.array(graph_example.edge_index[1]))))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"w5x1U3WKP_LX","executionInfo":{"status":"ok","timestamp":1686434648011,"user_tz":-120,"elapsed":7,"user":{"displayName":"Ehsan Mobaraki","userId":"06314958514195310048"}},"outputId":"a145089e-4d24-492f-a149-8ca612e795b6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["  (0, 1)\t0.035092147772491145\n","  (0, 5)\t0.7143755231591802\n","  (1, 0)\t0.7163840463028429\n","  (1, 2)\t0.4052103474807921\n","  (2, 1)\t0.4759074134777955\n","  (2, 3)\t0.9941871848865929\n","  (3, 2)\t0.4848262557143126\n","  (3, 4)\t0.23738776448995058\n","  (3, 9)\t0.9362373626887639\n","  (4, 3)\t0.32927258797738346\n","  (4, 5)\t0.22334926882361894\n","  (4, 6)\t0.7925484914913264\n","  (5, 0)\t0.8428135088426392\n","  (5, 4)\t0.5043848497923586\n","  (6, 4)\t0.7822545594070558\n","  (6, 7)\t0.3032604136659791\n","  (7, 6)\t0.023906806331804975\n","  (7, 8)\t0.5925565381607069\n","  (8, 7)\t0.1494193696025985\n","  (8, 9)\t0.7413661178556381\n","  (8, 13)\t0.9390891329608302\n","  (9, 3)\t0.17248293721094798\n","  (9, 8)\t0.006033802543148892\n","  (9, 10)\t0.19392451338635108\n","  (10, 9)\t0.06388803589475378\n","  (10, 11)\t0.16226032350304198\n","  (11, 10)\t0.4182358332682746\n","  (11, 12)\t0.31465224942320935\n","  (12, 11)\t0.8111245893280435\n","  (12, 13)\t0.7622638644632717\n","  (12, 14)\t0.9620399981035909\n","  (13, 8)\t0.4861888369259414\n","  (13, 12)\t0.910748003872269\n","  (14, 12)\t0.28029369087168854\n","  (14, 15)\t0.6560947246254943\n","  (14, 16)\t0.9479829006469124\n","  (15, 14)\t0.058937353198761366\n","  (16, 14)\t0.8233181540450858\n"]}]},{"cell_type":"code","source":["def get_hyperparameter_K_for_DGCNN(dataset):\n","    num_nodes_list = []\n","    for graph in dataset:\n","        num_nodes_list.append(len(graph.x))\n","    #print(num_nodes_list)\n","    #print(max(num_nodes_list))\n","    my_threshold = [[],[]]\n","    for threshold_size in range(max(num_nodes_list)):\n","        #x = [i for i in num_nodes_list if i > size]\n","        candidates = []\n","        for s in num_nodes_list:\n","            if s >= threshold_size:\n","                candidates.append(s)\n","                if round(len(candidates)/len(num_nodes_list), 0) >= 0.6:\n","                    my_threshold[0].append(threshold_size) # threshold values\n","                    my_threshold[1].append(round(len(candidates)/len(num_nodes_list), 0) >= 0.6) # fraction rate\n","                    #print(\"threshold_size: \", threshold_size, )\n","                    break\n","                else:\n","                    pass\n","\n","        #print(round(len(candidates)/len(num_nodes_list)*100, 0), \"%\", \"have more node count than \", threshold_size)\n","    return my_threshold[0][-1]\n","\n","K_DGCNN_HyperParameter = get_hyperparameter_K_for_DGCNN(dataset)\n","print(K_DGCNN_HyperParameter)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"n6hT_9XRXl1T","executionInfo":{"status":"ok","timestamp":1686401612295,"user_tz":-120,"elapsed":253,"user":{"displayName":"Ehsan Mobaraki","userId":"06314958514195310048"}},"outputId":"a80eda6e-78ad-4366-9591-b3c355026087"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["17\n"]}]}]}