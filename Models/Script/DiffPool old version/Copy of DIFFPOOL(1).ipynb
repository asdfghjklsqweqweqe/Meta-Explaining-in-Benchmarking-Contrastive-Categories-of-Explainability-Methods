{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOPTV1t02deZ97NCiDdTYl4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import os\n","import torch\n","os.environ['TORCH'] = torch.__version__\n","print(torch.__version__)\n","\n","!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n","!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n","!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZVT1irXhrkKG","outputId":"3f3b7d78-594a-4072-b5c3-f04c0bfd335d","executionInfo":{"status":"ok","timestamp":1687466185569,"user_tz":-120,"elapsed":39362,"user":{"displayName":"Ehsan Mobaraki","userId":"06314958514195310048"}}},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["2.0.1+cu118\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for torch_geometric (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"P0NjcUIbr5XI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1687468955889,"user_tz":-120,"elapsed":18110,"user":{"displayName":"Ehsan Mobaraki","userId":"06314958514195310048"}},"outputId":"7f33e826-d78d-44df-9748-f24a88ad4089"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":3,"metadata":{"id":"bQ3lraKjKUh1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1687472080466,"user_tz":-120,"elapsed":5092,"user":{"displayName":"Ehsan Mobaraki","userId":"06314958514195310048"}},"outputId":"6542cae4-663e-441a-9c8b-4bb7830f9664"},"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading https://www.chrsmrrs.com/graphkerneldatasets/MUTAG.zip\n","Extracting data/TUDataset/MUTAG/MUTAG.zip\n","Processing...\n"]},{"output_type":"stream","name":"stdout","text":["new_num_nodes:  10\n","ReLu is Selected.\n","Final Output Size:  torch.Size([3, 2])\n","Final Output:  tensor([[0.4960, 0.5040],\n","        [0.4978, 0.5022],\n","        [0.5000, 0.5000]], grad_fn=<SqueezeBackward0>)\n"]},{"output_type":"stream","name":"stderr","text":["Done!\n"]}],"source":["from torch_geometric.utils import dropout\n","from torch_geometric.loader import DataLoader\n","import torch\n","import torch.nn as nn\n","from torch.autograd import Variable\n","import math\n","import torch.nn.functional as F\n","from torch.nn.parameter import Parameter\n","from torch_geometric.utils.convert import to_scipy_sparse_matrix\n","from torch_geometric.utils.train_test_split_edges import torch_geometric\n","import networkx as nx\n","import numpy as np\n","from torch_geometric.nn import GCNConv\n","import sys\n","from torch_geometric.datasets import TUDataset\n","from scipy.sparse import csr_matrix\n","py_path = '/content/drive/MyDrive/Explainability Methods/Models/Script/Layers/'\n","sys.path.insert(0,py_path)\n","import Batched_GraphSage_Layer as batched_graphsage_layer\n","import Batched_DIFFPOOL_Assignment as batched_diffpool_assignment\n","import Batched_DIFFPOOL_Embedding as batched_diffpool_embedding\n","import Batched_DIFFPOOL_Layer as batched_diffpool_layer\n","\n","\n","\n","class GlobalMeanPool(nn.Module):\n","\n","    def __init__(self):\n","        super().__init__()\n","\n","    def forward(self, x, batch):\n","        return gnn.global_mean_pool(x, batch)\n","################################################################################\n","class IdenticalPool(nn.Module):\n","\n","    def __init__(self):\n","        super().__init__()\n","\n","    def forward(self, x, batch):\n","        return x\n","\n","################################################################################\n","class DIFFPOOL_Model(nn.Module):\n","    '''\n","        DIFFPOOL Model\n","    '''\n","    def __init__(self, diffpool_layers_dim, diffpool_layers_new_num_nodes, Weight_Initializer, Bias, num_classes, dropout_rate, normalize_embedding, aggregation, act_fun):\n","\n","        super(DIFFPOOL_Model, self).__init__()\n","        self.diffpool_layers_dim = diffpool_layers_dim\n","        self.diffpool_layers_new_num_nodes = diffpool_layers_new_num_nodes\n","        self.dropout_rate = dropout_rate\n","        self.Bias = Bias\n","        self.normalize_embedding = normalize_embedding\n","        self.aggregation = aggregation\n","        self.num_classes = num_classes\n","\n","\n","        self.diffpool_layer_1 = batched_diffpool_layer.Batched_DiffPool_Layer(input_dim_size=self.diffpool_layers_dim[0][0], new_feat_dim_size=self.diffpool_layers_dim[0][1], new_num_nodes=self.diffpool_layers_new_num_nodes[0], Bias=self.Bias, normalize_embedding=self.normalize_embedding, dropout=self.dropout_rate, aggregation=self.aggregation)\n","\n","        self.graph_sage_1 = batched_graphsage_layer.GNN_Batched_GraphSage_Layer(input_dim=self.diffpool_layers_dim[0][1], output_dim=self.diffpool_layers_dim[1][0], Bias=self.Bias, normalize_embedding=self.normalize_embedding, dropout=self.dropout_rate, aggregation=self.aggregation)\n","\n","        self.diffpool_layer_2 = batched_diffpool_layer.Batched_DiffPool_Layer(input_dim_size=self.diffpool_layers_dim[1][0], new_feat_dim_size=self.diffpool_layers_dim[1][1], new_num_nodes=self.diffpool_layers_new_num_nodes[1], Bias=self.Bias, normalize_embedding=self.normalize_embedding, dropout=self.dropout_rate, aggregation=self.aggregation)\n","\n","        self.graph_sage_2 = batched_graphsage_layer.GNN_Batched_GraphSage_Layer(input_dim=self.diffpool_layers_dim[1][1], output_dim=self.diffpool_layers_dim[1][1], Bias=self.Bias, normalize_embedding=self.normalize_embedding, dropout=self.dropout_rate, aggregation=self.aggregation)\n","\n","        self.lin1 = torch.nn.Linear(in_features=self.diffpool_layers_dim[1][1], out_features=self.diffpool_layers_dim[1][1])\n","        self.lin2 = torch.nn.Linear(in_features=self.diffpool_layers_dim[1][1], out_features=self.diffpool_layers_dim[1][1])\n","        self.lin3 = torch.nn.Linear(in_features=self.diffpool_layers_dim[1][1], out_features=self.num_classes)\n","\n","        if act_fun == 'ReLu':\n","            self.act_fun = F.relu\n","            print('ReLu is Selected.')\n","        elif act_fun == 'eLu':\n","            self.act_fun = nn.functional.elu\n","            print('eLu is Selected.')\n","        elif act_fun == 'tanh':\n","            self.act_fun = torch.tanh\n","            print('tanh is Selected.')\n","        self.act_fun_softmax = F.softmax\n","\n","\n","        mean = 0\n","        std = 0.1\n","        self.initialize_weights(Weight_Initializer, Bias, mean, std)\n","\n","\n","    def initialize_weights(model, Weight_Initializer, Bias, mean, std):\n","        # 1. Xavier Normal_.  2. Kaiming Normal_.  3. Uniform (0,0.1std)\n","        if Weight_Initializer == 1:                                             #.      1. Xavier Normal_.\n","            for i,layers in enumerate(model.children()):\n","                if isinstance(layers, torch.nn.ModuleList):\n","                    for j, layer in enumerate(layers.modules()):\n","                        if isinstance(layer, batched_graphsage_layer.GNN_Batched_GraphSage_Layer):\n","                            torch.nn.init.xavier_normal_(layer.learnable_weights.weight)\n","                            if Bias:\n","                                layer.learnable_weights.bias.data.zero_()\n","                        else:\n","                            pass\n","                elif isinstance(layers, batched_graphsage_layer.GNN_Batched_GraphSage_Layer):\n","                    torch.nn.init.xavier_normal_(layers.learnable_weights.weight)\n","                    if Bias:\n","                        layers.learnable_weights.bias.data.zero_()\n","                elif isinstance(layers, batched_diffpool_layer.Batched_DiffPool_Layer):\n","                    torch.nn.init.xavier_normal_(layers.new_assign.assinment_layer.learnable_weights.weight)\n","                    torch.nn.init.xavier_normal_(layers.new_embed.embedding_layer.learnable_weights.weight)\n","                    if Bias:\n","                        torch.nn.init.zeros_(layers.new_assign.assinment_layer.learnable_weights.bias)\n","                        torch.nn.init.zeros_(layers.new_embed.embedding_layer.learnable_weights.bias)\n","                elif isinstance(layers, torch.nn.Linear):\n","                    torch.nn.init.xavier_normal_(layers.weight)\n","                    if Bias:\n","                        torch.nn.init.zeros_(layers.bias)\n","\n","\n","        if Weight_Initializer == 2:                                             #.      2. Kaiming Normal_.\n","            for i,layers in enumerate(model.children()):\n","                if isinstance(layers, torch.nn.ModuleList):\n","                    for j, layer in enumerate(layers.modules()):\n","                        if isinstance(layer, batched_graphsage_layer.GNN_Batched_GraphSage_Layer):\n","                            torch.nn.init.kaiming_normal_(layer.learnable_weights.weight)\n","                            if Bias:\n","                                layer.learnable_weights.bias.data.zero_()\n","                        else:\n","                            pass\n","                elif isinstance(layers, batched_graphsage_layer.GNN_Batched_GraphSage_Layer):\n","                    torch.nn.init.kaiming_normal_(layers.learnable_weights.weight)\n","                    if Bias:\n","                        layers.learnable_weights.bias.data.zero_()\n","                elif isinstance(layers, batched_diffpool_layer.Batched_DiffPool_Layer):\n","                    torch.nn.init.kaiming_normal_(layers.new_assign.assinment_layer.learnable_weights.weight)\n","                    torch.nn.init.kaiming_normal_(layers.new_embed.embedding_layer.learnable_weights.weight)\n","                    if Bias:\n","                        torch.nn.init.zeros_(layers.new_assign.assinment_layer.learnable_weights.bias)\n","                        torch.nn.init.zeros_(layers.new_embed.embedding_layer.learnable_weights.bias)\n","                elif isinstance(layers, torch.nn.Linear):\n","                    torch.nn.init.kaiming_normal_(layers.weight)\n","                    if Bias:\n","                        torch.nn.init.zeros_(layers.bias)\n","\n","\n","        if Weight_Initializer == 3:                                             #.      3. Uniform (0,0.1std)\n","            for i,layers in enumerate(model.children()):\n","                if isinstance(layers, torch.nn.ModuleList):\n","                    for j, layer in enumerate(layers.modules()):\n","                        if isinstance(layer, batched_graphsage_layer.GNN_Batched_GraphSage_Layer):\n","                            torch.nn.init.normal_(layer.learnable_weights.weight.data, mean, std)\n","                            if Bias:\n","                                layer.learnable_weights.bias.data.zero_()\n","                        else:\n","                            pass\n","                elif isinstance(layers, batched_graphsage_layer.GNN_Batched_GraphSage_Layer):\n","                    torch.nn.init.normal_(layers.learnable_weights.weight, mean=mean, std=std)\n","                    if Bias:\n","                        layers.learnable_weights.bias.data.zero_()\n","                elif isinstance(layers, batched_diffpool_layer.Batched_DiffPool_Layer):\n","                    torch.nn.init.normal_(layers.new_assign.assinment_layer.learnable_weights.weight, mean=mean, std=std)\n","                    torch.nn.init.normal_(layers.new_embed.embedding_layer.learnable_weights.weight, mean=mean, std=std)\n","                    if Bias:\n","                        torch.nn.init.zeros_(layers.new_assign.assinment_layer.learnable_weights.bias)\n","                        torch.nn.init.zeros_(layers.new_embed.embedding_layer.learnable_weights.bias)\n","                elif isinstance(layers, torch.nn.Linear):\n","                    torch.nn.init.normal_(layers.weight, mean=mean, std=std)\n","                    if Bias:\n","                        torch.nn.init.zeros_(layers.bias)\n","\n","\n","    def computational_matricess(self, batched_graphs, edge_mask):\n","        if edge_mask == None:\n","            joint_tilda_adjacency_matrix = torch.tensor(to_scipy_sparse_matrix(batched_graphs.edge_index).todense()) + torch.eye(len(torch.tensor(to_scipy_sparse_matrix(batched_graphs.edge_index).todense())))\n","        else:\n","            joint_tilda_adjacency_matrix = torch.tensor(csr_matrix((np.array(edge_mask), (np.array(batched_graphs.edge_index[0]), np.array(batched_graphs.edge_index[1])))).todense()) + torch.eye(len(torch.tensor(to_scipy_sparse_matrix(batched_graphs.edge_index).todense())))\n","\n","        joint_tilda_adjacency_matrix = joint_tilda_adjacency_matrix.type(torch.float32)\n","        if batched_graphs.batch == None:\n","            batch_size = 1\n","        else:\n","            batch_size = batched_graphs.num_graphs\n","\n","        #print(\"whole_graphs_adjacency.size()[0]: \", whole_graphs_adjacency.size()[0])\n","        new_number_of_nodes = int(joint_tilda_adjacency_matrix.size()[0] / batch_size)\n","        #print(batch_size)\n","        adjacency_list = []\n","        feature_list = []\n","        for i in range(batch_size):\n","            start = i * new_number_of_nodes\n","            end = (i + 1) * new_number_of_nodes\n","            adjacency_list.append(joint_tilda_adjacency_matrix[start:end, start:end])\n","            feature_list.append(batched_graphs.x[start:end, :])\n","        adjacency_list = list(map(lambda x: torch.unsqueeze(x, 0), adjacency_list))\n","        feature_list = list(map(lambda x: torch.unsqueeze(x, 0), feature_list))\n","        new_adjacecny = torch.cat(adjacency_list, dim=0)\n","        new_features = torch.cat(feature_list, dim=0)\n","        new_adjacecny = new_adjacecny.view(batch_size, new_number_of_nodes,new_number_of_nodes)\n","\n","        return new_adjacecny, new_features\n","\n","\n","    def forward(self, batched_graphs, edge_mask):\n","        new_adjacecny, new_features = self.computational_matricess(batched_graphs, edge_mask)\n","\n","\n","\n","        new_features = new_features.to(torch.float32)\n","\n","        new_X, new_adjacency_1 = self.diffpool_layer_1(new_features, new_adjacecny)\n","        graph_sage1_output = self.graph_sage_1(new_X, new_adjacency_1)\n","\n","        new_X_2, new_adjacency_2 = self.diffpool_layer_2(graph_sage1_output, new_adjacency_1)\n","        graph_sage2_output = self.graph_sage_2(new_X_2, new_adjacency_2)\n","        #print(\"graph_sage2_output: \",graph_sage2_output.size())\n","\n","        #graph_sage2_output = graph_sage2_output.view((len(graph), self.diffpool_layers_new_num_nodes[1], self.diffpool_layers_dim[1][1]))\n","        #print(\"graph_sage2_output: \", graph_sage2_output.size, graph_sage2_output)\n","\n","        graph_sage2_output, q = torch.max(graph_sage2_output, dim=1, keepdim=True)\n","        #graph_sage2_output= graph_sage2_output.sum(dim=1, keepdim=True)\n","        #print(\"Summed_graph_sage2_output: \", graph_sage2_output)\n","\n","        linear1_output = self.lin1(graph_sage2_output)\n","        linear1_output = self.act_fun(linear1_output)\n","\n","        linear2_output = self.lin2(linear1_output)\n","        linear2_output = self.act_fun(linear2_output)\n","\n","        linear3_output = self.lin3(linear2_output)\n","        #linear3_output = self.act_fun_softmax(linear3_output, dim=2)\n","        linear3_output = self.act_fun(linear3_output)\n","        linear3_output = torch.squeeze(self.act_fun_softmax(linear3_output, dim=2))\n","\n","\n","        return new_X, new_adjacency_1, graph_sage1_output, new_X_2, new_adjacency_2, graph_sage2_output, linear1_output, linear2_output, linear3_output\n","\n","\n","\n","\n","dataset = TUDataset(root='data/TUDataset', name='MUTAG')\n","batch_size = 3\n","new_num_nodes = 10\n","print(\"new_num_nodes: \", new_num_nodes)\n","\n","node_feat_size = len(dataset[0].x[0])\n","hid_dim = 7\n","\n","import random\n","\n","diffpool_model_example = DIFFPOOL_Model(diffpool_layers_dim=[[node_feat_size, hid_dim], [hid_dim, node_feat_size]],\n","                                        diffpool_layers_new_num_nodes=[new_num_nodes, new_num_nodes], Weight_Initializer=1, Bias=True, num_classes=2,\n","                                        dropout_rate=0, normalize_embedding=True, aggregation='mean', act_fun='ReLu')\n","\n","batched_dataset = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n","\n","em = [random.uniform(0, 1) for i in range(len(next(iter(batched_dataset)).edge_index[0]))]\n","\n","for batched_graph in batched_dataset:\n","    x, edge_index, batch, y = batched_graph.x, batched_graph.edge_index, batched_graph.batch, batched_graph.y\n","    new_X, new_adjacency_1, graph_sage1_output, new_X_2, new_adjacency_2, graph_sage2_output, linear1_output, linear2_output, linear3_output = diffpool_model_example(batched_graph, em)\n","    print(\"Final Output Size: \", linear3_output.size())\n","    print(\"Final Output: \", linear3_output)\n","    break\n"]}]}