{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP5x73uWFT8rdMkwE3Zsx9f"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import os\n","import torch\n","os.environ['TORCH'] = torch.__version__\n","print(torch.__version__)\n","\n","!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n","!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n","!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Cvsh5p3Mfdh0","executionInfo":{"status":"ok","timestamp":1678050682394,"user_tz":-60,"elapsed":52945,"user":{"displayName":"Ehsan Mobaraki","userId":"06314958514195310048"}},"outputId":"737146df-9343-4619-8545-ff5851679a56"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["1.13.1+cu116\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m280.2/280.2 KB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Building wheel for torch-geometric (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","from torch.autograd import Variable\n","import math\n","import torch.nn.functional as F\n","from torch.nn.parameter import Parameter\n","from torch_geometric.utils.convert import to_scipy_sparse_matrix\n","from torch_geometric.utils.train_test_split_edges import torch_geometric\n","import networkx as nx\n","import numpy as np\n","from torch_geometric.nn import GCNConv\n","\n","from torch_geometric.datasets import TUDataset"],"metadata":{"id":"EYq5IcFQp8_L"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["m = nn.Dropout(p=0)\n","input = torch.randn(4, 4)\n","output = m(input)\n","print(input)\n","#print(output)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5K6XBqNTmZku","executionInfo":{"status":"ok","timestamp":1678021632888,"user_tz":-60,"elapsed":4,"user":{"displayName":"Ehsan Mobaraki","userId":"06314958514195310048"}},"outputId":"aaab1761-5dd1-419f-c3b9-daa9d669d61b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[ 1.2184,  0.1812,  1.6613, -0.0923],\n","        [ 0.8088, -1.8346, -0.2515,  0.1343],\n","        [ 1.8699,  1.8720,  1.7355, -0.4422],\n","        [-0.0989,  0.2519,  0.3898,  1.1324]])\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-jcetoUc-SUI","executionInfo":{"status":"ok","timestamp":1678050703191,"user_tz":-60,"elapsed":18365,"user":{"displayName":"Ehsan Mobaraki","userId":"06314958514195310048"}},"outputId":"5abf7992-5b24-43e8-d1fc-05982ac0b6b8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["dataset = TUDataset(root='data/TUDataset', name='MUTAG')"],"metadata":{"id":"ai5efxpkqEMr","executionInfo":{"status":"ok","timestamp":1678050704199,"user_tz":-60,"elapsed":1021,"user":{"displayName":"Ehsan Mobaraki","userId":"06314958514195310048"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"6f9e8453-c3e3-41c7-83ea-bdfe3e65ae4a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading https://www.chrsmrrs.com/graphkerneldatasets/MUTAG.zip\n","Extracting data/TUDataset/MUTAG/MUTAG.zip\n","Processing...\n","Done!\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_HfGP0awdzyD"},"outputs":[],"source":["\n","import sys \n","py_path = '/content/drive/MyDrive/Explainability Methods/Models/Script/Layers/'\n","sys.path.insert(0,py_path)\n","from torch_geometric.typing import Adj, OptPairTensor, Size, SparseTensor\n","import torch\n","import torch.nn as nn\n","import matrix_util as Mat_Util\n","class GNN_GraphSage_Layer(nn.Module):\n","    '''\n","        #    A single GraphSage Layer: Graph Sampling and Aggregate\n","    '''\n","    def __init__(self, input_dim, output_dim, Bias, normalize_embedding, dropout, aggregation):\n","        super(GNN_GraphSage_Layer, self).__init__()\n","        \n","        self.input_dim = input_dim\n","        self.output_dim = output_dim\n","        self.Bias = Bias\n","        self.dropout = dropout\n","        self.normalize_embedding = normalize_embedding\n","        self.aggregation = aggregation\n","        #self.add_self = add_self\n","        \n","        if self.aggregation == 'mean':\n","            self.learnable_weights = nn.Linear(self.input_dim*2, self.output_dim, bias=self.Bias)\n","        else:\n","            self.learnable_weights = nn.Linear(self.input_dim, self.output_dim, bias=self.Bias)\n","\n","        self.normalize = F.normalize\n","    \n","    def forward(self, input_tensor, tilda_adjacency_matrix):\n","\n","        if self.aggregation == 'mean':\n","            tilda_adjacency_matrix = tilda_adjacency_matrix / tilda_adjacency_matrix.sum(-1, keepdim=True)\n","        num_node_per_graph = tilda_adjacency_matrix.size(1)\n","        print(\"num_node_per_graph: \", num_node_per_graph)\n","\n","        tilda_adjacency_matrix_neghborhood = torch.mm(tilda_adjacency_matrix, input_tensor) # Y = A~ * X\n","\n","        neighborhood_aggregated = torch.cat((tilda_adjacency_matrix_neghborhood, input_tensor), 1)\n","\n","        node_linear = self.learnable_weights(neighborhood_aggregated) # Y * W\n","        \n","        if self.normalize_embedding:\n","            node_linear = self.normalize(node_linear, p=2, dim=1)\n","        \n","\n","\n","        return node_linear\n"]},{"cell_type":"code","source":["graphsage = GNN_GraphSage_Layer(input_dim=7, output_dim=7, Bias=True, normalize_embedding=True, dropout=0, aggregation='mean')\n","graph = dataset[0]\n","tilda_adjacency_matrix = torch.tensor(to_scipy_sparse_matrix(graph.edge_index).todense()) + torch.eye(len(torch.tensor(to_scipy_sparse_matrix(graph.edge_index).todense())))\n","#print(adjacency_matrix)\n","\n","graphsage_output = graphsage(graph.x, tilda_adjacency_matrix)\n","print(graphsage)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TqBPewh7PaG4","executionInfo":{"status":"ok","timestamp":1678030834209,"user_tz":-60,"elapsed":264,"user":{"displayName":"Ehsan Mobaraki","userId":"06314958514195310048"}},"outputId":"a29d2391-8a3a-41e3-eb92-3a469466086f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["num_node_per_graph:  17\n","GNN_GraphSage_Layer(\n","  (learnable_weights): Linear(in_features=14, out_features=7, bias=True)\n",")\n"]}]},{"cell_type":"code","source":["graphsage = GNN_GraphSage_Layer(input_dim=7, output_dim=7, Bias=True, normalize_embedding=True, dropout=0, aggregation='mean')\n","graph = dataset[0]\n","tilda_adjacency_matrix = torch.tensor(to_scipy_sparse_matrix(graph.edge_index).todense()) + torch.eye(len(torch.tensor(to_scipy_sparse_matrix(graph.edge_index).todense())))\n","#print(adjacency_matrix)\n","\n","graphsage_output = graphsage(graph.x, tilda_adjacency_matrix)"],"metadata":{"id":"YNeMNGLvq-W2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1678030804404,"user_tz":-60,"elapsed":248,"user":{"displayName":"Ehsan Mobaraki","userId":"06314958514195310048"}},"outputId":"28a0d56a-2d20-428d-aa9c-9d245f8ee207"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["num_node_per_graph:  17\n"]}]},{"cell_type":"code","source":["import sys \n","py_path = '/content/drive/MyDrive/Explainability Methods/Models/Script/Layers/'\n","sys.path.insert(0,py_path)\n","from torch_geometric.typing import Adj, OptPairTensor, Size, SparseTensor\n","import torch\n","import torch.nn as nn\n","import matrix_util as Mat_Util\n","class GNN_Batched_GraphSage_Layer(nn.Module):\n","    '''\n","        #    A single GraphSage Layer: Graph Sampling and Aggregate\n","    '''\n","    def __init__(self, input_dim, output_dim, Bias, normalize_embedding, dropout, aggregation):\n","        super(GNN_Batched_GraphSage_Layer, self).__init__()\n","        \n","        self.input_dim = input_dim\n","        self.output_dim = output_dim\n","        self.Bias = Bias\n","        self.dropout = dropout\n","        self.normalize_embedding = normalize_embedding\n","        self.aggregation = aggregation\n","        #self.add_self = add_self\n","        \n","        if self.aggregation == 'mean':\n","            self.learnable_weights = nn.Linear(self.input_dim*2, self.output_dim, bias=self.Bias)\n","        else:\n","            self.learnable_weights = nn.Linear(self.input_dim, self.output_dim, bias=self.Bias)\n","\n","        self.normalize = F.normalize\n","    \n","    \n","    def forward(self, new_features, tilda_adjacency_matrix):\n","        #tilda_adjacency_matrix, new_features = self.computational_matricess(batched_graphs)\n","        new_features = new_features.to(torch.float32)\n","\n","    \n","        if self.aggregation == 'mean':\n","            tilda_adjacency_matrix = tilda_adjacency_matrix / tilda_adjacency_matrix.sum(-2, keepdim=True)\n","        num_node_per_graph = tilda_adjacency_matrix.size(1)\n","\n","        tilda_adjacency_matrix_neghborhood = torch.bmm(tilda_adjacency_matrix, new_features) # Y = A~ * X\n","\n","        neighborhood_aggregated = torch.cat((tilda_adjacency_matrix_neghborhood, new_features), 2)\n","\n","        node_linear = self.learnable_weights(neighborhood_aggregated) # Y * W\n","        \n","        if self.normalize_embedding:\n","            node_linear = self.normalize(node_linear, p=2, dim=1)\n","        \n","\n","\n","        return node_linear"],"metadata":{"id":"YiXCuD2jK7cn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["mat = torch.arange(18).view(2,3, -1)\n","print(mat)\n","\n","print(torch.sum(mat, dim=-1))\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"saqAAO8dZNVY","executionInfo":{"status":"ok","timestamp":1678032376749,"user_tz":-60,"elapsed":257,"user":{"displayName":"Ehsan Mobaraki","userId":"06314958514195310048"}},"outputId":"bb98d75d-0168-4e27-fad9-2e845317641c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[[ 0,  1,  2],\n","         [ 3,  4,  5],\n","         [ 6,  7,  8]],\n","\n","        [[ 9, 10, 11],\n","         [12, 13, 14],\n","         [15, 16, 17]]])\n","tensor([[ 3, 12, 21],\n","        [30, 39, 48]])\n"]}]},{"cell_type":"code","source":["t = torch.tensor([[1.,2.,3.]])\n","print(\"tensor: \", t)\n","t1 = F.normalize(t, p=2.0, dim = 0)\n","t2 = F.normalize(t, p=2.0, dim = 1)\n","print(t1)\n","print(t2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tl4wpAzUh5ED","executionInfo":{"status":"ok","timestamp":1678033107330,"user_tz":-60,"elapsed":5,"user":{"displayName":"Ehsan Mobaraki","userId":"06314958514195310048"}},"outputId":"5b46b806-eff8-4810-e084-ddada3997adc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor:  tensor([[1., 2., 3.]])\n","tensor([[1., 1., 1.]])\n","tensor([[0.2673, 0.5345, 0.8018]])\n"]}]},{"cell_type":"code","source":["from torch_geometric.loader import DataLoader\n","batched_dataset = DataLoader(dataset, batch_size=20, shuffle=False)\n","batched_graphsage_example = GNN_Batched_GraphSage_Layer(input_dim=7, output_dim=7, Bias=True, normalize_embedding=True, dropout=0, aggregation='mean')\n","print(batched_graphsage_example)"],"metadata":{"id":"Yd2Gllqx6Cl-","executionInfo":{"status":"ok","timestamp":1678033653868,"user_tz":-60,"elapsed":243,"user":{"displayName":"Ehsan Mobaraki","userId":"06314958514195310048"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"61558c31-da04-4bb3-932a-27fcc604b209"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["GNN_Batched_GraphSage_Layer(\n","  (learnable_weights): Linear(in_features=14, out_features=7, bias=True)\n",")\n"]}]},{"cell_type":"code","source":["for batched_graph in batched_dataset:\n","    #print(len(batched_graph))\n","    x, edge_index, batch, y = batched_graph.x, batched_graph.edge_index, batched_graph.batch, batched_graph.y \n","    #print(batched_graph.num_graphs)\n","    tilda_adjacency_matrix = torch.tensor(to_scipy_sparse_matrix(batched_graph.edge_index).todense()) + torch.eye(len(torch.tensor(to_scipy_sparse_matrix(batched_graph.edge_index).todense())))\n","    #print(\"tilda_adjacency_matrix: \", tilda_adjacency_matrix.size())\n","    softmaxed_output = batched_graphsage_example(batched_graph)\n","    print(\"softmaxed_output: \", softmaxed_output.size())\n","    break\n","print(softmaxed_output)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ooZC3Suk6jIl","executionInfo":{"status":"ok","timestamp":1678033667298,"user_tz":-60,"elapsed":12,"user":{"displayName":"Ehsan Mobaraki","userId":"06314958514195310048"}},"outputId":"faa0c0c6-c951-45c5-acb9-0b90d62cfa31"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["softmaxed_output:  torch.Size([20, 16, 7])\n","tensor([[[-0.2269, -0.2606, -0.2084,  ..., -0.2661,  0.2555, -0.1350],\n","         [-0.2269, -0.2606, -0.2084,  ..., -0.2661,  0.2555, -0.1350],\n","         [-0.2038, -0.2728, -0.2191,  ..., -0.2553,  0.2572, -0.1282],\n","         ...,\n","         [-0.1806, -0.2851, -0.2299,  ..., -0.2445,  0.2588, -0.1215],\n","         [ 0.5219, -0.1534, -0.5636,  ..., -0.1558,  0.2297, -0.8279],\n","         [ 0.0251,  0.2255, -0.3063,  ...,  0.0689,  0.1843, -0.1422]],\n","\n","        [[ 0.0765,  0.0498, -0.5012,  ...,  0.0567,  0.1877,  0.0596],\n","         [-0.2249, -0.2889, -0.1493,  ..., -0.2998,  0.2661, -0.1027],\n","         [-0.2020, -0.3025, -0.1570,  ..., -0.2877,  0.2678, -0.0976],\n","         ...,\n","         [ 0.0391,  0.2199, -0.2385,  ...,  0.0745,  0.1937, -0.0859],\n","         [-0.2249, -0.2889, -0.1493,  ..., -0.2998,  0.2661, -0.1027],\n","         [-0.2249, -0.2889, -0.1493,  ..., -0.2998,  0.2661, -0.1027]],\n","\n","        [[-0.1646, -0.2260, -0.1266,  ..., -0.2173,  0.2629, -0.1651],\n","         [-0.2664, -0.2504, -0.1524,  ..., -0.3105,  0.2581, -0.1122],\n","         [-0.1988, -0.2893, -0.1786,  ..., -0.2755,  0.2631, -0.0969],\n","         ...,\n","         [-0.2213, -0.2764, -0.1699,  ..., -0.2872,  0.2615, -0.1020],\n","         [-0.2664, -0.2504, -0.1524,  ..., -0.3105,  0.2581, -0.1122],\n","         [-0.1762, -0.3023, -0.1873,  ..., -0.2639,  0.2648, -0.0918]],\n","\n","        ...,\n","\n","        [[-0.1104, -0.2886, -0.1195,  ..., -0.2629,  0.2852, -0.1052],\n","         [ 0.3882, -0.2467, -0.5236,  ..., -0.2703,  0.2364, -0.4126],\n","         [ 0.0332,  0.2267, -0.2064,  ...,  0.0900,  0.2039, -0.0651],\n","         ...,\n","         [ 0.4194, -0.0310, -0.1385,  ..., -0.1941,  0.2598, -0.5708],\n","         [ 0.4390, -0.1753, -0.3493,  ..., -0.2120,  0.2517, -0.4774],\n","         [ 0.0211,  0.2578, -0.1898,  ...,  0.0937,  0.2020, -0.0820]],\n","\n","        [[ 0.0819,  0.0452, -0.5823,  ...,  0.0524,  0.1833,  0.0632],\n","         [ 0.5117,  0.0997, -0.0794,  ..., -0.0981,  0.2358, -0.9232],\n","         [-0.2409, -0.2622, -0.1734,  ..., -0.2773,  0.2599, -0.1088],\n","         ...,\n","         [-0.2655, -0.2499, -0.1645,  ..., -0.2885,  0.2582, -0.1142],\n","         [-0.1730, -0.3715, -0.4389,  ..., -0.2516,  0.2539, -0.0092],\n","         [ 0.0140,  0.1545, -0.3169,  ...,  0.0268,  0.1894, -0.0365]],\n","\n","        [[ 0.5227, -0.2842, -0.5766,  ..., -0.1604,  0.2389, -0.4725],\n","         [ 0.0224,  0.2446, -0.2001,  ...,  0.0842,  0.1968, -0.0954],\n","         [ 0.0224,  0.2446, -0.2001,  ...,  0.0842,  0.1968, -0.0954],\n","         ...,\n","         [ 0.4737, -0.2739, -0.5730,  ..., -0.2033,  0.2355, -0.4664],\n","         [ 0.0352,  0.2151, -0.2175,  ...,  0.0809,  0.1986, -0.0757],\n","         [ 0.0352,  0.2151, -0.2175,  ...,  0.0809,  0.1986, -0.0757]]],\n","       grad_fn=<DivBackward0>)\n"]}]},{"cell_type":"code","source":["print(graphsage_output.size())\n","print(graphsage_output)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wY5Z1xSor6UZ","executionInfo":{"status":"ok","timestamp":1677986639464,"user_tz":-60,"elapsed":264,"user":{"displayName":"Ehsan Mobaraki","userId":"06314958514195310048"}},"outputId":"4af0a0ff-ad42-43a5-efef-4c72add2ae55"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([17, 7])\n","tensor([[ 0.5391, -0.0586,  0.2297, -0.1688, -0.6186, -0.4392,  0.2216],\n","        [ 0.5391, -0.0586,  0.2297, -0.1688, -0.6186, -0.4392,  0.2216],\n","        [ 0.5391, -0.0586,  0.2297, -0.1688, -0.6186, -0.4392,  0.2216],\n","        [ 0.5391, -0.0586,  0.2297, -0.1688, -0.6186, -0.4392,  0.2216],\n","        [ 0.5391, -0.0586,  0.2297, -0.1688, -0.6186, -0.4392,  0.2216],\n","        [ 0.5391, -0.0586,  0.2297, -0.1688, -0.6186, -0.4392,  0.2216],\n","        [ 0.5391, -0.0586,  0.2297, -0.1688, -0.6186, -0.4392,  0.2216],\n","        [ 0.5391, -0.0586,  0.2297, -0.1688, -0.6186, -0.4392,  0.2216],\n","        [ 0.5391, -0.0586,  0.2297, -0.1688, -0.6186, -0.4392,  0.2216],\n","        [ 0.5391, -0.0586,  0.2297, -0.1688, -0.6186, -0.4392,  0.2216],\n","        [ 0.5391, -0.0586,  0.2297, -0.1688, -0.6186, -0.4392,  0.2216],\n","        [ 0.5391, -0.0586,  0.2297, -0.1688, -0.6186, -0.4392,  0.2216],\n","        [ 0.6193, -0.0558,  0.3279, -0.2110, -0.5358, -0.3831,  0.1656],\n","        [ 0.5391, -0.0586,  0.2297, -0.1688, -0.6186, -0.4392,  0.2216],\n","        [ 0.0833,  0.7969, -0.1444, -0.1451,  0.3239, -0.4434, -0.1207],\n","        [ 0.4752, -0.4233, -0.0693, -0.2001, -0.5904, -0.1068, -0.4362],\n","        [ 0.4752, -0.4233, -0.0693, -0.2001, -0.5904, -0.1068, -0.4362]],\n","       grad_fn=<DivBackward0>)\n"]}]},{"cell_type":"code","source":["sum=0\n","length_list = []\n","for i in range(len(dataset)):\n","    sum = sum + len(dataset[i].x)\n","    length_list.append(len(dataset[i].x))\n","    #print(len(dataset[i].x))\n","print(\"number of nodes in whole dataset: \", sum)\n","print(length_list)\n","print(\"max number of nodes in graphs: \", max(length_list))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_1XO_SJu6cSQ","executionInfo":{"status":"ok","timestamp":1677978724294,"user_tz":-60,"elapsed":3,"user":{"displayName":"Ehsan Mobaraki","userId":"06314958514195310048"}},"outputId":"45cddb9b-2e48-43fe-bbf9-bbba0370ab10"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["number of nodes in whole dataset:  3371\n","[17, 13, 13, 19, 11, 28, 16, 20, 12, 17, 17, 20, 22, 13, 19, 22, 11, 17, 13, 18, 18, 17, 23, 27, 17, 13, 23, 17, 23, 23, 22, 24, 23, 13, 17, 14, 17, 15, 15, 13, 17, 13, 19, 17, 12, 23, 22, 17, 20, 16, 26, 26, 19, 19, 14, 17, 21, 25, 23, 19, 17, 11, 23, 20, 16, 16, 20, 23, 19, 14, 26, 16, 16, 23, 18, 10, 16, 16, 17, 19, 12, 25, 16, 11, 23, 23, 16, 12, 13, 23, 25, 19, 23, 19, 19, 25, 18, 13, 15, 16, 23, 26, 19, 23, 17, 20, 25, 19, 28, 24, 11, 15, 13, 16, 12, 10, 21, 23, 21, 13, 25, 21, 17, 11, 19, 20, 21, 15, 14, 11, 19, 11, 21, 22, 11, 20, 22, 13, 11, 16, 11, 20, 12, 11, 16, 16, 13, 15, 20, 12, 12, 22, 15, 12, 14, 12, 20, 20, 20, 14, 26, 24, 22, 22, 19, 23, 22, 11, 13, 17, 24, 12, 21, 22, 22, 12, 20, 13, 22, 28, 11, 14, 22, 22, 13, 12, 21, 16]\n","max number of nodes in graphs:  28\n"]}]},{"cell_type":"code","source":["\n","class DiffPool_Embedding_Layer(nn.Module):\n","    '''\n","   #     Z, new features size\n","    '''\n","    def __init__(self, input_dim_size, new_feat_dim_size, Bias, normalize_embedding, dropout, aggregation):\n","        super(DiffPool_Embedding_Layer, self).__init__()\n","        self.input_dim_size = input_dim_size\n","        self.new_feat_dim_size = new_feat_dim_size\n","        self.Bias = Bias\n","        self.normalize_embedding = normalize_embedding\n","        self.dropout = dropout\n","        self.aggregation = aggregation\n","        self.embedding_layer = GNN_GraphSage_Layer(input_dim=self.input_dim_size, output_dim=self.new_feat_dim_size, Bias=self.Bias, normalize_embedding=self.normalize_embedding, dropout=self.dropout, aggregation=self.aggregation)\n","        #self.embedding_layer = GNN_Batched_GraphSage_Layer(input_dim=self.input_dim_size, output_dim=self.new_feat_dim_size, Bias=self.Bias, normalize_embedding=self.normalize_embedding, dropout=self.dropout, aggregation=self.aggregation)\n","        self.act_fun = F.relu\n","\n","\n","    def forward(self, input_tensor, tilda_adjacency_matrix):\n","        #x, edge_index, batch, y = batched_graph.x, batched_graph.edge_index, batched_graph.batch, batched_graph.y \n","        z_l_init = self.embedding_layer(input_tensor, tilda_adjacency_matrix)\n","        z_l_init = self.act_fun(z_l_init)\n","\n","        return z_l_init\n"],"metadata":{"id":"vjPrH-wRncpZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import sys \n","py_path = '/content/drive/MyDrive/Explainability Methods/Models/Script/Layers/'\n","sys.path.insert(0,py_path)\n","from torch_geometric.typing import Adj, OptPairTensor, Size, SparseTensor\n","import torch\n","import torch.nn as nn\n","import matrix_util as Mat_Util\n","class Batched_DiffPool_Embedding_Layer(nn.Module):\n","    '''\n","   #     Z, new features size\n","    '''\n","    def __init__(self, input_dim_size, new_feat_dim_size, Bias, normalize_embedding, dropout, aggregation):\n","        super(Batched_DiffPool_Embedding_Layer, self).__init__()\n","        self.input_dim_size = input_dim_size\n","        self.new_feat_dim_size = new_feat_dim_size\n","        self.Bias = Bias\n","        self.normalize_embedding = normalize_embedding\n","        self.dropout = dropout\n","        self.aggregation = aggregation\n","        #self.embedding_layer = GNN_GraphSage_Layer(input_dim=self.input_dim_size, output_dim=self.new_feat_dim_size, Bias=self.Bias, normalize_embedding=self.normalize_embedding, dropout=self.dropout, aggregation=self.aggregation)\n","        self.embedding_layer = GNN_Batched_GraphSage_Layer(input_dim=self.input_dim_size, output_dim=self.new_feat_dim_size, Bias=self.Bias, normalize_embedding=self.normalize_embedding, dropout=self.dropout, aggregation=self.aggregation)\n","        self.act_fun = F.relu\n","\n","\n","    def forward(self, input_tensor, tilda_adjacency_matrix):\n","        #x, edge_index, batch, y = batched_graph.x, batched_graph.edge_index, batched_graph.batch, batched_graph.y \n","        z_l_init = self.embedding_layer(input_tensor, tilda_adjacency_matrix)\n","        z_l_init = self.act_fun(z_l_init)\n","\n","        return z_l_init\n"],"metadata":{"id":"0cDYFIM8lqAJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#import binascii\n","\n","class DiffPool_Assignment_Layer(nn.Module):\n","    '''\n","    #    S, new clusters, new number of nodes\n","    '''\n","    def __init__(self, input_dim_size, new_num_nodes, Bias, normalize_embedding, dropout, aggregation):\n","        super(DiffPool_Assignment_Layer, self).__init__()\n","        self.input_dim_size = input_dim_size\n","        self.new_num_nodes = new_num_nodes\n","        self.Bias = Bias\n","        self.normalize_embedding = normalize_embedding\n","        self.dropout = dropout\n","        self.aggregation = aggregation\n","        self.assinment_layer = GNN_GraphSage_Layer(input_dim=self.input_dim_size, output_dim=self.new_num_nodes, Bias=self.Bias, normalize_embedding=self.normalize_embedding, dropout=self.dropout, aggregation=self.aggregation)\n","        #self.assinment_layer = GNN_Batched_GraphSage_Layer(input_dim=self.input_dim_size, output_dim=self.new_num_nodes, Bias=self.Bias, normalize_embedding=self.normalize_embedding, dropout=self.dropout, aggregation=self.aggregation)\n","        self.act_fun = F.relu\n","    \n","    def forward(self, input_tensor, tilda_adjacency_matrix):\n","        #x, edge_index, batch, y = batched_graph.x, batched_graph.edge_index, batched_graph.batch, batched_graph.y \n","        s_l_init = self.assinment_layer(input_tensor, tilda_adjacency_matrix)\n","        s_l_init = self.act_fun(s_l_init)\n","\n","        s_l = F.softmax(s_l_init, dim=-1)\n","        \n","        return s_l\n"],"metadata":{"id":"lZa_8jjctleJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import sys \n","py_path = '/content/drive/MyDrive/Explainability Methods/Models/Script/Layers/'\n","sys.path.insert(0,py_path)\n","from torch_geometric.typing import Adj, OptPairTensor, Size, SparseTensor\n","import torch\n","import torch.nn as nn\n","import matrix_util as Mat_Util\n","\n","class Batched_DiffPool_Assignment_Layer(nn.Module):\n","    '''\n","    #    S, new clusters, new number of nodes\n","    '''\n","    def __init__(self, input_dim_size, new_num_nodes, Bias, normalize_embedding, dropout, aggregation):\n","        super(Batched_DiffPool_Assignment_Layer, self).__init__()\n","        self.input_dim_size = input_dim_size\n","        self.new_num_nodes = new_num_nodes\n","        self.Bias = Bias\n","        self.normalize_embedding = normalize_embedding\n","        self.dropout = dropout\n","        self.aggregation = aggregation\n","        #self.assinment_layer = GNN_GraphSage_Layer(input_dim=self.input_dim_size, output_dim=self.new_num_nodes, Bias=self.Bias, normalize_embedding=self.normalize_embedding, dropout=self.dropout, aggregation=self.aggregation)\n","        self.assinment_layer = GNN_Batched_GraphSage_Layer(input_dim=self.input_dim_size, output_dim=self.new_num_nodes, Bias=self.Bias, normalize_embedding=self.normalize_embedding, dropout=self.dropout, aggregation=self.aggregation)\n","        self.act_fun = F.relu\n","    \n","    def forward(self, input_tensor, tilda_adjacency_matrix):\n","        #x, edge_index, batch, y = batched_graph.x, batched_graph.edge_index, batched_graph.batch, batched_graph.y \n","        s_l_init = self.assinment_layer(input_tensor, tilda_adjacency_matrix)\n","        s_l_init = self.act_fun(s_l_init)\n","\n","        s_l = F.softmax(s_l_init, dim=-1)\n","        \n","        return s_l\n"],"metadata":{"id":"9HLZ0QnKlwTF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","class DiffPool_Layer(nn.Module):\n","    def __init__(self, input_dim_size, new_feat_dim_size, new_num_nodes, Bias, normalize_embedding, dropout, aggregation):\n","        super(DiffPool_Layer, self).__init__()\n","        self.input_dim_size = input_dim_size\n","        self.new_feat_dim_size = new_feat_dim_size\n","        self.new_num_nodes = new_num_nodes\n","        self.Bias = Bias\n","        self.normalize_embedding = normalize_embedding\n","        self.dropout = dropout\n","        self.aggregation = aggregation\n","\n","        self.new_embed = DiffPool_Embedding_Layer(input_dim_size=input_dim_size, new_feat_dim_size=new_feat_dim_size, Bias=self.Bias, normalize_embedding=self.normalize_embedding, dropout=self.dropout, aggregation=self.aggregation)\n","        self.new_assign = DiffPool_Assignment_Layer(input_dim_size=self.input_dim_size, new_num_nodes=self.new_num_nodes, Bias=self.Bias , normalize_embedding=self.normalize_embedding , dropout=self.dropout, aggregation=self.aggregation)\n","\n","        \n","    def forward(self, input_tensor, tilda_adjacency_matrix):\n","        #x, edge_index, batch, y = batched_graph.x, batched_graph.edge_index, batched_graph.batch, batched_graph.y \n","\n","        z_l = self.new_embed(input_tensor, tilda_adjacency_matrix)\n","        s_l = self.new_assign(input_tensor, tilda_adjacency_matrix)\n","\n","        new_X = torch.mm(s_l.transpose(-1, -2), z_l)\n","        new_adjacency = (s_l.transpose(-1, -2)).mm(tilda_adjacency_matrix).mm(s_l)\n","\n","        return new_X, new_adjacency\n"],"metadata":{"id":"qZ4_Zx1Bnm21"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import sys \n","py_path = '/content/drive/MyDrive/Explainability Methods/Models/Script/Layers/'\n","sys.path.insert(0,py_path)\n","from torch_geometric.typing import Adj, OptPairTensor, Size, SparseTensor\n","import torch\n","import torch.nn as nn\n","import matrix_util as Mat_Util\n","class Batched_DiffPool_Layer(nn.Module):\n","    def __init__(self, input_dim_size, new_feat_dim_size, new_num_nodes, Bias, normalize_embedding, dropout, aggregation):\n","        super(Batched_DiffPool_Layer, self).__init__()\n","        self.input_dim_size = input_dim_size\n","        self.new_feat_dim_size = new_feat_dim_size\n","        self.new_num_nodes = new_num_nodes\n","        self.Bias = Bias\n","        self.normalize_embedding = normalize_embedding\n","        self.dropout = dropout\n","        self.aggregation = aggregation\n","\n","        self.new_embed = Batched_DiffPool_Embedding_Layer(input_dim_size=input_dim_size, new_feat_dim_size=new_feat_dim_size, Bias=self.Bias, normalize_embedding=self.normalize_embedding, dropout=self.dropout, aggregation=self.aggregation)\n","        self.new_assign = Batched_DiffPool_Assignment_Layer(input_dim_size=self.input_dim_size, new_num_nodes=self.new_num_nodes, Bias=self.Bias , normalize_embedding=self.normalize_embedding , dropout=self.dropout, aggregation=self.aggregation)  \n","\n","\n","    def forward(self, new_features, new_adjacecny):\n","        #x, edge_index, batch, y = batched_graph.x, batched_graph.edge_index, batched_graph.batch, batched_graph.y \n","        #new_adjacecny, new_features = self.computational_matricess(batched_graphs)\n","\n","        z_l = self.new_embed(new_features, new_adjacecny)\n","        s_l = self.new_assign(new_features, new_adjacecny)\n","\n","        print(\"z_l: \", z_l.size())\n","        print(\"s_l: \", s_l.size())\n","        new_X = torch.bmm(s_l.transpose(-1, -2), z_l)\n","        new_adjacency = (s_l.transpose(-1, -2)).bmm(new_adjacecny).bmm(s_l)\n","\n","        return new_X, new_adjacency\n"],"metadata":{"id":"lBUfixvNl4fM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["batched_diffpool_layer_example = Batched_DiffPool_Layer(input_dim_size=7, new_feat_dim_size=8, new_num_nodes=13, Bias=True, normalize_embedding=True, dropout=0, aggregation='mean')"],"metadata":{"id":"UCsN_tDb_MQE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torch_geometric.loader import DataLoader\n","batched_dataset = DataLoader(dataset, batch_size=30, shuffle=False)\n","for batched_graph in batched_dataset:\n","    x, edge_index, batch, y = batched_graph.x, batched_graph.edge_index, batched_graph.batch, batched_graph.y \n","\n","    new_X, new_adjacency = batched_diffpool_layer_example(batched_graph)\n","    print(\"softmaxed_output: \", new_X.size())\n","    print(\"softmaxed_output: \", new_adjacency.size())\n","    break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Mll4CF45_iBj","executionInfo":{"status":"ok","timestamp":1678037497492,"user_tz":-60,"elapsed":8,"user":{"displayName":"Ehsan Mobaraki","userId":"06314958514195310048"}},"outputId":"e3245308-404d-4bb8-e037-9dc02a61d241"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["z_l:  torch.Size([30, 17, 8])\n","s_l:  torch.Size([30, 17, 13])\n","softmaxed_output:  torch.Size([30, 13, 8])\n","softmaxed_output:  torch.Size([30, 13, 13])\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"UKrL0SvbxMYw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["drive.mount('/content/drive/',force_remount=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3FtxgK9wDdXs","executionInfo":{"status":"ok","timestamp":1677975067892,"user_tz":-60,"elapsed":3167,"user":{"displayName":"Ehsan Mobaraki","userId":"06314958514195310048"}},"outputId":"9fc6e728-2100-4796-ac69-742e7a6284ed"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n"]}]},{"cell_type":"code","source":["from torch_geometric.utils import dropout\n","from torch_geometric.loader import DataLoader\n","import torch\n","import torch.nn as nn\n","from torch.autograd import Variable\n","import math\n","import torch.nn.functional as F\n","from torch.nn.parameter import Parameter\n","from torch_geometric.utils.convert import to_scipy_sparse_matrix\n","from torch_geometric.utils.train_test_split_edges import torch_geometric\n","import networkx as nx\n","import numpy as np\n","from torch_geometric.nn import GCNConv\n","import sys \n","from torch_geometric.datasets import TUDataset\n","py_path = '/content/drive/MyDrive/Explainability Methods/Models/Script/Layers/'\n","sys.path.insert(0,py_path)\n","import Batched_GraphSage_Layer as batched_graphsage_layer\n","import Batched_DIFFPOOL_Assignment as batched_diffpool_assignment\n","import Batched_DIFFPOOL_Embedding as batched_diffpool_embedding\n","import Batched_DIFFPOOL_Layer as batched_diffpool_layer\n","\n","\n","\n","class GlobalMeanPool(nn.Module):\n","\n","    def __init__(self):\n","        super().__init__()\n","\n","    def forward(self, x, batch):\n","        return gnn.global_mean_pool(x, batch)\n","################################################################################\n","class IdenticalPool(nn.Module):\n","\n","    def __init__(self):\n","        super().__init__()\n","\n","    def forward(self, x, batch):\n","        return x\n","\n","################################################################################\n","class DIFFPOOL_Model(nn.Module):\n","    '''\n","        DIFFPOOL Mode\n","    '''\n","    def __init__(self, diffpool_layers_dim, diffpool_layers_new_num_nodes, Weight_Initializer, Bias, num_classes, dropout_rate, normalize_embedding, aggregation, act_fun):\n","        \n","        super(DIFFPOOL_Model, self).__init__()\n","        self.diffpool_layers_dim = diffpool_layers_dim\n","        self.diffpool_layers_new_num_nodes = diffpool_layers_new_num_nodes\n","        self.dropout_rate = dropout_rate\n","        self.Bias = Bias\n","        self.normalize_embedding = normalize_embedding\n","        self.aggregation = aggregation\n","        self.num_classes = num_classes\n","\n","                                \n","        self.diffpool_layer_1 = batched_diffpool_layer.Batched_DiffPool_Layer(input_dim_size=self.diffpool_layers_dim[0][0], new_feat_dim_size=self.diffpool_layers_dim[0][1], new_num_nodes=self.diffpool_layers_new_num_nodes[0], Bias=self.Bias, normalize_embedding=self.normalize_embedding, dropout=self.dropout_rate, aggregation=self.aggregation)\n","\n","        self.graph_sage_1 = batched_graphsage_layer.GNN_Batched_GraphSage_Layer(input_dim=self.diffpool_layers_dim[0][1], output_dim=self.diffpool_layers_dim[1][0], Bias=self.Bias, normalize_embedding=self.normalize_embedding, dropout=self.dropout_rate, aggregation=self.aggregation)\n","\n","        self.diffpool_layer_2 = batched_diffpool_layer.Batched_DiffPool_Layer(input_dim_size=self.diffpool_layers_dim[1][0], new_feat_dim_size=self.diffpool_layers_dim[1][1], new_num_nodes=self.diffpool_layers_new_num_nodes[1], Bias=self.Bias, normalize_embedding=self.normalize_embedding, dropout=self.dropout_rate, aggregation=self.aggregation)\n","\n","        self.graph_sage_2 = batched_graphsage_layer.GNN_Batched_GraphSage_Layer(input_dim=self.diffpool_layers_dim[1][1], output_dim=self.diffpool_layers_dim[1][1], Bias=self.Bias, normalize_embedding=self.normalize_embedding, dropout=self.dropout_rate, aggregation=self.aggregation)\n","\n","        self.lin1 = torch.nn.Linear(in_features=self.diffpool_layers_dim[1][1], out_features=self.diffpool_layers_dim[1][1])\n","        self.lin2 = torch.nn.Linear(in_features=self.diffpool_layers_dim[1][1], out_features=self.diffpool_layers_dim[1][1])\n","        self.lin3 = torch.nn.Linear(in_features=self.diffpool_layers_dim[1][1], out_features=self.num_classes)\n","        \n","        if act_fun == 'ReLu':\n","            self.act_fun = F.relu\n","            print('ReLu is Selected.')\n","        elif act_fun == 'eLu':\n","            self.act_fun = nn.functional.elu\n","            print('eLu is Selected.')\n","        elif act_fun == 'tanh':\n","            self.act_fun = torch.tanh\n","            print('tanh is Selected.')\n","        self.act_fun_softmax = F.softmax       \n","        \n","\n","        mean = 0\n","        std = 0.1\n","        self.initialize_weights(Weight_Initializer, Bias, mean, std)\n","    \n","\n","    def initialize_weights(model, Weight_Initializer, Bias, mean, std):\n","        # 1. Xavier Normal_.  2. Kaiming Normal_.  3. Uniform (0,0.1std)\n","        if Weight_Initializer == 1:                                             #.      1. Xavier Normal_.\n","            for i,layers in enumerate(model.children()):\n","                if isinstance(layers, torch.nn.ModuleList):\n","                    for j, layer in enumerate(layers.modules()):\n","                        if isinstance(layer, batched_graphsage_layer.GNN_Batched_GraphSage_Layer):\n","                            torch.nn.init.xavier_normal_(layer.learnable_weights.weight)\n","                            if Bias:\n","                                layer.learnable_weights.bias.data.zero_()\n","                        else:\n","                            pass\n","                elif isinstance(layers, batched_graphsage_layer.GNN_Batched_GraphSage_Layer):\n","                    torch.nn.init.xavier_normal_(layers.learnable_weights.weight)\n","                    if Bias:\n","                        layers.learnable_weights.bias.data.zero_()\n","                elif isinstance(layers, batched_diffpool_layer.Batched_DiffPool_Layer):\n","                    torch.nn.init.xavier_normal_(layers.new_assign.assinment_layer.learnable_weights.weight)\n","                    torch.nn.init.xavier_normal_(layers.new_embed.embedding_layer.learnable_weights.weight)\n","                    if Bias:\n","                        torch.nn.init.zeros_(layers.new_assign.assinment_layer.learnable_weights.bias)\n","                        torch.nn.init.zeros_(layers.new_embed.embedding_layer.learnable_weights.bias)\n","                elif isinstance(layers, torch.nn.Linear):\n","                    torch.nn.init.xavier_normal_(layers.weight)\n","                    if Bias:\n","                        torch.nn.init.zeros_(layers.bias)\n","                    \n","\n","        if Weight_Initializer == 2:                                             #.      2. Kaiming Normal_.\n","            for i,layers in enumerate(model.children()):\n","                if isinstance(layers, torch.nn.ModuleList):\n","                    for j, layer in enumerate(layers.modules()):\n","                        if isinstance(layer, batched_graphsage_layer.GNN_Batched_GraphSage_Layer):\n","                            torch.nn.init.kaiming_normal_(layer.learnable_weights.weight)\n","                            if Bias:\n","                                layer.learnable_weights.bias.data.zero_()\n","                        else:\n","                            pass\n","                elif isinstance(layers, batched_graphsage_layer.GNN_Batched_GraphSage_Layer):\n","                    torch.nn.init.kaiming_normal_(layers.learnable_weights.weight)\n","                    if Bias:\n","                        layers.learnable_weights.bias.data.zero_()\n","                elif isinstance(layers, batched_diffpool_layer.Batched_DiffPool_Layer):\n","                    torch.nn.init.kaiming_normal_(layers.new_assign.assinment_layer.learnable_weights.weight)\n","                    torch.nn.init.kaiming_normal_(layers.new_embed.embedding_layer.learnable_weights.weight)\n","                    if Bias:\n","                        torch.nn.init.zeros_(layers.new_assign.assinment_layer.learnable_weights.bias)\n","                        torch.nn.init.zeros_(layers.new_embed.embedding_layer.learnable_weights.bias)\n","                elif isinstance(layers, torch.nn.Linear):\n","                    torch.nn.init.kaiming_normal_(layers.weight)\n","                    if Bias:\n","                        torch.nn.init.zeros_(layers.bias)\n","                    \n","                            \n","        if Weight_Initializer == 3:                                             #.      3. Uniform (0,0.1std)\n","            for i,layers in enumerate(model.children()):\n","                if isinstance(layers, torch.nn.ModuleList):\n","                    for j, layer in enumerate(layers.modules()):\n","                        if isinstance(layer, batched_graphsage_layer.GNN_Batched_GraphSage_Layer):\n","                            torch.nn.init.normal_(layer.learnable_weights.weight.data, mean, std)\n","                            if Bias:\n","                                layer.learnable_weights.bias.data.zero_()\n","                        else:\n","                            pass\n","                elif isinstance(layers, batched_graphsage_layer.GNN_Batched_GraphSage_Layer):\n","                    torch.nn.init.normal_(layers.learnable_weights.weight, mean=mean, std=std)\n","                    if Bias:\n","                        layers.learnable_weights.bias.data.zero_()\n","                elif isinstance(layers, batched_diffpool_layer.Batched_DiffPool_Layer):\n","                    torch.nn.init.normal_(layers.new_assign.assinment_layer.learnable_weights.weight, mean=mean, std=std)\n","                    torch.nn.init.normal_(layers.new_embed.embedding_layer.learnable_weights.weight, mean=mean, std=std)\n","                    if Bias:\n","                        torch.nn.init.zeros_(layers.new_assign.assinment_layer.learnable_weights.bias)\n","                        torch.nn.init.zeros_(layers.new_embed.embedding_layer.learnable_weights.bias)\n","                elif isinstance(layers, torch.nn.Linear):\n","                    torch.nn.init.normal_(layers.weight, mean=mean, std=std)\n","                    if Bias:\n","                        torch.nn.init.zeros_(layers.bias)\n","                    \n","\n","    def computational_matricess(self, batched_graphs):\n","        joint_tilda_adjacency_matrix = torch.tensor(to_scipy_sparse_matrix(batched_graphs.edge_index).todense()) + torch.eye(len(torch.tensor(to_scipy_sparse_matrix(batched_graphs.edge_index).todense())))\n","        joint_tilda_adjacency_matrix = joint_tilda_adjacency_matrix.type(torch.float32)\n","        batch_size = batched_graphs.num_graphs\n","\n","        #print(\"whole_graphs_adjacency.size()[0]: \", whole_graphs_adjacency.size()[0])\n","        new_number_of_nodes = int(joint_tilda_adjacency_matrix.size()[0] / batch_size)\n","        #print(batch_size)\n","        adjacency_list = []\n","        feature_list = []\n","        for i in range(batch_size):\n","            start = i * new_number_of_nodes\n","            end = (i + 1) * new_number_of_nodes\n","            adjacency_list.append(joint_tilda_adjacency_matrix[start:end, start:end])\n","            feature_list.append(batched_graphs.x[start:end, :])\n","        adjacency_list = list(map(lambda x: torch.unsqueeze(x, 0), adjacency_list))\n","        feature_list = list(map(lambda x: torch.unsqueeze(x, 0), feature_list))\n","        new_adjacecny = torch.cat(adjacency_list, dim=0)\n","        new_features = torch.cat(feature_list, dim=0)\n","        new_adjacecny = new_adjacecny.view(batch_size, new_number_of_nodes,new_number_of_nodes)\n","\n","        return new_adjacecny, new_features  \n","\n","    \n","    def forward(self, batched_graphs):\n","        new_adjacecny, new_features = self.computational_matricess(batched_graphs)\n","        \n","        #if batch is not None:\n","        #    max_dim = 1\n","        #else:\n","        #    max_dim = 0\n","\n","        new_features = new_features.to(torch.float32)\n","\n","        new_X, new_adjacency_1 = self.diffpool_layer_1(new_features, new_adjacecny)\n","        graph_sage1_output = self.graph_sage_1(new_X, new_adjacency_1)\n","\n","        new_X_2, new_adjacency_2 = self.diffpool_layer_2(graph_sage1_output, new_adjacency_1)\n","        graph_sage2_output = self.graph_sage_2(new_X_2, new_adjacency_2)\n","\n","        #graph_sage2_output = graph_sage2_output.view((len(graph), self.diffpool_layers_new_num_nodes[1], self.diffpool_layers_dim[1][1]))\n","        graph_sage2_output, q = torch.max(graph_sage2_output, dim=1, keepdim=True)\n","\n","        linear1_output = self.lin1(graph_sage2_output)\n","        linear1_output = self.act_fun(linear1_output)\n","\n","        linear2_output = self.lin2(linear1_output)\n","        linear2_output = self.act_fun(linear2_output)\n","\n","        linear3_output = self.lin3(linear2_output)\n","        linear3_output = self.act_fun_softmax(linear3_output, dim=2)\n","\n","        \n","        return linear3_output\n","\n","\n","\n","\n","dataset = TUDataset(root='data/TUDataset', name='MUTAG')\n","sum=0\n","length_list = []\n","for i in range(len(dataset)):\n","    sum = sum + len(dataset[i].x)\n","    length_list.append(len(dataset[i].x))\n","\n","#print(\"max number of nodes in graphs: \", max(length_list))\n","batch_size = 32\n","new_num_nodes = 10#int(length_list[0] / batch_size)\n","print(\"new_num_nodes: \", new_num_nodes)\n","\n","node_feat_size = len(dataset[0].x[0])\n","\n","hid_dim = 7\n","\n","diffpool_model_example = DIFFPOOL_Model(diffpool_layers_dim=[[node_feat_size, hid_dim], [hid_dim, node_feat_size]], diffpool_layers_new_num_nodes=[new_num_nodes, new_num_nodes], \n","                                        Weight_Initializer=1, Bias=True, num_classes=2, dropout_rate=0, normalize_embedding=True, aggregation='mean', \n","                                        act_fun='ReLu')\n","\n","batched_dataset = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n","\n","for batched_graph in batched_dataset:\n","    x, edge_index, batch, y = batched_graph.x, batched_graph.edge_index, batched_graph.batch, batched_graph.y\n","    linear3_output = diffpool_model_example(batched_graph)\n","    print(\"Final Output: \", linear3_output)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OjaXBTSXO85P","executionInfo":{"status":"ok","timestamp":1678051505895,"user_tz":-60,"elapsed":272,"user":{"displayName":"Ehsan Mobaraki","userId":"06314958514195310048"}},"outputId":"5e0fc8e8-826a-408a-add3-67bddb884744"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["new_num_nodes:  10\n","ReLu is Selected.\n","z_l:  torch.Size([32, 18, 7])\n","s_l:  torch.Size([32, 18, 10])\n","z_l:  torch.Size([32, 10, 7])\n","s_l:  torch.Size([32, 10, 1])\n","Final Output:  tensor([[[0.4560, 0.5440]],\n","\n","        [[0.1369, 0.8631]],\n","\n","        [[0.1369, 0.8631]],\n","\n","        [[0.1369, 0.8631]],\n","\n","        [[0.1369, 0.8631]],\n","\n","        [[0.3093, 0.6907]],\n","\n","        [[0.4560, 0.5440]],\n","\n","        [[0.1369, 0.8631]],\n","\n","        [[0.3093, 0.6907]],\n","\n","        [[0.4560, 0.5440]],\n","\n","        [[0.4560, 0.5440]],\n","\n","        [[0.1369, 0.8631]],\n","\n","        [[0.1369, 0.8631]],\n","\n","        [[0.1369, 0.8631]],\n","\n","        [[0.4560, 0.5440]],\n","\n","        [[0.1369, 0.8631]],\n","\n","        [[0.1369, 0.8631]],\n","\n","        [[0.1369, 0.8631]],\n","\n","        [[0.1369, 0.8631]],\n","\n","        [[0.3342, 0.6658]],\n","\n","        [[0.4560, 0.5440]],\n","\n","        [[0.4560, 0.5440]],\n","\n","        [[0.3761, 0.6239]],\n","\n","        [[0.4560, 0.5440]],\n","\n","        [[0.1369, 0.8631]],\n","\n","        [[0.4560, 0.5440]],\n","\n","        [[0.3093, 0.6907]],\n","\n","        [[0.3093, 0.6907]],\n","\n","        [[0.3342, 0.6658]],\n","\n","        [[0.4560, 0.5440]],\n","\n","        [[0.4560, 0.5440]],\n","\n","        [[0.4560, 0.5440]]], grad_fn=<SoftmaxBackward0>)\n","z_l:  torch.Size([32, 18, 7])\n","s_l:  torch.Size([32, 18, 10])\n","z_l:  torch.Size([32, 10, 7])\n","s_l:  torch.Size([32, 10, 1])\n","Final Output:  tensor([[[0.2481, 0.7519]],\n","\n","        [[0.3342, 0.6658]],\n","\n","        [[0.1369, 0.8631]],\n","\n","        [[0.4560, 0.5440]],\n","\n","        [[0.4560, 0.5440]],\n","\n","        [[0.3093, 0.6907]],\n","\n","        [[0.1369, 0.8631]],\n","\n","        [[0.1369, 0.8631]],\n","\n","        [[0.1369, 0.8631]],\n","\n","        [[0.4560, 0.5440]],\n","\n","        [[0.4560, 0.5440]],\n","\n","        [[0.1369, 0.8631]],\n","\n","        [[0.4560, 0.5440]],\n","\n","        [[0.3093, 0.6907]],\n","\n","        [[0.1369, 0.8631]],\n","\n","        [[0.1369, 0.8631]],\n","\n","        [[0.1369, 0.8631]],\n","\n","        [[0.1369, 0.8631]],\n","\n","        [[0.4560, 0.5440]],\n","\n","        [[0.4560, 0.5440]],\n","\n","        [[0.1369, 0.8631]],\n","\n","        [[0.4560, 0.5440]],\n","\n","        [[0.1369, 0.8631]],\n","\n","        [[0.1369, 0.8631]],\n","\n","        [[0.1369, 0.8631]],\n","\n","        [[0.4560, 0.5440]],\n","\n","        [[0.4560, 0.5440]],\n","\n","        [[0.1369, 0.8631]],\n","\n","        [[0.3093, 0.6907]],\n","\n","        [[0.3093, 0.6907]],\n","\n","        [[0.1369, 0.8631]],\n","\n","        [[0.4560, 0.5440]]], grad_fn=<SoftmaxBackward0>)\n","z_l:  torch.Size([32, 18, 7])\n","s_l:  torch.Size([32, 18, 10])\n","z_l:  torch.Size([32, 10, 7])\n","s_l:  torch.Size([32, 10, 1])\n","Final Output:  tensor([[[0.4560, 0.5440]],\n","\n","        [[0.1369, 0.8631]],\n","\n","        [[0.4560, 0.5440]],\n","\n","        [[0.1369, 0.8631]],\n","\n","        [[0.4560, 0.5440]],\n","\n","        [[0.1369, 0.8631]],\n","\n","        [[0.1369, 0.8631]],\n","\n","        [[0.3093, 0.6907]],\n","\n","        [[0.4560, 0.5440]],\n","\n","        [[0.1369, 0.8631]],\n","\n","        [[0.4560, 0.5440]],\n","\n","        [[0.1369, 0.8631]],\n","\n","        [[0.1369, 0.8631]],\n","\n","        [[0.4560, 0.5440]],\n","\n","        [[0.4560, 0.5440]],\n","\n","        [[0.3093, 0.6907]],\n","\n","        [[0.3093, 0.6907]],\n","\n","        [[0.1369, 0.8631]],\n","\n","        [[0.1369, 0.8631]],\n","\n","        [[0.4560, 0.5440]],\n","\n","        [[0.4560, 0.5440]],\n","\n","        [[0.4560, 0.5440]],\n","\n","        [[0.4560, 0.5440]],\n","\n","        [[0.1369, 0.8631]],\n","\n","        [[0.4560, 0.5440]],\n","\n","        [[0.4560, 0.5440]],\n","\n","        [[0.4560, 0.5440]],\n","\n","        [[0.4560, 0.5440]],\n","\n","        [[0.1369, 0.8631]],\n","\n","        [[0.4560, 0.5440]],\n","\n","        [[0.4560, 0.5440]],\n","\n","        [[0.3093, 0.6907]]], grad_fn=<SoftmaxBackward0>)\n","z_l:  torch.Size([32, 18, 7])\n","s_l:  torch.Size([32, 18, 10])\n","z_l:  torch.Size([32, 10, 7])\n","s_l:  torch.Size([32, 10, 1])\n","Final Output:  tensor([[[0.4560, 0.5440]],\n","\n","        [[0.1369, 0.8631]],\n","\n","        [[0.1369, 0.8631]],\n","\n","        [[0.4560, 0.5440]],\n","\n","        [[0.4560, 0.5440]],\n","\n","        [[0.1369, 0.8631]],\n","\n","        [[0.4560, 0.5440]],\n","\n","        [[0.4560, 0.5440]],\n","\n","        [[0.4560, 0.5440]],\n","\n","        [[0.1369, 0.8631]],\n","\n","        [[0.4560, 0.5440]],\n","\n","        [[0.1369, 0.8631]],\n","\n","        [[0.4560, 0.5440]],\n","\n","        [[0.1369, 0.8631]],\n","\n","        [[0.1369, 0.8631]],\n","\n","        [[0.3093, 0.6907]],\n","\n","        [[0.4560, 0.5440]],\n","\n","        [[0.3093, 0.6907]],\n","\n","        [[0.1369, 0.8631]],\n","\n","        [[0.1369, 0.8631]],\n","\n","        [[0.3342, 0.6658]],\n","\n","        [[0.3093, 0.6907]],\n","\n","        [[0.3342, 0.6658]],\n","\n","        [[0.1369, 0.8631]],\n","\n","        [[0.1369, 0.8631]],\n","\n","        [[0.1369, 0.8631]],\n","\n","        [[0.1369, 0.8631]],\n","\n","        [[0.4560, 0.5440]],\n","\n","        [[0.1369, 0.8631]],\n","\n","        [[0.4560, 0.5440]],\n","\n","        [[0.1369, 0.8631]],\n","\n","        [[0.1369, 0.8631]]], grad_fn=<SoftmaxBackward0>)\n","z_l:  torch.Size([32, 15, 7])\n","s_l:  torch.Size([32, 15, 10])\n","z_l:  torch.Size([32, 10, 7])\n","s_l:  torch.Size([32, 10, 1])\n","Final Output:  tensor([[[0.1369, 0.8631]],\n","\n","        [[0.4560, 0.5440]],\n","\n","        [[0.4560, 0.5440]],\n","\n","        [[0.4560, 0.5440]],\n","\n","        [[0.2024, 0.7976]],\n","\n","        [[0.4560, 0.5440]],\n","\n","        [[0.1369, 0.8631]],\n","\n","        [[0.3093, 0.6907]],\n","\n","        [[0.4560, 0.5440]],\n","\n","        [[0.1369, 0.8631]],\n","\n","        [[0.1369, 0.8631]],\n","\n","        [[0.1369, 0.8631]],\n","\n","        [[0.1369, 0.8631]],\n","\n","        [[0.3093, 0.6907]],\n","\n","        [[0.2024, 0.7976]],\n","\n","        [[0.4560, 0.5440]],\n","\n","        [[0.3093, 0.6907]],\n","\n","        [[0.4560, 0.5440]],\n","\n","        [[0.1369, 0.8631]],\n","\n","        [[0.1369, 0.8631]],\n","\n","        [[0.1369, 0.8631]],\n","\n","        [[0.1369, 0.8631]],\n","\n","        [[0.3761, 0.6239]],\n","\n","        [[0.4560, 0.5440]],\n","\n","        [[0.1369, 0.8631]],\n","\n","        [[0.4560, 0.5440]],\n","\n","        [[0.1369, 0.8631]],\n","\n","        [[0.1369, 0.8631]],\n","\n","        [[0.1369, 0.8631]],\n","\n","        [[0.1369, 0.8631]],\n","\n","        [[0.3093, 0.6907]],\n","\n","        [[0.1369, 0.8631]]], grad_fn=<SoftmaxBackward0>)\n","z_l:  torch.Size([28, 18, 7])\n","s_l:  torch.Size([28, 18, 10])\n","z_l:  torch.Size([28, 10, 7])\n","s_l:  torch.Size([28, 10, 1])\n","Final Output:  tensor([[[0.1369, 0.8631]],\n","\n","        [[0.4560, 0.5440]],\n","\n","        [[0.1369, 0.8631]],\n","\n","        [[0.3093, 0.6907]],\n","\n","        [[0.3093, 0.6907]],\n","\n","        [[0.4560, 0.5440]],\n","\n","        [[0.4560, 0.5440]],\n","\n","        [[0.4560, 0.5440]],\n","\n","        [[0.4560, 0.5440]],\n","\n","        [[0.1369, 0.8631]],\n","\n","        [[0.1369, 0.8631]],\n","\n","        [[0.4560, 0.5440]],\n","\n","        [[0.1369, 0.8631]],\n","\n","        [[0.4560, 0.5440]],\n","\n","        [[0.4560, 0.5440]],\n","\n","        [[0.3093, 0.6907]],\n","\n","        [[0.4967, 0.5033]],\n","\n","        [[0.1369, 0.8631]],\n","\n","        [[0.1369, 0.8631]],\n","\n","        [[0.4560, 0.5440]],\n","\n","        [[0.4560, 0.5440]],\n","\n","        [[0.3093, 0.6907]],\n","\n","        [[0.1369, 0.8631]],\n","\n","        [[0.1369, 0.8631]],\n","\n","        [[0.3093, 0.6907]],\n","\n","        [[0.3093, 0.6907]],\n","\n","        [[0.1369, 0.8631]],\n","\n","        [[0.1369, 0.8631]]], grad_fn=<SoftmaxBackward0>)\n"]}]},{"cell_type":"code","source":["for batched_graph in batched_dataset:\n","    x, edge_index, batch, y = batched_graph.x, batched_graph.edge_index, batched_graph.batch, batched_graph.y\n","    tilda_adjacency_matrix = torch.tensor(to_scipy_sparse_matrix(batched_graph.edge_index).todense()) + torch.eye(len(torch.tensor(to_scipy_sparse_matrix(batched_graph.edge_index).todense())))\n","    #print(\"tilda_adjacency_matrix: \", tilda_adjacency_matrix.size())\n","    gnn_sage1_output = diffpool_model_example(batched_graph)\n","    #print(\"gnn_sage1_output: \", gnn_sage1_output.size())\n","    print(gnn_sage1_output)\n","    #break"],"metadata":{"id":"BxCgTV56GHY0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["output_diffpool = diffpool_model_example(dataset[2])\n","print(output_diffpool)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xsMEt15OYlVb","executionInfo":{"status":"ok","timestamp":1677972593815,"user_tz":-60,"elapsed":442,"user":{"displayName":"Ehsan Mobaraki","userId":"06314958514195310048"}},"outputId":"6986de0d-4be6-4e5f-918f-05086ac0baf3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[[0.5012, 0.4988],\n","         [0.5012, 0.4988],\n","         [0.5012, 0.4988],\n","         [0.5012, 0.4988],\n","         [0.5012, 0.4988],\n","         [0.5012, 0.4988],\n","         [0.5012, 0.4988]]], grad_fn=<SoftmaxBackward0>)\n"]}]},{"cell_type":"code","source":["def batching_adjacency(whole_graphs_adjacency, batch_feat, batch_size):\n","    \"\"\"\n","        transform a batched graph to batched adjacency tensor and node feature tensor\n","    \"\"\"\n","    #print(\"whole_graphs_adjacency.size()[0]: \", whole_graphs_adjacency.size()[0])\n","    new_number_of_nodes = int(whole_graphs_adjacency.size()[0] / batch_size)\n","    #print(batch_size)\n","    adjacency_list = []\n","    feature_list = []\n","    for i in range(batch_size):\n","        start = i * new_number_of_nodes\n","        end = (i + 1) * new_number_of_nodes\n","        adjacency_list.append(whole_graphs_adjacency[start:end, start:end])\n","        feature_list.append(batch_feat[start:end, :])\n","    adjacency_list = list(map(lambda x: torch.unsqueeze(x, 0), adjacency_list))\n","    feature_list = list(map(lambda x: torch.unsqueeze(x, 0), feature_list))\n","    new_adjacecny = torch.cat(adjacency_list, dim=0)\n","    new_features = torch.cat(feature_list, dim=0)\n","\n","    return new_features, new_adjacecny"],"metadata":{"id":"z671puW8nF0H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["batch_size = 32\n","batched_dataset = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n","\n","for batched_graph in batched_dataset:\n","    x, edge_index, batch, y = batched_graph.x, batched_graph.edge_index, batched_graph.batch, batched_graph.y\n","    tilda_adjacency_matrix = torch.tensor(to_scipy_sparse_matrix(batched_graph.edge_index).todense()) + torch.eye(len(torch.tensor(to_scipy_sparse_matrix(batched_graph.edge_index).todense())))\n","    feat,adj = batching_adjacency(tilda_adjacency_matrix, x, batch_size)\n","    break\n","print(adj.size())\n","print(feat.size())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZLKQhMHbf10J","executionInfo":{"status":"ok","timestamp":1678023494677,"user_tz":-60,"elapsed":385,"user":{"displayName":"Ehsan Mobaraki","userId":"06314958514195310048"}},"outputId":"644b020b-2d50-4ad8-f097-d1c2eaae1dab"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["whole_graphs_adjacency.size()[0]:  585\n","32\n","torch.Size([32, 18, 18])\n","torch.Size([32, 18, 7])\n"]}]},{"cell_type":"code","source":["num_graphs=20\n","n = 7\n","adj=torch.randint(0, 1, (num_graphs, n, n))\n","offset, row, col = (batch.adj > 0).nonzero().t()\n","edge_weight = adj[offset, row, col]\n","row += offset * n\n","col += offset * n\n","edge_index = torch.stack([row, col], dim=0)\n","x = x.view(num_graphs * n, num_feats)\n","batch = torch.arange(0, num_graphs).view(-1, 1).repeat(1, n).view(-1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":232},"id":"VzfDXORP08LR","executionInfo":{"status":"error","timestamp":1677987162739,"user_tz":-60,"elapsed":287,"user":{"displayName":"Ehsan Mobaraki","userId":"06314958514195310048"}},"outputId":"20a1b679-5160-482d-9810-a1df473f57a0"},"execution_count":null,"outputs":[{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-118-038331eb113c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0madj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnum_graphs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0moffset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madj\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0medge_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0moffset\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'adj'"]}]},{"cell_type":"code","source":["        #tilda_degree_vector = torch.sum(tilda_adjacency_matrix, dim=1)\n","\n","        #k = tilda_degree_vector.size(0)\n","        #tilda_degree_matrix = torch.zeros(k, k)\n","        #tilda_degree_matrix.as_strided([k], [k + 1]).copy_(tilda_degree_vector)\n","        #tilda_degree_matrix = tilda_degree_matrix.to(torch.float32)\n","\n","        #ones_matrix = torch.ones_like(tilda_degree_matrix)\n","\n","        #reciprocal_tilda_degree_matrix = ones_matrix.div(tilda_degree_matrix)\n","        #reciprocal_tilda_degree_matrix = torch.nan_to_num(reciprocal_tilda_degree_matrix, nan=0, posinf=0, neginf=0)\n","\n","        #return tilda_adjacency_matrix, reciprocal_tilda_degree_matrix\n","        #tilda_degree_matrix = torch.nan_to_num(tilda_degree_matrix, nan=0, posinf=0, neginf=0)"],"metadata":{"id":"tSjrOjKSGI7J"},"execution_count":null,"outputs":[]}]}