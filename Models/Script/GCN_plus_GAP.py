# -*- coding: utf-8 -*-
"""GCN_plus_GAP.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-HWyU45KD6aPBQu9iA6RxWix4xUK2ox0
"""

# Install required packages.
import os
import torch
import importlib
from importlib import reload
from ast import mod
import argparse
import torch as th
import torch.nn as nn
import torch_geometric
import matplotlib.pyplot as plt
import torch.nn.functional as F
from torch.nn import Linear
from sklearn.model_selection import train_test_split
import numpy as np
import random
from torch_geometric.datasets import TUDataset
from torch_geometric.loader import DataLoader
import torch_geometric.nn as gnn
from sklearn import metrics
from time import perf_counter
import sys
py_path = '/data/cs.aau.dk/ey33jw/Explainability_Methods/Models/Script/Layers/'
sys.path.insert(0, py_path)

import GCN_Layer as gcn_layer
import GlobalAveragePooling as globalaveragepooling
import IdenticalPooling as identicalpooling


################################################################################
class GCN_plus_GAP_Model(torch.nn.Module):
    def __init__(self, GNN_layers, num_classes, Bias, act_fun, Weight_Initializer, dropout_rate, model_level, pred_hidden_dims=[]):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.GNN_layers = GNN_layers
        self.input_dim = GNN_layers[0]
        self.output_dim = GNN_layers[-1]
        self.num_GNN_layers = len(GNN_layers)
        self.num_classes = num_classes
        self.Bias = Bias

        super(GCN_plus_GAP_Model, self).__init__()
        print('GCN Layer Input_Dimension:', self.input_dim)

        print('GCN Layer Number_of_Layers:', self.num_GNN_layers)

        print('GCN Layer Output_Dimension:', self.output_dim)

        if act_fun == 'ReLu':
            self.act_fun = F.relu
            print('ReLu is Selected.')
        elif act_fun == 'eLu':
            self.act_fun = nn.functional.elu
            print('eLu is Selected.')

        self.GCN_Layers = torch.nn.ModuleList()

        for i in range(self.num_GNN_layers):
            if self.num_GNN_layers == 1:
                self.GCN_Layers.append(gcn_layer.GCN_Layer(self.input_dim, self.output_dim, Bias=self.Bias))
            elif self.num_GNN_layers > 1:
                if i == 0:
                    self.GCN_Layers.append(gcn_layer.GCN_Layer(self.input_dim, self.GNN_layers[i], Bias=self.Bias))
                elif i == self.num_GNN_layers-1:
                    self.GCN_Layers.append(gcn_layer.GCN_Layer(self.GNN_layers[i-1], self.output_dim, Bias=self.Bias))
                elif 0 < i < self.num_GNN_layers-1:
                    self.GCN_Layers.append(gcn_layer.GCN_Layer(self.GNN_layers[i-1], self.GNN_layers[i], Bias=self.Bias))
            else:
                print("please enter layer config")

        self.dropout = nn.Dropout(p=dropout_rate)

        if model_level == 'node':
            self.readout = identicalpooling.IdenticalPooling()
        else:
            self.readout = globalaveragepooling.GlobalAveragePooling()

        self.ffn = nn.Linear(self.output_dim, self.num_classes, bias=self.Bias)
        self.to(self.device)

        mean = 0
        std = 0.1
        self.initialize_weights(Weight_Initializer, self.Bias, mean, std)

    def initialize_weights(model, Weight_Initializer, Bias, mean, std):
        # 1. Xavier Normal_.  2. Kaiming Normal_.  3. Uniform (0,0.1std)
        if Weight_Initializer == 1:                                             #.      1. Xavier Normal_.
            for i,layers in enumerate(model.children()):
                if isinstance(layers, torch.nn.ModuleList):
                    for j, layer in enumerate(layers.modules()):
                        if isinstance(layer, gcn_layer.GCN_Layer):
                            torch.nn.init.xavier_normal_(layer.conv_params.weight.data)
                            if Bias:
                                layer.conv_params.bias.data.zero_()
                        else:
                            pass
                if isinstance(layers, torch.nn.Linear):
                    # print("Dense before message passing: ", layers.weight)
                    torch.nn.init.xavier_normal_(layers.weight)
                    if Bias:
                        layers.bias.data.zero_()
                elif isinstance(layers, (globalaveragepooling.GlobalAveragePooling)):
                    #print(i, "   GAP:      ",  layers)
                    pass
                elif isinstance(layers, (identicalpooling.IdenticalPooling)):
                    #print(i, "   IAP:      ",  layers)
                    pass

        if Weight_Initializer == 2:                                             #.      2. Kaiming Normal_.
            for i,layers in enumerate(model.children()):
                #print("no condition: ", i, layers)
                if isinstance(layers, torch.nn.ModuleList):
                    for j, layer in enumerate(layers.modules()):
                        if isinstance(layer, gcn_layer.GCN_Layer):
                            torch.nn.init.kaiming_normal_(layer.conv_params.weight.data)
                            if Bias:
                                layer.conv_params.bias.data.zero_()
                        else:
                            pass
                if isinstance(layers, torch.nn.Linear):
                    # print("Dense before message passing: ", layers.weight)
                    torch.nn.init.kaiming_normal_(layers.weight)
                    if Bias:
                        layers.bias.data.zero_()
                elif isinstance(layers, (globalaveragepooling.GlobalAveragePooling)):
                    #print(i, "   GAP:      ",  layers)
                    pass
                elif isinstance(layers, (identicalpooling.IdenticalPooling)):
                    #print(i, "   IAP:      ",  layers)
                    pass

        if Weight_Initializer == 3:                                             #.      3. Uniform (0,0.1std)
            for i,layers in enumerate(model.children()):
                if isinstance(layers, torch.nn.ModuleList):
                    for j, layer in enumerate(layers.modules()):
                        if isinstance(layer, gcn_layer.GCN_Layer):
                            torch.nn.init.normal_(layer.conv_params.weight.data, mean, std)
                            if Bias:
                                layer.conv_params.bias.data.zero_()
                        else:
                            pass
                if isinstance(layers, torch.nn.Linear):
                    # print("Dense before message passing: ", layers.weight)
                    torch.nn.init.normal_(layers.weight, mean, std)
                    if Bias:
                        layers.bias.data.zero_()
                elif isinstance(layers, (globalaveragepooling.GlobalAveragePooling)):
                    #print(i, "   GAP:      ",  layers)
                    pass
                elif isinstance(layers, (identicalpooling.IdenticalPooling)):
                    #print(i, "   IAP:      ",  layers)
                    pass

    def pad_sparse_tensor(self, sparse_tensor, pad, value):
        dense_tensor = sparse_tensor.to_dense()
        padded_dense_tensor = F.pad(dense_tensor, pad, mode='constant', value=value)
        padded_sparse_tensor = padded_dense_tensor.to_sparse().type(torch.float32)
        return padded_sparse_tensor

    def compute_degree_matrix(self, sparse_adj_matrix):
        degree_vector = torch.sparse.sum(sparse_adj_matrix, dim=1).to_dense()
        num_nodes = degree_vector.size(0)
        indices = torch.arange(num_nodes, device=sparse_adj_matrix.device)
        degree_matrix = torch.sparse_coo_tensor(torch.stack([indices, indices]),
                                                degree_vector,
                                                (num_nodes, num_nodes),
                                                device=sparse_adj_matrix.device)
        return degree_matrix

    def compute_reciprocal_sqrt_sparse_degree_matrix(self, degree_matrix):
        indices = degree_matrix._indices()
        values = degree_matrix._values()
        sqrt_values = torch.sqrt(values)
        reciprocal_sqrt_values = torch.reciprocal(sqrt_values)
        reciprocal_sqrt_degree_matrix = torch.sparse_coo_tensor(indices, reciprocal_sqrt_values, degree_matrix.size(), device=degree_matrix.device)
        return reciprocal_sqrt_degree_matrix

    def update_batch_tensor_for_fixed_size(self, batch, max_size):
        unique_graph_indices, counts = batch.unique(return_counts=True)
        new_batch = []

        for graph_index, count in zip(unique_graph_indices, counts):
            new_batch.extend([graph_index.item()] * count.item())
            new_batch.extend([graph_index.item()] * (max_size - count.item()))
        new_batch = np.array(new_batch)

        return torch.from_numpy(new_batch).to(batch.device)

    def computational_matrices(self, batched_graphs, edge_mask):
        node_features = batched_graphs.x
        edge_index = batched_graphs.edge_index
        batch_tensor = batched_graphs.batch

        if batch_tensor is not None:
            batch_tensor = batch_tensor.to(edge_index.device)
            batch_size = batch_tensor.max().item() + 1
            unique_graph_indices, counts = batch_tensor.unique(return_counts=True)
        else:
            batch_size = 1
            batch_tensor = torch.zeros(node_features.size(0), dtype=torch.long).to(edge_index.device)
            unique_graph_indices, counts = batch_tensor.unique(return_counts=True)

        max_graph_size = counts.max().item()

        adj_3d_list = []
        graph_3d_list = []
        degree_3d_list = []

        for graph_index in unique_graph_indices:
            one_graph_node_indices = (batch_tensor == graph_index).nonzero(as_tuple=True)[0]
            node_map = {node.item(): idx for idx, node in enumerate(one_graph_node_indices)}
            edge_index_intersection = (batch_tensor[edge_index[0]] == graph_index) & (batch_tensor[edge_index[1]] == graph_index)
            local_edge_index = edge_index[:, edge_index_intersection].clone()

            local_edge_index[0] = torch.tensor([node_map[n.item()] for n in local_edge_index[0]], dtype=torch.long, device=edge_index.device)
            local_edge_index[1] = torch.tensor([node_map[n.item()] for n in local_edge_index[1]], dtype=torch.long, device=edge_index.device)
            num_nodes = one_graph_node_indices.size(0)

            if edge_mask is None:
                adj_matrix = torch.sparse_coo_tensor(local_edge_index, torch.ones(local_edge_index.shape[1], dtype=torch.float32, device=edge_index.device), (num_nodes, num_nodes))
            else:
                local_edge_mask = edge_mask[edge_index_intersection]
                adj_matrix = torch.sparse_coo_tensor(local_edge_index, local_edge_mask, (num_nodes, num_nodes))

            identity_indices = torch.arange(num_nodes, device=adj_matrix.device)
            identity_values = torch.ones(num_nodes, dtype=torch.float32, device=adj_matrix.device)
            identity_sparse = torch.sparse_coo_tensor(torch.stack([identity_indices, identity_indices]), identity_values, (num_nodes, num_nodes))

            tilda_adj_matrix = adj_matrix + identity_sparse
            padding_offset = max_graph_size - num_nodes

            padded_tilda_adj_matrix = self.pad_sparse_tensor(tilda_adj_matrix, (0, padding_offset, 0, padding_offset), value=0).unsqueeze(0)
            adj_3d_list.append(padded_tilda_adj_matrix)

            one_graph_node_features = node_features[batch_tensor == graph_index]
            one_graph_node_features_3d = F.pad(one_graph_node_features, (0, 0, 0, padding_offset), value=0).unsqueeze(0)
            graph_3d_list.append(one_graph_node_features_3d)

            tilda_degree_matrix = self.compute_degree_matrix(tilda_adj_matrix)
            reciprocal_sqrt_tilda_degree_matrix = self.compute_reciprocal_sqrt_sparse_degree_matrix(tilda_degree_matrix)
            reciprocal_sqrt_tilda_degree_matrix = torch.nan_to_num(reciprocal_sqrt_tilda_degree_matrix, nan=0, neginf=0.0, posinf=0.0)
            padded_reciprocal_sqrt_tilda_degree_matrix = self.pad_sparse_tensor(reciprocal_sqrt_tilda_degree_matrix, (0, padding_offset, 0, padding_offset), value=0).unsqueeze(0)
            degree_3d_list.append(padded_reciprocal_sqrt_tilda_degree_matrix.to_dense())

        adjacency_batch = torch.cat(adj_3d_list, dim=0)
        padded_reciprocal_sqrt_degree = torch.cat(degree_3d_list, dim=0)
        new_feat_batch = torch.cat(graph_3d_list, dim=0).to_dense()

        return adjacency_batch, padded_reciprocal_sqrt_degree, new_feat_batch

    def forward(self, graph, edge_mask):
        x, edge_index, batch, y = graph.x, graph.edge_index, graph.batch, graph.y

        if batch is not None:
            graph_sizes = [len(graph[i].x) for i in range(len(graph))]
        else:
            graph_sizes = [len(graph.x)]

        Output_of_GNN_Layers = []
        new_adjacecny, padded_reciprocal_sqrt_degree, x = self.computational_matrices(graph, edge_mask)

        Output_of_Hidden_Layers = []
        for i in range(self.num_GNN_layers):
            x = self.GCN_Layers[i](x, new_adjacecny, padded_reciprocal_sqrt_degree)
            x = self.act_fun(x)
            x = self.dropout(x)
            Output_of_Hidden_Layers.append(x)
        # print("old batch: ", batch)
        # batch = self.update_batch_tensor_for_fixed_size(batch, x.size(1))
        # print("new batch: ", batch)
        pooling_layer_output = self.readout(x)
        ffn_output = self.ffn(pooling_layer_output)
        ffn_output = self.act_fun(ffn_output)

        # log_soft = F.log_softmax(ffn_output, dim=1)
        soft = F.softmax(ffn_output, dim=1)

        return Output_of_Hidden_Layers, pooling_layer_output, ffn_output, soft

# GNN_Model = GCN_plus_GAP_Model(model_level='graph', GNN_layers=[7, 7], num_classes=2, Bias=True, act_fun='ReLu',
#                                Weight_Initializer=3, dropout_rate=0.1)
# print('===================================================================================')
# print(GNN_Model)
# print('===================================================================================')
#