{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"L4","authorship_tag":"ABX9TyM8T3yL9TeAP6F4bf58lopT"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["import os\n","import torch\n","os.environ['TORCH'] = torch.__version__\n","print(torch.__version__)\n","\n","!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n","!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n","!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git\n","!pip install torch-cluster -f https://data.pyg.org/whl/torch-{torch.__version__}.html"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TEaiwHxqUW9g","executionInfo":{"status":"ok","timestamp":1716061755271,"user_tz":-120,"elapsed":37044,"user":{"displayName":"Ehsan Mobaraki","userId":"06314958514195310048"}},"outputId":"2196c9be-4823-499a-9b16-c1c47cbe021c"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["2.2.1+cu121\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m45.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for torch-geometric (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Looking in links: https://data.pyg.org/whl/torch-2.2.1+cu121.html\n","Collecting torch-cluster\n","  Downloading https://data.pyg.org/whl/torch-2.2.0%2Bcu121/torch_cluster-1.6.3%2Bpt22cu121-cp310-cp310-linux_x86_64.whl (3.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-cluster) (1.11.4)\n","Requirement already satisfied: numpy<1.28.0,>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from scipy->torch-cluster) (1.25.2)\n","Installing collected packages: torch-cluster\n","Successfully installed torch-cluster-1.6.3+pt22cu121\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xFV4KPwApAQF","executionInfo":{"status":"ok","timestamp":1716061776887,"user_tz":-120,"elapsed":21624,"user":{"displayName":"Ehsan Mobaraki","userId":"06314958514195310048"}},"outputId":"4ff26451-0849-41a8-dd31-2e70785ebcf4"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["from torch_geometric.utils import dropout\n","from torch_geometric.loader import DataLoader\n","import torch\n","import torch.nn as nn\n","from torch.autograd import Variable\n","import math\n","import torch.nn.functional as F\n","from torch.nn.parameter import Parameter\n","from torch_geometric.utils.convert import to_scipy_sparse_matrix\n","import torch_geometric\n","import networkx as nx\n","import numpy as np\n","from torch_geometric.nn import GCNConv\n","import sys\n","from torch_geometric.datasets import TUDataset\n","from torch_geometric.nn import global_add_pool\n","py_path = '/content/drive/MyDrive/Explainability Methods/Models/Script/Layers/'\n","sys.path.insert(0,py_path)\n","import GIN_MLP_Layers as gin_mlp_layers"],"metadata":{"id":"XuN-w8xQUfay","executionInfo":{"status":"ok","timestamp":1716061782687,"user_tz":-120,"elapsed":5813,"user":{"displayName":"Ehsan Mobaraki","userId":"06314958514195310048"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DCpiwgdJTGNa","executionInfo":{"status":"ok","timestamp":1716061785193,"user_tz":-120,"elapsed":2512,"user":{"displayName":"Ehsan Mobaraki","userId":"06314958514195310048"}},"outputId":"a8b5dc44-1462-4299-bb33-6b3abfcb301d"},"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading https://www.chrsmrrs.com/graphkerneldatasets/MUTAG.zip\n","Processing...\n","Done!\n"]},{"output_type":"stream","name":"stdout","text":["lin2_output_softmaxed:  torch.Size([2, 2])\n","tensor([[0.3742, 0.6258],\n","        [0.4460, 0.5540]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n"]}],"source":["from torch_geometric.utils import dropout\n","from torch_geometric.loader import DataLoader\n","import torch\n","import torch.nn as nn\n","from torch.autograd import Variable\n","import math\n","import torch.nn.functional as F\n","from torch.nn.parameter import Parameter\n","from torch_geometric.utils.convert import to_scipy_sparse_matrix\n","import torch_geometric\n","import networkx as nx\n","import numpy as np\n","from torch_geometric.nn import GCNConv\n","import sys\n","from torch_geometric.datasets import TUDataset\n","from torch_geometric.nn import global_add_pool\n","from scipy.sparse import csr_matrix\n","py_path = '/content/drive/MyDrive/Explainability Methods/Models/Script/Layers/'\n","sys.path.insert(0,py_path)\n","import GIN_MLP_Layers as gin_mlp_layers\n","\n","\n","class GlobalSUMPool(nn.Module):\n","\n","    def __init__(self):\n","        super().__init__()\n","\n","    def forward(self, x, batch):\n","        return torch_geometric.nn.global_add_pool(x, batch)\n","################################################################################\n","class IdenticalPool(nn.Module):\n","\n","    def __init__(self):\n","        super().__init__()\n","\n","    def forward(self, x, batch):\n","        return x\n","\n","################################################################################\n","class GIN_Model(nn.Module):\n","    def __init__(self, num_mlp_layers, Bias, num_slp_layers, mlp_input_dim, mlp_hid_dim, mlp_output_dim, mlp_act_fun, dropout_rate,\n","                 Weight_Initializer, joint_embeddings):\n","        super(GIN_Model, self).__init__()\n","        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","        self.mlp_input_dim = mlp_input_dim\n","        self.mlp_hid_dim = mlp_hid_dim\n","        self.mlp_output_dim = mlp_output_dim\n","        self.num_slp_layers = num_slp_layers\n","        self.mlp_act_fun = mlp_act_fun\n","        self.lin_act_fun = mlp_act_fun\n","        self.dropout_rate = dropout_rate\n","        self.Weight_Initializer = Weight_Initializer\n","\n","        self.num_mlp_layers = num_mlp_layers\n","        self.Bias = Bias\n","        self.joint_embeddings = joint_embeddings\n","\n","        self.eps = nn.Parameter(torch.zeros(self.num_mlp_layers)).to(self.device)\n","        self.gin_mlp_layers = nn.ModuleList().to(self.device)\n","        self.global_summing = GlobalSUMPool().to(self.device)\n","\n","        self.the_first_layer = nn.Linear(in_features=self.mlp_input_dim, out_features=self.mlp_hid_dim, bias=self.Bias).to(self.device)\n","        #if self.joint_embeddings:\n","        #    self.lin1 = nn.Linear(in_features=self.mlp_hid_dim * (self.num_mlp_layers + 1), out_features=self.mlp_hid_dim * (self.num_mlp_layers + 1))\n","        #    self.lin2 = nn.Linear(in_features=self.mlp_hid_dim * (self.num_mlp_layers + 1), out_features=self.mlp_output_dim)\n","        #else:\n","        self.lin1 = nn.Linear(in_features=self.mlp_hid_dim, out_features=self.mlp_hid_dim).to(self.device)\n","        self.lin2 = nn.Linear(in_features=self.mlp_hid_dim, out_features=self.mlp_output_dim).to(self.device)\n","        self.dorpout = nn.Dropout(p=dropout_rate).to(self.device)\n","        self.act_fun_softmax = F.softmax\n","\n","        for i in range(self.num_mlp_layers):\n","            a_new_layer = gin_mlp_layers.GIN_MLPs(num_slp_layers=self.num_slp_layers, mlp_input_dim=self.mlp_hid_dim, mlp_hid_dim=self.mlp_hid_dim, mlp_act_fun=self.mlp_act_fun, Bias=self.Bias).to(self.device)\n","            self.gin_mlp_layers.append(a_new_layer)\n","\n","        if self.lin_act_fun == 'ReLu':\n","            self.lin_act_fun = F.relu\n","            #print('ReLu is Selected.')\n","        elif self.lin_act_fun == 'eLu':\n","            self.lin_act_fun = nn.functional.elu\n","            #print('eLu is Selected.')\n","        elif self.lin_act_fun == 'tanh':\n","            self.lin_act_fun = torch.tanh\n","            #print('tanh is Selected.')\n","\n","        mean = 0\n","        std = 0.1\n","        self.initialize_weights(self.Weight_Initializer, Bias, mean, std)\n","\n","    def initialize_weights(model, Weight_Initializer, Bias, mean, std):\n","        # 1. Xavier Normal_.  2. Kaiming Normal_.  3. Uniform (0,0.1std)\n","        if Weight_Initializer == 1:                                             #.      1. Xavier Normal_.\n","            for i, modules in enumerate(model.children()):\n","                if isinstance(modules, torch.nn.ModuleList):\n","                    for module in modules:\n","                        if isinstance(module, gin_mlp_layers.GIN_MLPs):\n","                            for final_module in module.children():\n","                                if isinstance(final_module, torch.nn.ModuleList):\n","                                    for layers in final_module:\n","                                        if isinstance(layers, torch.nn.Linear):\n","                                            torch.nn.init.xavier_normal_(layers.weight)\n","                                            #layers.weight.data.zero_()\n","                                            #print(layers.weight)\n","                                            if Bias:\n","                                                layers.bias.data.zero_()\n","                                                #print(\"ok\")\n","                                        elif isinstance(layers, torch.nn.BatchNorm1d):\n","                                            #print(\"ok\")\n","                                            pass\n","                elif isinstance(modules, torch.nn.Linear):\n","                    #print(\"predict layer\")\n","                    #modules.weight.data.zero_()\n","                    #print(modules.weight)\n","                    torch.nn.init.xavier_normal_(modules.weight)\n","                elif isinstance(modules, GlobalSUMPool):\n","                    #print(\"GlobalSUMPool\")\n","                    #print(modules)\n","                    pass\n","                elif isinstance(modules, nn.Dropout):\n","                    #print(\"Dropout\")\n","                    #print(modules)\n","                    pass\n","                else:\n","                    pass\n","\n","        if Weight_Initializer == 2:                                             #.      2. Kaiming Normal_.\n","            for i, modules in enumerate(model.children()):\n","                if isinstance(modules, torch.nn.ModuleList):\n","                    for module in modules:\n","                        if isinstance(module, gin_mlp_layers.GIN_MLPs):\n","                            for final_module in module.children():\n","                                if isinstance(final_module, torch.nn.ModuleList):\n","                                    for layers in final_module:\n","                                        if isinstance(layers, torch.nn.Linear):\n","                                            torch.nn.init.kaiming_normal_(layers.weight)\n","                                            #layers.weight.data.zero_()\n","                                            #print(layers.weight)\n","                                            if Bias:\n","                                                layers.bias.data.zero_()\n","                                                #print(\"ok\")\n","                                        elif isinstance(layers, torch.nn.BatchNorm1d):\n","                                            #print(\"ok\")\n","                                            pass\n","                elif isinstance(modules, torch.nn.Linear):\n","                    #print(\"predict layer\")\n","                    #modules.weight.data.zero_()\n","                    #print(modules.weight)\n","                    torch.nn.init.kaiming_normal_(modules.weight)\n","                elif isinstance(modules, GlobalSUMPool):\n","                    #print(\"GlobalSUMPool\")\n","                    #print(modules)\n","                    pass\n","                elif isinstance(modules, nn.Dropout):\n","                    #print(\"Dropout\")\n","                    #print(modules)\n","                    pass\n","                else:\n","                    pass\n","\n","        if Weight_Initializer == 3:                                             #.      3. Uniform (0,0.1std)\n","            for i, modules in enumerate(model.children()):\n","                if isinstance(modules, torch.nn.ModuleList):\n","                    for module in modules:\n","                        if isinstance(module, gin_mlp_layers.GIN_MLPs):\n","                            for final_module in module.children():\n","                                if isinstance(final_module, torch.nn.ModuleList):\n","                                    for layers in final_module:\n","                                        if isinstance(layers, torch.nn.Linear):\n","                                            torch.nn.init.normal_(layers.weight, mean=mean, std=std)\n","                                            #layers.weight.data.zero_()\n","                                            #print(layers.weight)\n","                                            if Bias:\n","                                                layers.bias.data.zero_()\n","                                                #print(\"ok\")\n","                                        elif isinstance(layers, torch.nn.BatchNorm1d):\n","                                            #print(\"ok\")\n","                                            pass\n","                elif isinstance(modules, torch.nn.Linear):\n","                    #print(\"predict layer\")\n","                    #modules.weight.data.zero_()\n","                    #print(modules.weight)\n","                    torch.nn.init.normal_(modules.weight, mean=mean, std=std)\n","                elif isinstance(modules, GlobalSUMPool):\n","                    #print(\"GlobalSUMPool\")\n","                    #print(modules)\n","                    pass\n","                elif isinstance(modules, nn.Dropout):\n","                    #print(\"Dropout\")\n","                    #print(modules)\n","                    pass\n","                else:\n","                    pass\n","\n","\n","    def gin_neighborhood_aggregation(self, h, batched_graphs, edge_mask):\n","\n","        #joint_tilda_adjacency_matrix = torch.tensor(to_scipy_sparse_matrix(batched_graphs.edge_index).todense())# + torch.eye(len(torch.tensor(to_scipy_sparse_matrix(batched_graphs.edge_index).todense())))\n","        if edge_mask == None:\n","            joint_tilda_adjacency_matrix = torch.tensor(to_scipy_sparse_matrix(batched_graphs.edge_index).todense())\n","        else:\n","            # edge_mask = edge_mask.to(self.device)\n","            joint_tilda_adjacency_matrix = torch.tensor(csr_matrix((np.array(edge_mask), (np.array(batched_graphs.edge_index[0]), np.array(batched_graphs.edge_index[1])))).todense())\n","\n","        joint_tilda_adjacency_matrix = joint_tilda_adjacency_matrix.type(torch.float32).to(self.device)\n","        if batched_graphs.batch == None:\n","            batch_size = 1\n","        else:\n","            batch_size = batched_graphs.num_graphs\n","\n","        pooled = torch.mm(joint_tilda_adjacency_matrix, h)\n","\n","\n","        return joint_tilda_adjacency_matrix, pooled\n","\n","\n","    def gin_layer_process_eps(self, h, layer, batched_graphs, edge_mask):\n","\n","        joint_tilda_adjacency_matrix, pooled = self.gin_neighborhood_aggregation(h, batched_graphs, edge_mask)\n","\n","        pooled = pooled + (1 + self.eps[layer])*h\n","        pooled = pooled.to(self.device)\n","        pooled_rep = self.gin_mlp_layers[layer](pooled)\n","\n","\n","        return pooled_rep\n","\n","    def merging_process(self, one_mlp, graph_sizes):\n","        new=[]\n","        start=0\n","        for j in range(len(graph_sizes)):\n","            end = start + graph_sizes[j]\n","            new.append(one_mlp[start:end])\n","            start = end\n","        return new\n","\n","    def reshape_mlps_outputs(self, mlps_output_embeds, graph_sizes):\n","        merged_mlps_output_embeds = []\n","        for i in range(len(graph_sizes)):\n","            merged_mlps_output_embeds.append([])\n","\n","        for i in range(len(mlps_output_embeds)):\n","            for j in range(len(mlps_output_embeds[i])):\n","                merged_mlps_output_embeds[j].extend(mlps_output_embeds[i][j])\n","        return merged_mlps_output_embeds\n","\n","\n","    def forward(self, batched_graphs, edge_mask):\n","        batched_graphs = batched_graphs.to(self.device)\n","        if batched_graphs.batch is not None:\n","            graph_sizes = [len(batched_graphs[i].x) for i in range(len(batched_graphs))]\n","        else:\n","            graph_sizes = [len(batched_graphs.x)]\n","\n","        mlps_output_embeds = []\n","        mlps_output_embeds_pooled = []\n","        hid_rep = self.the_first_layer(batched_graphs.x).to(self.device)\n","        mlps_output_embeds.append(hid_rep)\n","\n","        for layer in range(self.num_mlp_layers):\n","            hid_rep = self.gin_layer_process_eps(hid_rep, layer, batched_graphs, edge_mask).to(self.device)\n","            mlps_output_embeds.append(hid_rep)\n","\n","        mlp_outputs_globalSUMpooled = 0\n","        if self.joint_embeddings:\n","            for mlp_output in mlps_output_embeds:\n","                mlp_outputs_globalSUMpooled += self.global_summing(mlp_output, batched_graphs.batch)\n","        else:\n","            mlp_outputs_globalSUMpooled = self.global_summing(hid_rep, batched_graphs.batch)\n","\n","        lin1_output = self.lin1(mlp_outputs_globalSUMpooled)\n","        lin1_output = self.lin_act_fun(lin1_output)\n","\n","        lin1_output_dropouted = self.dorpout(lin1_output)\n","\n","        lin2_output = self.lin2(lin1_output_dropouted)\n","        lin2_output_softmaxed = self.act_fun_softmax(lin2_output, dim=-1)\n","\n","        return mlps_output_embeds, mlp_outputs_globalSUMpooled, lin1_output, lin1_output_dropouted, lin2_output, lin2_output_softmaxed\n","\n","\n","dataset = TUDataset(root='data/TUDataset', name='MUTAG')\n","batch_size = 2\n","node_feat_size = len(dataset[0].x[0])\n","batched_dataset = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n","gin_model_example = GIN_Model(num_mlp_layers=4, Bias=True, num_slp_layers=2, mlp_input_dim=node_feat_size, mlp_hid_dim=10,\n","                              mlp_output_dim=2, mlp_act_fun=\"ReLu\", dropout_rate=0.5, Weight_Initializer=3, joint_embeddings=False)\n","\n","\n","for batched_graphs in batched_dataset:\n","    #x, edge_index, batch, y = batched_graphs.x, batched_graphs.edge_index, batched_graphs.batch, batched_graphs.y\n","    mlps_output_embeds, mlp_outputs_globalSUMpooled, lin1_output, lin1_output_dropouted, lin2_output, lin2_output_softmaxed = gin_model_example(batched_graphs, None)\n","    print(\"lin2_output_softmaxed: \", lin2_output_softmaxed.size())\n","    print(lin2_output_softmaxed)\n","    break"]},{"cell_type":"code","source":["sum=0\n","for i in range(batch_size):\n","    sum = sum + len(dataset[i].x)\n","print(sum)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4v_11xrecrwh","executionInfo":{"status":"ok","timestamp":1678987488352,"user_tz":-60,"elapsed":6,"user":{"displayName":"Ehsan Mobaraki","userId":"06314958514195310048"}},"outputId":"8384fadd-304a-4b6e-8305-ffa7fd7fff10"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["338\n"]}]}]}