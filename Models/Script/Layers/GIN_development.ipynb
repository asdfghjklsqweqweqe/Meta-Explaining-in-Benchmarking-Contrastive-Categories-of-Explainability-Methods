{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM7zpBJ7fQhFNkjUfYftM7k"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tdC-8f8gbMgp","outputId":"64e49583-7dab-43c7-bdf8-ccf6b462888f","executionInfo":{"status":"ok","timestamp":1714077918416,"user_tz":-120,"elapsed":63150,"user":{"displayName":"Ehsan Mobaraki","userId":"06314958514195310048"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["2.2.1+cu121\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m48.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m61.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for torch-geometric (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"]}],"source":["import os\n","import torch\n","os.environ['TORCH'] = torch.__version__\n","print(torch.__version__)\n","\n","!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n","!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n","!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"UKnGS7OrKgDz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1714077939040,"user_tz":-120,"elapsed":20631,"user":{"displayName":"Ehsan Mobaraki","userId":"06314958514195310048"}},"outputId":"2c947c67-c37e-4509-e7bc-17ae80ac11fa"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["from torch_geometric.utils import dropout\n","from torch_geometric.loader import DataLoader\n","import torch\n","import torch.nn as nn\n","from torch.autograd import Variable\n","import math\n","import torch.nn.functional as F\n","from torch.nn.parameter import Parameter\n","from torch_geometric.utils.convert import to_scipy_sparse_matrix\n","import torch_geometric\n","import networkx as nx\n","import numpy as np\n","from torch_geometric.nn import GCNConv\n","import sys\n","from torch_geometric.datasets import TUDataset\n","from torch_geometric.nn import global_add_pool\n","py_path = '/content/drive/MyDrive/Explainability Methods/Models/Script/Layers/'\n","sys.path.insert(0,py_path)\n","import GIN_MLP_Layers as gin_mlp_layers\n","from scipy.sparse import csr_matrix\n","from sklearn import metrics\n","from time import perf_counter"],"metadata":{"id":"irQWh6uobTsZ","executionInfo":{"status":"ok","timestamp":1714077951747,"user_tz":-120,"elapsed":12717,"user":{"displayName":"Ehsan Mobaraki","userId":"06314958514195310048"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["class GIN_MLPs(nn.Module):\n","    def __init__(self, num_slp_layers, mlp_input_dim, mlp_hid_dim, mlp_act_fun, Bias):\n","        super(GIN_MLPs, self).__init__()\n","        self.mlp_input_dim = mlp_input_dim\n","        self.mlp_hid_dim = mlp_hid_dim\n","        self.num_slp_layers = num_slp_layers\n","        self.Bias = Bias\n","\n","        if mlp_act_fun == 'ReLu':\n","            self.mlp_act_fun = F.relu\n","        elif mlp_act_fun == 'eLu':\n","            self.mlp_act_fun = nn.functional.elu\n","        elif mlp_act_fun == 'tanh':\n","            self.mlp_act_fun = torch.tanh\n","\n","        self.gin_mlp_layers = torch.nn.ModuleList()\n","        self.gin_batch_normalization = torch.nn.ModuleList()\n","\n","\n","        if self.num_slp_layers == 1:\n","            self.gin_mlp_layers.append(nn.Linear(in_features=self.mlp_input_dim, out_features=self.mlp_hid_dim, bias=self.Bias))\n","            self.gin_batch_normalization.append(nn.BatchNorm1d(self.mlp_hid_dim))\n","\n","        elif self.num_slp_layers > 1:\n","            for i in range(self.num_slp_layers):\n","                if i == 0:\n","                    self.gin_mlp_layers.append(nn.Linear(in_features=self.mlp_input_dim, out_features=self.mlp_hid_dim, bias=self.Bias))\n","                    self.gin_batch_normalization.append(nn.BatchNorm1d(self.mlp_hid_dim))\n","\n","                else:\n","                    self.gin_mlp_layers.append(nn.Linear(in_features=self.mlp_hid_dim, out_features=self.mlp_hid_dim, bias=self.Bias))\n","                    self.gin_batch_normalization.append(nn.BatchNorm1d(self.mlp_hid_dim))\n","        else:\n","            print(\"please enter layer config\")\n","\n","    def forward(self, h):\n","        for i in range(self.num_slp_layers):\n","            layer = self.gin_mlp_layers[i](h)\n","            bnorm = self.gin_batch_normalization[i](layer)\n","            h = self.mlp_act_fun(bnorm)\n","        return h\n","\n","x_inp_dim = 7\n","mlp_example = GIN_MLPs(num_slp_layers=2, mlp_input_dim=x_inp_dim, mlp_hid_dim=10, mlp_act_fun=\"ReLu\", Bias=True)\n","x = torch.rand(20,x_inp_dim)\n","#x = x.permute(0, 2, 1)\n","y = mlp_example(x)\n","print(y.size())\n"],"metadata":{"id":"M6kRCdBkbbGZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1714079721528,"user_tz":-120,"elapsed":266,"user":{"displayName":"Ehsan Mobaraki","userId":"06314958514195310048"}},"outputId":"30262974-c033-465c-b925-bbc910d8202c"},"execution_count":56,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([20, 10])\n"]}]},{"cell_type":"code","source":["\"\"\"class GIN_MLPs(nn.Module):\n","    def __init__(self, num_slp_layers, mlp_input_dim, mlp_hid_dim, mlp_output_dim, mlp_act_fun, Bias):\n","        super(GIN_MLPs, self).__init__()\n","        self.mlp_input_dim = mlp_input_dim\n","        self.mlp_hid_dim = mlp_hid_dim\n","        self.mlp_output_dim = mlp_output_dim\n","        self.num_slp_layers = num_slp_layers\n","        self.Bias = Bias\n","\n","        if mlp_act_fun == 'ReLu':\n","            self.mlp_act_fun = F.relu\n","        elif mlp_act_fun == 'eLu':\n","            self.mlp_act_fun = nn.functional.elu\n","        elif mlp_act_fun == 'tanh':\n","            self.mlp_act_fun = torch.tanh\n","\n","        self.gin_mlp_layers = torch.nn.ModuleList()\n","        self.gin_batch_normalization = torch.nn.ModuleList()\n","\n","        if self.num_slp_layers == 1:\n","            self.gin_mlp_layers.append(nn.Linear(in_features=self.mlp_output_dim, out_features=self.mlp_output_dim, bias=self.Bias))\n","            self.gin_batch_normalization.append(nn.BatchNorm1d(self.mlp_output_dim))\n","        elif self.num_slp_layers > 1:\n","            for i in range(self.num_slp_layers):\n","                if i == 0:\n","                    self.gin_mlp_layers.append(nn.Linear(in_features=self.mlp_output_dim, out_features=self.mlp_hid_dim, bias=self.Bias))\n","                    self.gin_batch_normalization.append(nn.BatchNorm1d(num_features=self.mlp_hid_dim))\n","                elif i == self.num_slp_layers-1:\n","                    self.gin_mlp_layers.append(nn.Linear(in_features=self.mlp_hid_dim, out_features=self.mlp_output_dim, bias=self.Bias))\n","                    self.gin_batch_normalization.append(nn.BatchNorm1d(self.mlp_output_dim))\n","                elif 0 < i < self.num_slp_layers-1:\n","                    self.gin_mlp_layers.append(nn.Linear(in_features=self.mlp_hid_dim, out_features=self.mlp_hid_dim, bias=self.Bias))\n","                    self.gin_batch_normalization.append(nn.BatchNorm1d(self.mlp_hid_dim))\n","        else:\n","            print(\"please enter layer config\")\n","\n","    def forward(self, h):\n","\n","        for i in range(self.num_slp_layers):\n","            if i == 0:\n","                layer = self.gin_mlp_layers[i](h)\n","                layer = layer.permute(0, 2, 1)\n","                bnorm = self.gin_batch_normalization[i](layer)\n","                bnorm = bnorm.permute(0, 2, 1)\n","                h = self.mlp_act_fun(bnorm)\n","            else:\n","                h = self.mlp_act_fun(self.gin_mlp_layers[i](h))\n","        return h\n","\n","x_inp_dim = 7\n","mlp_example = GIN_MLPs(num_slp_layers=2, mlp_input_dim=x_inp_dim, mlp_hid_dim=17, mlp_output_dim=7, mlp_act_fun=\"ReLu\", Bias=True)\n","x = torch.rand(1,20,x_inp_dim)\n","#x = x.permute(0, 2, 1)\n","y = mlp_example(x)\n","print(y.size())\n","\"\"\""],"metadata":{"id":"CbGruDICiE4o"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# With Learnable Parameters\n","#m = nn.BatchNorm1d(100)\n","# Without Learnable Parameters\n","m = nn.BatchNorm1d(7, affine=False)\n","input = torch.randn(300, 100, 7)\n","input = input.permute(0, 2, 1)\n","output = m(input)\n","output = output.permute(0, 2, 1)\n","print(output.size())"],"metadata":{"id":"SPI3YxVB8HwN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class GlobalSUMPool(nn.Module):\n","\n","    def __init__(self):\n","        super().__init__()\n","\n","    def forward(self, x, batch):\n","        return torch_geometric.nn.global_add_pool(x, batch)\n","################################################################################\n","class IdenticalPool(nn.Module):\n","\n","    def __init__(self):\n","        super().__init__()\n","\n","    def forward(self, x, batch):\n","        return x\n","\n","################################################################################"],"metadata":{"id":"wOUiMbu00GJE","executionInfo":{"status":"ok","timestamp":1714079618718,"user_tz":-120,"elapsed":244,"user":{"displayName":"Ehsan Mobaraki","userId":"06314958514195310048"}}},"execution_count":44,"outputs":[]},{"cell_type":"code","source":["global_sum_pool = GlobalSUMPool()"],"metadata":{"id":"DU3D9D1P0H5A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["x = torch.arange(140)\n","x = x.view(2,10, 7)\n","print(x)\n","x = x.permute(0,1,2)\n","print(x)"],"metadata":{"id":"bwvPXa6a0Nwg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from IPython.core.display import deepcopy\n","class GlobalSUMPool(nn.Module):\n","\n","    def __init__(self):\n","        super().__init__()\n","\n","    def forward(self, x, batch):\n","        return torch_geometric.nn.global_add_pool(x, batch)\n","################################################################################\n","class IdenticalPool(nn.Module):\n","\n","    def __init__(self):\n","        super().__init__()\n","\n","    def forward(self, x, batch):\n","        return x\n","\n","################################################################################\n","from torch_geometric.utils import dropout\n","from torch_geometric.loader import DataLoader\n","import torch\n","import torch.nn as nn\n","from torch.autograd import Variable\n","import math\n","import torch_geometric\n","import torch.nn.functional as F\n","from torch.nn.parameter import Parameter\n","from torch_geometric.utils.convert import to_scipy_sparse_matrix\n","#from torch_geometric.utils.train_test_split_edges import torch_geometric\n","import networkx as nx\n","import numpy as np\n","from torch_geometric.nn import GCNConv\n","import sys\n","from torch_geometric.datasets import TUDataset\n","from torch_geometric.nn import global_add_pool\n","from scipy.sparse import csr_matrix\n","py_path = '/content/drive/MyDrive/Explainability Methods/Models/Script/Layers/'\n","sys.path.insert(0,py_path)\n","import GIN_MLP_Layers as gin_mlp_layers\n","\n","\n","class GlobalSUMPool(nn.Module):\n","\n","    def __init__(self):\n","        super().__init__()\n","\n","    def forward(self, x, batch):\n","        return torch_geometric.nn.global_add_pool(x, batch)\n","################################################################################\n","class IdenticalPool(nn.Module):\n","\n","    def __init__(self):\n","        super().__init__()\n","\n","    def forward(self, x, batch):\n","        return x\n","\n","################################################################################\n","class GIN_Model(nn.Module):\n","    def __init__(self, num_mlp_layers, Bias, num_slp_layers, mlp_input_dim, mlp_hid_dim, mlp_output_dim, mlp_act_fun, dropout_rate,\n","                 Weight_Initializer, joint_embeddings):\n","        super(GIN_Model, self).__init__()\n","\n","        self.mlp_input_dim = mlp_input_dim\n","        self.mlp_hid_dim = mlp_hid_dim\n","        self.mlp_output_dim = mlp_output_dim\n","        self.num_slp_layers = num_slp_layers\n","        self.mlp_act_fun = mlp_act_fun\n","        self.lin_act_fun = mlp_act_fun\n","        self.dropout_rate = dropout_rate\n","        self.Weight_Initializer = Weight_Initializer\n","\n","        self.num_mlp_layers = num_mlp_layers\n","        self.Bias = Bias\n","        self.joint_embeddings = joint_embeddings\n","\n","        self.eps = nn.Parameter(torch.zeros(self.num_mlp_layers))\n","        self.gin_mlp_layers = nn.ModuleList()\n","        self.global_summing = GlobalSUMPool()\n","\n","        self.the_first_layer = nn.Linear(in_features=self.mlp_input_dim, out_features=self.mlp_hid_dim, bias=self.Bias)\n","        #if self.joint_embeddings:\n","        #    self.lin1 = nn.Linear(in_features=self.mlp_hid_dim * (self.num_mlp_layers + 1), out_features=self.mlp_hid_dim * (self.num_mlp_layers + 1))\n","        #    self.lin2 = nn.Linear(in_features=self.mlp_hid_dim * (self.num_mlp_layers + 1), out_features=self.mlp_output_dim)\n","        #else:\n","        self.lin1 = nn.Linear(in_features=self.mlp_hid_dim, out_features=self.mlp_hid_dim)\n","        self.lin2 = nn.Linear(in_features=self.mlp_hid_dim, out_features=self.mlp_output_dim)\n","        self.dorpout = nn.Dropout(p=dropout_rate)\n","        self.act_fun_softmax = F.softmax\n","\n","        for i in range(self.num_mlp_layers):\n","            self.gin_mlp_layers.append(GIN_MLPs(num_slp_layers=self.num_slp_layers, mlp_input_dim=self.mlp_hid_dim,\n","                                                mlp_hid_dim=self.mlp_hid_dim, mlp_act_fun=self.mlp_act_fun, Bias=self.Bias))\n","\n","        if self.lin_act_fun == 'ReLu':\n","            self.lin_act_fun = F.relu\n","            #print('ReLu is Selected.')\n","        elif self.lin_act_fun == 'eLu':\n","            self.lin_act_fun = nn.functional.elu\n","            #print('eLu is Selected.')\n","        elif self.lin_act_fun == 'tanh':\n","            self.lin_act_fun = torch.tanh\n","            #print('tanh is Selected.')\n","\n","        mean = 0\n","        std = 0.1\n","        self.initialize_weights(self.Weight_Initializer, Bias, mean, std)\n","\n","    def initialize_weights(model, Weight_Initializer, Bias, mean, std):\n","        # 1. Xavier Normal_.  2. Kaiming Normal_.  3. Uniform (0,0.1std)\n","        if Weight_Initializer == 1:                                             #.      1. Xavier Normal_.\n","            for i, modules in enumerate(model.children()):\n","                if isinstance(modules, torch.nn.ModuleList):\n","                    for module in modules:\n","                        if isinstance(module, gin_mlp_layers.GIN_MLPs):\n","                            for final_module in module.children():\n","                                if isinstance(final_module, torch.nn.ModuleList):\n","                                    for layers in final_module:\n","                                        if isinstance(layers, torch.nn.Linear):\n","                                            torch.nn.init.xavier_normal_(layers.weight)\n","                                            #layers.weight.data.zero_()\n","                                            #print(layers.weight)\n","                                            if Bias:\n","                                                layers.bias.data.zero_()\n","                                                #print(\"ok\")\n","                                        elif isinstance(layers, torch.nn.BatchNorm1d):\n","                                            #print(\"ok\")\n","                                            pass\n","                elif isinstance(modules, torch.nn.Linear):\n","                    #print(\"predict layer\")\n","                    #modules.weight.data.zero_()\n","                    #print(modules.weight)\n","                    torch.nn.init.xavier_normal_(modules.weight)\n","                elif isinstance(modules, GlobalSUMPool):\n","                    #print(\"GlobalSUMPool\")\n","                    #print(modules)\n","                    pass\n","                elif isinstance(modules, nn.Dropout):\n","                    #print(\"Dropout\")\n","                    #print(modules)\n","                    pass\n","                else:\n","                    pass\n","\n","        if Weight_Initializer == 2:                                             #.      2. Kaiming Normal_.\n","            for i, modules in enumerate(model.children()):\n","                if isinstance(modules, torch.nn.ModuleList):\n","                    for module in modules:\n","                        if isinstance(module, gin_mlp_layers.GIN_MLPs):\n","                            for final_module in module.children():\n","                                if isinstance(final_module, torch.nn.ModuleList):\n","                                    for layers in final_module:\n","                                        if isinstance(layers, torch.nn.Linear):\n","                                            torch.nn.init.kaiming_normal_(layers.weight)\n","                                            #layers.weight.data.zero_()\n","                                            #print(layers.weight)\n","                                            if Bias:\n","                                                layers.bias.data.zero_()\n","                                                #print(\"ok\")\n","                                        elif isinstance(layers, torch.nn.BatchNorm1d):\n","                                            #print(\"ok\")\n","                                            pass\n","                elif isinstance(modules, torch.nn.Linear):\n","                    #print(\"predict layer\")\n","                    #modules.weight.data.zero_()\n","                    #print(modules.weight)\n","                    torch.nn.init.kaiming_normal_(modules.weight)\n","                elif isinstance(modules, GlobalSUMPool):\n","                    #print(\"GlobalSUMPool\")\n","                    #print(modules)\n","                    pass\n","                elif isinstance(modules, nn.Dropout):\n","                    #print(\"Dropout\")\n","                    #print(modules)\n","                    pass\n","                else:\n","                    pass\n","\n","        if Weight_Initializer == 3:                                             #.      3. Uniform (0,0.1std)\n","            for i, modules in enumerate(model.children()):\n","                if isinstance(modules, torch.nn.ModuleList):\n","                    for module in modules:\n","                        if isinstance(module, gin_mlp_layers.GIN_MLPs):\n","                            for final_module in module.children():\n","                                if isinstance(final_module, torch.nn.ModuleList):\n","                                    for layers in final_module:\n","                                        if isinstance(layers, torch.nn.Linear):\n","                                            torch.nn.init.normal_(layers.weight, mean=mean, std=std)\n","                                            #layers.weight.data.zero_()\n","                                            #print(layers.weight)\n","                                            if Bias:\n","                                                layers.bias.data.zero_()\n","                                                #print(\"ok\")\n","                                        elif isinstance(layers, torch.nn.BatchNorm1d):\n","                                            #print(\"ok\")\n","                                            pass\n","                elif isinstance(modules, torch.nn.Linear):\n","                    #print(\"predict layer\")\n","                    #modules.weight.data.zero_()\n","                    #print(modules.weight)\n","                    torch.nn.init.normal_(modules.weight, mean=mean, std=std)\n","                elif isinstance(modules, GlobalSUMPool):\n","                    #print(\"GlobalSUMPool\")\n","                    #print(modules)\n","                    pass\n","                elif isinstance(modules, nn.Dropout):\n","                    #print(\"Dropout\")\n","                    #print(modules)\n","                    pass\n","                else:\n","                    pass\n","\n","\n","    def gin_neighborhood_aggregation(self, h, batched_graphs, edge_mask):\n","\n","        #joint_tilda_adjacency_matrix = torch.tensor(to_scipy_sparse_matrix(batched_graphs.edge_index).todense())# + torch.eye(len(torch.tensor(to_scipy_sparse_matrix(batched_graphs.edge_index).todense())))\n","        if edge_mask == None:\n","            joint_tilda_adjacency_matrix = torch.tensor(to_scipy_sparse_matrix(batched_graphs.edge_index).todense())\n","        else:\n","            joint_tilda_adjacency_matrix = torch.tensor(csr_matrix((np.array(edge_mask), (np.array(batched_graphs.edge_index[0]), np.array(batched_graphs.edge_index[1])))).todense())\n","\n","        joint_tilda_adjacency_matrix = joint_tilda_adjacency_matrix.type(torch.float32)\n","        if batched_graphs.batch == None:\n","            batch_size = 1\n","        else:\n","            batch_size = batched_graphs.num_graphs\n","\n","        pooled = torch.mm(joint_tilda_adjacency_matrix, h)\n","\n","\n","        return joint_tilda_adjacency_matrix, pooled\n","\n","\n","    def gin_layer_process_eps(self, h, layer, batched_graphs, edge_mask):\n","\n","        joint_tilda_adjacency_matrix, pooled = self.gin_neighborhood_aggregation(h, batched_graphs, edge_mask)\n","\n","        pooled = pooled + (1 + self.eps[layer])*h\n","        pooled_rep = self.gin_mlp_layers[layer](pooled)\n","\n","\n","        return pooled_rep\n","\n","    def merging_process(self, one_mlp, graph_sizes):\n","        new=[]\n","        start=0\n","        for j in range(len(graph_sizes)):\n","            end = start + graph_sizes[j]\n","            new.append(one_mlp[start:end])\n","            start = end\n","        return new\n","\n","    def reshape_mlps_outputs(self, mlps_output_embeds, graph_sizes):\n","        merged_mlps_output_embeds = []\n","        for i in range(len(graph_sizes)):\n","            merged_mlps_output_embeds.append([])\n","\n","        for i in range(len(mlps_output_embeds)):\n","            for j in range(len(mlps_output_embeds[i])):\n","                merged_mlps_output_embeds[j].extend(mlps_output_embeds[i][j])\n","        return merged_mlps_output_embeds\n","\n","\n","    def forward(self, batched_graphs, edge_mask):\n","\n","        if batched_graphs.batch is not None:\n","            graph_sizes = [len(batched_graphs[i].x) for i in range(len(batched_graphs))]\n","        else:\n","            graph_sizes = [len(batched_graphs.x)]\n","\n","        mlps_output_embeds = []\n","        mlps_output_embeds_pooled = []\n","        hid_rep = self.the_first_layer(batched_graphs.x)\n","        mlps_output_embeds.append(hid_rep)\n","\n","        for layer in range(self.num_mlp_layers):\n","            hid_rep = self.gin_layer_process_eps(hid_rep, layer, batched_graphs, edge_mask)\n","            mlps_output_embeds.append(hid_rep)\n","\n","        mlp_outputs_globalSUMpooled = 0\n","        if self.joint_embeddings:\n","            for mlp_output in mlps_output_embeds:\n","                mlp_outputs_globalSUMpooled += self.global_summing(mlp_output, batched_graphs.batch)\n","        else:\n","            mlp_outputs_globalSUMpooled = self.global_summing(hid_rep, batched_graphs.batch)\n","\n","        lin1_output = self.lin1(mlp_outputs_globalSUMpooled)\n","        lin1_output = self.lin_act_fun(lin1_output)\n","\n","        lin1_output_dropouted = self.dorpout(lin1_output)\n","\n","        lin2_output = self.lin2(lin1_output_dropouted)\n","        lin2_output_softmaxed = self.act_fun_softmax(lin2_output, dim=-1)\n","\n","        return mlps_output_embeds, mlp_outputs_globalSUMpooled, lin1_output, lin1_output_dropouted, lin2_output, lin2_output_softmaxed\n","\n","\n","\n","mutag_dataset = TUDataset(root='data/TUDataset', name='MUTAG')\n","batch_size = 20\n","node_feat_size = len(mutag_dataset[0].x[0])\n","mutag_train_dataset = mutag_dataset[:150]\n","mutag_test_dataset = mutag_dataset[150:]\n","mutag_batched_train_dataloader = DataLoader(mutag_train_dataset, batch_size=batch_size, shuffle=True)\n","mutag_batched_test_dataloader = DataLoader(mutag_test_dataset, batch_size=batch_size, shuffle=True)\n","gin_model_example = GIN_Model(num_mlp_layers=4, Bias=True, num_slp_layers=2, mlp_input_dim=node_feat_size, mlp_hid_dim=8,\n","                              mlp_output_dim=2, mlp_act_fun=\"ReLu\", dropout_rate=0.5, Weight_Initializer=3, joint_embeddings=False)\n","#print(gin_model_example)\n","\n","for batched_graphs in mutag_batched_train_dataloader:\n","    #x, edge_index, batch, y = batched_graphs.x, batched_graphs.edge_index, batched_graphs.batch, batched_graphs.y\n","    mlps_output_embeds, mlp_outputs_globalSUMpooled, lin1_output, lin1_output_dropouted, lin2_output, lin2_output_softmaxed = gin_model_example(batched_graphs, None)\n","    print(\"lin2_output_softmaxed: \", lin2_output_softmaxed.size())\n","    print(lin2_output_softmaxed)\n","    break"],"metadata":{"id":"aLT-uUXqq_nX","executionInfo":{"status":"ok","timestamp":1714079729056,"user_tz":-120,"elapsed":262,"user":{"displayName":"Ehsan Mobaraki","userId":"06314958514195310048"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"d4eec8aa-e7e7-499d-9e55-3543721dbf84"},"execution_count":57,"outputs":[{"output_type":"stream","name":"stdout","text":["lin2_output_softmaxed:  torch.Size([20, 2])\n","tensor([[0.5915, 0.4085],\n","        [0.6043, 0.3957],\n","        [0.4643, 0.5357],\n","        [0.3353, 0.6647],\n","        [0.7954, 0.2046],\n","        [0.5611, 0.4389],\n","        [0.7308, 0.2692],\n","        [0.6453, 0.3547],\n","        [0.1647, 0.8353],\n","        [0.6432, 0.3568],\n","        [0.5698, 0.4302],\n","        [0.6988, 0.3012],\n","        [0.4305, 0.5695],\n","        [0.6571, 0.3429],\n","        [0.7512, 0.2488],\n","        [0.6883, 0.3117],\n","        [0.3994, 0.6006],\n","        [0.6057, 0.3943],\n","        [0.7891, 0.2109],\n","        [0.6881, 0.3119]], grad_fn=<SoftmaxBackward0>)\n"]}]},{"cell_type":"code","source":["GNN_Model_optimizer = torch.optim.Adam(gin_model_example.parameters(), lr=0.001, weight_decay=1e-4)"],"metadata":{"id":"10VWs9CayvgG","executionInfo":{"status":"ok","timestamp":1714079738414,"user_tz":-120,"elapsed":243,"user":{"displayName":"Ehsan Mobaraki","userId":"06314958514195310048"}}},"execution_count":58,"outputs":[]},{"cell_type":"code","source":["for batched_graphs in mutag_batched_train_dataloader:\n","    mlps_output_embeds, mlp_outputs_globalSUMpooled, lin1_output, lin1_output_dropouted, lin2_output, lin2_output_softmaxed = gin_model_example(batched_graphs, None)\n","    print(\"lin2_output_softmaxed: \", lin2_output_softmaxed.size())\n","    print(lin2_output_softmaxed)\n","    break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NsMxycm1eVGU","executionInfo":{"status":"ok","timestamp":1714079740343,"user_tz":-120,"elapsed":226,"user":{"displayName":"Ehsan Mobaraki","userId":"06314958514195310048"}},"outputId":"8ec1a053-f416-45d1-eadd-eb4cf80151f4"},"execution_count":59,"outputs":[{"output_type":"stream","name":"stdout","text":["lin2_output_softmaxed:  torch.Size([20, 2])\n","tensor([[0.5595, 0.4405],\n","        [0.7480, 0.2520],\n","        [0.5598, 0.4402],\n","        [0.5317, 0.4683],\n","        [0.6762, 0.3238],\n","        [0.5814, 0.4186],\n","        [0.3324, 0.6676],\n","        [0.6984, 0.3016],\n","        [0.2218, 0.7782],\n","        [0.5346, 0.4654],\n","        [0.6427, 0.3573],\n","        [0.4228, 0.5772],\n","        [0.6610, 0.3390],\n","        [0.3636, 0.6364],\n","        [0.0402, 0.9598],\n","        [0.5698, 0.4302],\n","        [0.5721, 0.4279],\n","        [0.5876, 0.4124],\n","        [0.5397, 0.4603],\n","        [0.6489, 0.3511]], grad_fn=<SoftmaxBackward0>)\n"]}]},{"cell_type":"code","source":["criterion = torch.nn.CrossEntropyLoss()\n","def loss_calculations(preds, gtruth):\n","    loss_per_epoch = criterion(preds, gtruth)\n","    return loss_per_epoch\n"],"metadata":{"id":"J4fbv6JNMzDB","executionInfo":{"status":"ok","timestamp":1714079741880,"user_tz":-120,"elapsed":332,"user":{"displayName":"Ehsan Mobaraki","userId":"06314958514195310048"}}},"execution_count":60,"outputs":[]},{"cell_type":"code","source":["def train_step(data):\n","    GNN_Model_loss_batch = []\n","    Pred_Labels = []\n","    Real_Labels = []\n","\n","    gin_model_example.train()\n","    gin_model_example.zero_grad()\n","    #torch.autograd.set_detect_anomaly(True)\n","    for batch_of_graphs in data:\n","\n","        mlps_output_embeds, mlp_outputs_globalSUMpooled, lin1_output, lin1_output_dropouted, lin2_output, softmaxed_h2 = gin_model_example(batch_of_graphs, None)\n","        batch_loss = loss_calculations(softmaxed_h2, batch_of_graphs.y)\n","\n","        #print(softmaxed_h2, softmaxed_h2.argmax(dim=1))\n","        Pred_Labels.extend(softmaxed_h2.argmax(dim=1).detach().tolist())\n","        Real_Labels.extend(batch_of_graphs.y.detach().tolist())\n","        GNN_Model_loss_batch.append(batch_loss)\n","\n","        batch_loss.backward()\n","        #print(gin_model_example.state_dict()['gin_mlp_layers.0.gin_mlp_layers.0.weight'])\n","        GNN_Model_optimizer.step()\n","\n","    return torch.mean(torch.tensor(GNN_Model_loss_batch)), metrics.accuracy_score(Real_Labels, Pred_Labels), gin_model_example\n"],"metadata":{"id":"9UkhcEcwy0Mh","executionInfo":{"status":"ok","timestamp":1714079742500,"user_tz":-120,"elapsed":2,"user":{"displayName":"Ehsan Mobaraki","userId":"06314958514195310048"}}},"execution_count":61,"outputs":[]},{"cell_type":"code","source":["#from IPython.display import Javascript  # Restrict height of output cell.\n","#display(Javascript('''google.colab.output.setIframeHeight(0, true, {maxHeight: 300})'''))\n","\n","\n","SA_Model_training_time_per_epoch = []\n","SA_Model_training_Acc_per_epoch = []\n","\n","def train(EPOCHS, load_index, data):\n","    SA_training_loss_per_epoch = []\n","\n","    for epoch in range(EPOCHS):\n","\n","        start_generation = perf_counter()\n","        SA_model_training_loss, training_acc, gin_model_example = train_step(data)\n","        SA_Model_training_time_per_epoch.append(perf_counter() - start_generation)\n","        SA_Model_training_Acc_per_epoch.append(training_acc)\n","\n","        print(f'Epoch: {epoch+1:03d}, Model Loss: {SA_model_training_loss:.4f} Accuracy: {training_acc}')\n","\n","        SA_training_loss_per_epoch.append(SA_model_training_loss)\n","\n","\n","        #if (epoch + load_index + 1) % Visualization_Parameter == 0 and epoch > 0:\n","        #    visualize_losses(SA_training_loss_per_epoch, epoch + load_index + 1)\n","        #if (epoch + load_index + 1) % Model_Saving_Parameter == 0 and epoch > 0:\n","        #    torch.save({'epoch': epoch+load_index+1, 'model_state_dict': GNN_Model.state_dict(), 'optimizer_state_dict': GNN_Model_optimizer.state_dict(), 'loss': SA_training_loss_per_epoch,}, \"/content/drive/My Drive/Explainability Methods/\" + str(Explainability_name) + \" on \" + str(Task_name) + \"/Model/\" + File_Name + str(epoch + load_index + 1)+\".pt\")\n","    return gin_model_example\n","\n"],"metadata":{"id":"3O8obnFXzNLv","executionInfo":{"status":"ok","timestamp":1714079744018,"user_tz":-120,"elapsed":291,"user":{"displayName":"Ehsan Mobaraki","userId":"06314958514195310048"}}},"execution_count":62,"outputs":[]},{"cell_type":"code","source":["print(gin_model_example.state_dict()['gin_mlp_layers.0.gin_mlp_layers.0.weight'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"efdJvO11L8qD","executionInfo":{"status":"ok","timestamp":1714079747425,"user_tz":-120,"elapsed":237,"user":{"displayName":"Ehsan Mobaraki","userId":"06314958514195310048"}},"outputId":"ead40101-be03-4474-aa9f-36e73501273c"},"execution_count":63,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[-0.1597, -0.2933, -0.0153,  0.0976,  0.3306, -0.0476,  0.0958,  0.2779],\n","        [ 0.2149,  0.0996,  0.0077,  0.2491,  0.0335,  0.1778, -0.0147,  0.3481],\n","        [ 0.0018,  0.0817, -0.0168, -0.2734,  0.3361,  0.2240,  0.2341, -0.0358],\n","        [-0.1962, -0.2848, -0.0325,  0.1340,  0.2170,  0.0180,  0.0260, -0.3439],\n","        [ 0.2158,  0.3150,  0.0214,  0.0277, -0.3298,  0.1835,  0.1944, -0.2944],\n","        [ 0.0066, -0.0089,  0.2982,  0.2753, -0.1625,  0.2939,  0.1651, -0.1186],\n","        [ 0.1905, -0.1403, -0.1574,  0.2863,  0.0099, -0.2352,  0.0642, -0.0143],\n","        [-0.1220, -0.2692, -0.1532, -0.2907, -0.1294, -0.0472,  0.1681, -0.0118]])\n"]}]},{"cell_type":"code","source":["EPOCHS = 300\n","load_index = 0\n","\n","gin_model_example = train(EPOCHS, load_index, mutag_batched_train_dataloader)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"alNoSkvzzPQa","executionInfo":{"status":"ok","timestamp":1714079792454,"user_tz":-120,"elapsed":43931,"user":{"displayName":"Ehsan Mobaraki","userId":"06314958514195310048"}},"outputId":"0890152d-1050-463d-cb58-96f683aa5cde"},"execution_count":64,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 001, Model Loss: 0.7301 Accuracy: 0.4533333333333333\n","Epoch: 002, Model Loss: 0.7029 Accuracy: 0.5266666666666666\n","Epoch: 003, Model Loss: 0.6306 Accuracy: 0.5533333333333333\n","Epoch: 004, Model Loss: 0.6336 Accuracy: 0.62\n","Epoch: 005, Model Loss: 0.6163 Accuracy: 0.6266666666666667\n","Epoch: 006, Model Loss: 0.6131 Accuracy: 0.6133333333333333\n","Epoch: 007, Model Loss: 0.5831 Accuracy: 0.64\n","Epoch: 008, Model Loss: 0.5857 Accuracy: 0.66\n","Epoch: 009, Model Loss: 0.5783 Accuracy: 0.6666666666666666\n","Epoch: 010, Model Loss: 0.5438 Accuracy: 0.72\n","Epoch: 011, Model Loss: 0.5477 Accuracy: 0.6933333333333334\n","Epoch: 012, Model Loss: 0.5338 Accuracy: 0.7333333333333333\n","Epoch: 013, Model Loss: 0.5357 Accuracy: 0.76\n","Epoch: 014, Model Loss: 0.5023 Accuracy: 0.7866666666666666\n","Epoch: 015, Model Loss: 0.4898 Accuracy: 0.8266666666666667\n","Epoch: 016, Model Loss: 0.4952 Accuracy: 0.84\n","Epoch: 017, Model Loss: 0.4787 Accuracy: 0.8466666666666667\n","Epoch: 018, Model Loss: 0.5120 Accuracy: 0.7866666666666666\n","Epoch: 019, Model Loss: 0.4835 Accuracy: 0.8066666666666666\n","Epoch: 020, Model Loss: 0.4843 Accuracy: 0.82\n","Epoch: 021, Model Loss: 0.4486 Accuracy: 0.86\n","Epoch: 022, Model Loss: 0.4628 Accuracy: 0.8266666666666667\n","Epoch: 023, Model Loss: 0.4707 Accuracy: 0.8133333333333334\n","Epoch: 024, Model Loss: 0.4710 Accuracy: 0.8\n","Epoch: 025, Model Loss: 0.4678 Accuracy: 0.82\n","Epoch: 026, Model Loss: 0.4709 Accuracy: 0.8133333333333334\n","Epoch: 027, Model Loss: 0.4471 Accuracy: 0.86\n","Epoch: 028, Model Loss: 0.4682 Accuracy: 0.84\n","Epoch: 029, Model Loss: 0.4469 Accuracy: 0.8333333333333334\n","Epoch: 030, Model Loss: 0.4255 Accuracy: 0.84\n","Epoch: 031, Model Loss: 0.4395 Accuracy: 0.86\n","Epoch: 032, Model Loss: 0.4321 Accuracy: 0.8733333333333333\n","Epoch: 033, Model Loss: 0.4178 Accuracy: 0.8666666666666667\n","Epoch: 034, Model Loss: 0.4378 Accuracy: 0.8466666666666667\n","Epoch: 035, Model Loss: 0.4252 Accuracy: 0.8666666666666667\n","Epoch: 036, Model Loss: 0.4690 Accuracy: 0.82\n","Epoch: 037, Model Loss: 0.4260 Accuracy: 0.8666666666666667\n","Epoch: 038, Model Loss: 0.4366 Accuracy: 0.8733333333333333\n","Epoch: 039, Model Loss: 0.4328 Accuracy: 0.8266666666666667\n","Epoch: 040, Model Loss: 0.4529 Accuracy: 0.8066666666666666\n","Epoch: 041, Model Loss: 0.4213 Accuracy: 0.88\n","Epoch: 042, Model Loss: 0.4177 Accuracy: 0.8666666666666667\n","Epoch: 043, Model Loss: 0.4250 Accuracy: 0.8666666666666667\n","Epoch: 044, Model Loss: 0.4578 Accuracy: 0.8266666666666667\n","Epoch: 045, Model Loss: 0.4344 Accuracy: 0.88\n","Epoch: 046, Model Loss: 0.4382 Accuracy: 0.8133333333333334\n","Epoch: 047, Model Loss: 0.4096 Accuracy: 0.8666666666666667\n","Epoch: 048, Model Loss: 0.3884 Accuracy: 0.8933333333333333\n","Epoch: 049, Model Loss: 0.4266 Accuracy: 0.84\n","Epoch: 050, Model Loss: 0.4287 Accuracy: 0.8466666666666667\n","Epoch: 051, Model Loss: 0.4081 Accuracy: 0.88\n","Epoch: 052, Model Loss: 0.4360 Accuracy: 0.8533333333333334\n","Epoch: 053, Model Loss: 0.4131 Accuracy: 0.8733333333333333\n","Epoch: 054, Model Loss: 0.4045 Accuracy: 0.88\n","Epoch: 055, Model Loss: 0.4329 Accuracy: 0.8533333333333334\n","Epoch: 056, Model Loss: 0.4340 Accuracy: 0.8533333333333334\n","Epoch: 057, Model Loss: 0.4209 Accuracy: 0.8533333333333334\n","Epoch: 058, Model Loss: 0.4243 Accuracy: 0.8666666666666667\n","Epoch: 059, Model Loss: 0.4549 Accuracy: 0.7933333333333333\n","Epoch: 060, Model Loss: 0.4025 Accuracy: 0.9066666666666666\n","Epoch: 061, Model Loss: 0.4368 Accuracy: 0.8133333333333334\n","Epoch: 062, Model Loss: 0.4017 Accuracy: 0.8866666666666667\n","Epoch: 063, Model Loss: 0.4031 Accuracy: 0.8733333333333333\n","Epoch: 064, Model Loss: 0.4508 Accuracy: 0.8533333333333334\n","Epoch: 065, Model Loss: 0.4213 Accuracy: 0.88\n","Epoch: 066, Model Loss: 0.4205 Accuracy: 0.92\n","Epoch: 067, Model Loss: 0.4177 Accuracy: 0.8933333333333333\n","Epoch: 068, Model Loss: 0.4263 Accuracy: 0.9\n","Epoch: 069, Model Loss: 0.4097 Accuracy: 0.9466666666666667\n","Epoch: 070, Model Loss: 0.3984 Accuracy: 0.9133333333333333\n","Epoch: 071, Model Loss: 0.4096 Accuracy: 0.9133333333333333\n","Epoch: 072, Model Loss: 0.4451 Accuracy: 0.9066666666666666\n","Epoch: 073, Model Loss: 0.4165 Accuracy: 0.9066666666666666\n","Epoch: 074, Model Loss: 0.4112 Accuracy: 0.8933333333333333\n","Epoch: 075, Model Loss: 0.3917 Accuracy: 0.94\n","Epoch: 076, Model Loss: 0.4083 Accuracy: 0.9266666666666666\n","Epoch: 077, Model Loss: 0.4263 Accuracy: 0.88\n","Epoch: 078, Model Loss: 0.4055 Accuracy: 0.9133333333333333\n","Epoch: 079, Model Loss: 0.3977 Accuracy: 0.94\n","Epoch: 080, Model Loss: 0.3780 Accuracy: 0.96\n","Epoch: 081, Model Loss: 0.4376 Accuracy: 0.8866666666666667\n","Epoch: 082, Model Loss: 0.3781 Accuracy: 0.9266666666666666\n","Epoch: 083, Model Loss: 0.4062 Accuracy: 0.9333333333333333\n","Epoch: 084, Model Loss: 0.3857 Accuracy: 0.9533333333333334\n","Epoch: 085, Model Loss: 0.4107 Accuracy: 0.92\n","Epoch: 086, Model Loss: 0.4172 Accuracy: 0.9066666666666666\n","Epoch: 087, Model Loss: 0.4134 Accuracy: 0.9266666666666666\n","Epoch: 088, Model Loss: 0.4016 Accuracy: 0.9333333333333333\n","Epoch: 089, Model Loss: 0.4020 Accuracy: 0.9266666666666666\n","Epoch: 090, Model Loss: 0.3760 Accuracy: 0.96\n","Epoch: 091, Model Loss: 0.3941 Accuracy: 0.9266666666666666\n","Epoch: 092, Model Loss: 0.4036 Accuracy: 0.9133333333333333\n","Epoch: 093, Model Loss: 0.4246 Accuracy: 0.8933333333333333\n","Epoch: 094, Model Loss: 0.4073 Accuracy: 0.9133333333333333\n","Epoch: 095, Model Loss: 0.4222 Accuracy: 0.9\n","Epoch: 096, Model Loss: 0.3945 Accuracy: 0.9266666666666666\n","Epoch: 097, Model Loss: 0.4027 Accuracy: 0.9333333333333333\n","Epoch: 098, Model Loss: 0.3789 Accuracy: 0.9466666666666667\n","Epoch: 099, Model Loss: 0.3953 Accuracy: 0.9266666666666666\n","Epoch: 100, Model Loss: 0.3929 Accuracy: 0.9133333333333333\n","Epoch: 101, Model Loss: 0.3805 Accuracy: 0.9466666666666667\n","Epoch: 102, Model Loss: 0.3874 Accuracy: 0.9533333333333334\n","Epoch: 103, Model Loss: 0.4223 Accuracy: 0.9133333333333333\n","Epoch: 104, Model Loss: 0.4083 Accuracy: 0.94\n","Epoch: 105, Model Loss: 0.3953 Accuracy: 0.94\n","Epoch: 106, Model Loss: 0.3925 Accuracy: 0.9533333333333334\n","Epoch: 107, Model Loss: 0.3863 Accuracy: 0.9466666666666667\n","Epoch: 108, Model Loss: 0.3897 Accuracy: 0.94\n","Epoch: 109, Model Loss: 0.3772 Accuracy: 0.9666666666666667\n","Epoch: 110, Model Loss: 0.4054 Accuracy: 0.9266666666666666\n","Epoch: 111, Model Loss: 0.4078 Accuracy: 0.9\n","Epoch: 112, Model Loss: 0.3856 Accuracy: 0.9333333333333333\n","Epoch: 113, Model Loss: 0.3690 Accuracy: 0.9733333333333334\n","Epoch: 114, Model Loss: 0.4075 Accuracy: 0.9333333333333333\n","Epoch: 115, Model Loss: 0.3810 Accuracy: 0.9466666666666667\n","Epoch: 116, Model Loss: 0.3716 Accuracy: 0.9533333333333334\n","Epoch: 117, Model Loss: 0.3709 Accuracy: 0.9533333333333334\n","Epoch: 118, Model Loss: 0.3836 Accuracy: 0.9466666666666667\n","Epoch: 119, Model Loss: 0.4083 Accuracy: 0.92\n","Epoch: 120, Model Loss: 0.3864 Accuracy: 0.9333333333333333\n","Epoch: 121, Model Loss: 0.4103 Accuracy: 0.9333333333333333\n","Epoch: 122, Model Loss: 0.4032 Accuracy: 0.9333333333333333\n","Epoch: 123, Model Loss: 0.3769 Accuracy: 0.9466666666666667\n","Epoch: 124, Model Loss: 0.3885 Accuracy: 0.9466666666666667\n","Epoch: 125, Model Loss: 0.4034 Accuracy: 0.9133333333333333\n","Epoch: 126, Model Loss: 0.3880 Accuracy: 0.92\n","Epoch: 127, Model Loss: 0.4121 Accuracy: 0.92\n","Epoch: 128, Model Loss: 0.4166 Accuracy: 0.8933333333333333\n","Epoch: 129, Model Loss: 0.4000 Accuracy: 0.9266666666666666\n","Epoch: 130, Model Loss: 0.3829 Accuracy: 0.9466666666666667\n","Epoch: 131, Model Loss: 0.3991 Accuracy: 0.92\n","Epoch: 132, Model Loss: 0.4046 Accuracy: 0.9133333333333333\n","Epoch: 133, Model Loss: 0.3920 Accuracy: 0.9333333333333333\n","Epoch: 134, Model Loss: 0.4045 Accuracy: 0.9066666666666666\n","Epoch: 135, Model Loss: 0.4124 Accuracy: 0.9333333333333333\n","Epoch: 136, Model Loss: 0.3904 Accuracy: 0.92\n","Epoch: 137, Model Loss: 0.3987 Accuracy: 0.9333333333333333\n","Epoch: 138, Model Loss: 0.4118 Accuracy: 0.92\n","Epoch: 139, Model Loss: 0.3916 Accuracy: 0.94\n","Epoch: 140, Model Loss: 0.4008 Accuracy: 0.9266666666666666\n","Epoch: 141, Model Loss: 0.3924 Accuracy: 0.94\n","Epoch: 142, Model Loss: 0.4339 Accuracy: 0.88\n","Epoch: 143, Model Loss: 0.3857 Accuracy: 0.94\n","Epoch: 144, Model Loss: 0.4134 Accuracy: 0.8866666666666667\n","Epoch: 145, Model Loss: 0.4336 Accuracy: 0.8866666666666667\n","Epoch: 146, Model Loss: 0.4014 Accuracy: 0.9333333333333333\n","Epoch: 147, Model Loss: 0.4252 Accuracy: 0.9\n","Epoch: 148, Model Loss: 0.4595 Accuracy: 0.8733333333333333\n","Epoch: 149, Model Loss: 0.4216 Accuracy: 0.8866666666666667\n","Epoch: 150, Model Loss: 0.4150 Accuracy: 0.8933333333333333\n","Epoch: 151, Model Loss: 0.3909 Accuracy: 0.9266666666666666\n","Epoch: 152, Model Loss: 0.4368 Accuracy: 0.8933333333333333\n","Epoch: 153, Model Loss: 0.4122 Accuracy: 0.9\n","Epoch: 154, Model Loss: 0.4258 Accuracy: 0.9133333333333333\n","Epoch: 155, Model Loss: 0.4096 Accuracy: 0.92\n","Epoch: 156, Model Loss: 0.4114 Accuracy: 0.92\n","Epoch: 157, Model Loss: 0.4325 Accuracy: 0.8933333333333333\n","Epoch: 158, Model Loss: 0.4055 Accuracy: 0.92\n","Epoch: 159, Model Loss: 0.4067 Accuracy: 0.9066666666666666\n","Epoch: 160, Model Loss: 0.3880 Accuracy: 0.9266666666666666\n","Epoch: 161, Model Loss: 0.3854 Accuracy: 0.9266666666666666\n","Epoch: 162, Model Loss: 0.3862 Accuracy: 0.94\n","Epoch: 163, Model Loss: 0.3999 Accuracy: 0.92\n","Epoch: 164, Model Loss: 0.3770 Accuracy: 0.94\n","Epoch: 165, Model Loss: 0.4014 Accuracy: 0.9133333333333333\n","Epoch: 166, Model Loss: 0.4072 Accuracy: 0.9\n","Epoch: 167, Model Loss: 0.3989 Accuracy: 0.9266666666666666\n","Epoch: 168, Model Loss: 0.3938 Accuracy: 0.94\n","Epoch: 169, Model Loss: 0.3872 Accuracy: 0.9133333333333333\n","Epoch: 170, Model Loss: 0.3847 Accuracy: 0.94\n","Epoch: 171, Model Loss: 0.3998 Accuracy: 0.9\n","Epoch: 172, Model Loss: 0.4129 Accuracy: 0.9066666666666666\n","Epoch: 173, Model Loss: 0.4150 Accuracy: 0.9133333333333333\n","Epoch: 174, Model Loss: 0.3922 Accuracy: 0.92\n","Epoch: 175, Model Loss: 0.3956 Accuracy: 0.92\n","Epoch: 176, Model Loss: 0.3793 Accuracy: 0.9333333333333333\n","Epoch: 177, Model Loss: 0.3734 Accuracy: 0.96\n","Epoch: 178, Model Loss: 0.3652 Accuracy: 0.9733333333333334\n","Epoch: 179, Model Loss: 0.3786 Accuracy: 0.94\n","Epoch: 180, Model Loss: 0.4283 Accuracy: 0.8666666666666667\n","Epoch: 181, Model Loss: 0.3768 Accuracy: 0.9333333333333333\n","Epoch: 182, Model Loss: 0.3786 Accuracy: 0.94\n","Epoch: 183, Model Loss: 0.3634 Accuracy: 0.96\n","Epoch: 184, Model Loss: 0.3872 Accuracy: 0.9266666666666666\n","Epoch: 185, Model Loss: 0.3872 Accuracy: 0.9466666666666667\n","Epoch: 186, Model Loss: 0.3854 Accuracy: 0.9333333333333333\n","Epoch: 187, Model Loss: 0.3952 Accuracy: 0.9066666666666666\n","Epoch: 188, Model Loss: 0.3914 Accuracy: 0.9066666666666666\n","Epoch: 189, Model Loss: 0.3846 Accuracy: 0.9333333333333333\n","Epoch: 190, Model Loss: 0.3732 Accuracy: 0.94\n","Epoch: 191, Model Loss: 0.3814 Accuracy: 0.9333333333333333\n","Epoch: 192, Model Loss: 0.3683 Accuracy: 0.9533333333333334\n","Epoch: 193, Model Loss: 0.3591 Accuracy: 0.9533333333333334\n","Epoch: 194, Model Loss: 0.3759 Accuracy: 0.9466666666666667\n","Epoch: 195, Model Loss: 0.3744 Accuracy: 0.9533333333333334\n","Epoch: 196, Model Loss: 0.3668 Accuracy: 0.96\n","Epoch: 197, Model Loss: 0.3694 Accuracy: 0.9533333333333334\n","Epoch: 198, Model Loss: 0.3789 Accuracy: 0.9533333333333334\n","Epoch: 199, Model Loss: 0.3842 Accuracy: 0.94\n","Epoch: 200, Model Loss: 0.3913 Accuracy: 0.9333333333333333\n","Epoch: 201, Model Loss: 0.3794 Accuracy: 0.9466666666666667\n","Epoch: 202, Model Loss: 0.3708 Accuracy: 0.9533333333333334\n","Epoch: 203, Model Loss: 0.3604 Accuracy: 0.96\n","Epoch: 204, Model Loss: 0.3832 Accuracy: 0.94\n","Epoch: 205, Model Loss: 0.3695 Accuracy: 0.9533333333333334\n","Epoch: 206, Model Loss: 0.4103 Accuracy: 0.9133333333333333\n","Epoch: 207, Model Loss: 0.3788 Accuracy: 0.9333333333333333\n","Epoch: 208, Model Loss: 0.3885 Accuracy: 0.92\n","Epoch: 209, Model Loss: 0.3755 Accuracy: 0.9266666666666666\n","Epoch: 210, Model Loss: 0.3934 Accuracy: 0.94\n","Epoch: 211, Model Loss: 0.4037 Accuracy: 0.9266666666666666\n","Epoch: 212, Model Loss: 0.4135 Accuracy: 0.9066666666666666\n","Epoch: 213, Model Loss: 0.3872 Accuracy: 0.94\n","Epoch: 214, Model Loss: 0.3814 Accuracy: 0.9333333333333333\n","Epoch: 215, Model Loss: 0.4002 Accuracy: 0.9133333333333333\n","Epoch: 216, Model Loss: 0.3955 Accuracy: 0.9133333333333333\n","Epoch: 217, Model Loss: 0.3851 Accuracy: 0.9466666666666667\n","Epoch: 218, Model Loss: 0.3664 Accuracy: 0.9466666666666667\n","Epoch: 219, Model Loss: 0.3664 Accuracy: 0.96\n","Epoch: 220, Model Loss: 0.3699 Accuracy: 0.9466666666666667\n","Epoch: 221, Model Loss: 0.4251 Accuracy: 0.88\n","Epoch: 222, Model Loss: 0.4370 Accuracy: 0.88\n","Epoch: 223, Model Loss: 0.4632 Accuracy: 0.84\n","Epoch: 224, Model Loss: 0.4434 Accuracy: 0.8733333333333333\n","Epoch: 225, Model Loss: 0.4403 Accuracy: 0.8866666666666667\n","Epoch: 226, Model Loss: 0.4015 Accuracy: 0.9133333333333333\n","Epoch: 227, Model Loss: 0.4047 Accuracy: 0.92\n","Epoch: 228, Model Loss: 0.4463 Accuracy: 0.88\n","Epoch: 229, Model Loss: 0.4634 Accuracy: 0.84\n","Epoch: 230, Model Loss: 0.4872 Accuracy: 0.8333333333333334\n","Epoch: 231, Model Loss: 0.4991 Accuracy: 0.8\n","Epoch: 232, Model Loss: 0.4524 Accuracy: 0.8533333333333334\n","Epoch: 233, Model Loss: 0.4209 Accuracy: 0.9133333333333333\n","Epoch: 234, Model Loss: 0.4325 Accuracy: 0.8733333333333333\n","Epoch: 235, Model Loss: 0.4206 Accuracy: 0.9066666666666666\n","Epoch: 236, Model Loss: 0.3927 Accuracy: 0.92\n","Epoch: 237, Model Loss: 0.3836 Accuracy: 0.9333333333333333\n","Epoch: 238, Model Loss: 0.4050 Accuracy: 0.9133333333333333\n","Epoch: 239, Model Loss: 0.4114 Accuracy: 0.8933333333333333\n","Epoch: 240, Model Loss: 0.4135 Accuracy: 0.8933333333333333\n","Epoch: 241, Model Loss: 0.3881 Accuracy: 0.92\n","Epoch: 242, Model Loss: 0.3684 Accuracy: 0.94\n","Epoch: 243, Model Loss: 0.3924 Accuracy: 0.92\n","Epoch: 244, Model Loss: 0.4021 Accuracy: 0.92\n","Epoch: 245, Model Loss: 0.3764 Accuracy: 0.94\n","Epoch: 246, Model Loss: 0.3953 Accuracy: 0.92\n","Epoch: 247, Model Loss: 0.3988 Accuracy: 0.9133333333333333\n","Epoch: 248, Model Loss: 0.3684 Accuracy: 0.9533333333333334\n","Epoch: 249, Model Loss: 0.4002 Accuracy: 0.92\n","Epoch: 250, Model Loss: 0.3737 Accuracy: 0.9333333333333333\n","Epoch: 251, Model Loss: 0.3778 Accuracy: 0.94\n","Epoch: 252, Model Loss: 0.4104 Accuracy: 0.9133333333333333\n","Epoch: 253, Model Loss: 0.3887 Accuracy: 0.9133333333333333\n","Epoch: 254, Model Loss: 0.3743 Accuracy: 0.9466666666666667\n","Epoch: 255, Model Loss: 0.3693 Accuracy: 0.96\n","Epoch: 256, Model Loss: 0.3917 Accuracy: 0.92\n","Epoch: 257, Model Loss: 0.3753 Accuracy: 0.9466666666666667\n","Epoch: 258, Model Loss: 0.3762 Accuracy: 0.94\n","Epoch: 259, Model Loss: 0.3774 Accuracy: 0.9333333333333333\n","Epoch: 260, Model Loss: 0.4016 Accuracy: 0.92\n","Epoch: 261, Model Loss: 0.4025 Accuracy: 0.9066666666666666\n","Epoch: 262, Model Loss: 0.4056 Accuracy: 0.9066666666666666\n","Epoch: 263, Model Loss: 0.3865 Accuracy: 0.92\n","Epoch: 264, Model Loss: 0.3776 Accuracy: 0.9333333333333333\n","Epoch: 265, Model Loss: 0.3646 Accuracy: 0.94\n","Epoch: 266, Model Loss: 0.3772 Accuracy: 0.94\n","Epoch: 267, Model Loss: 0.4021 Accuracy: 0.9133333333333333\n","Epoch: 268, Model Loss: 0.3945 Accuracy: 0.9\n","Epoch: 269, Model Loss: 0.3555 Accuracy: 0.96\n","Epoch: 270, Model Loss: 0.3865 Accuracy: 0.92\n","Epoch: 271, Model Loss: 0.3950 Accuracy: 0.9466666666666667\n","Epoch: 272, Model Loss: 0.3799 Accuracy: 0.9333333333333333\n","Epoch: 273, Model Loss: 0.4278 Accuracy: 0.8933333333333333\n","Epoch: 274, Model Loss: 0.3817 Accuracy: 0.9466666666666667\n","Epoch: 275, Model Loss: 0.3775 Accuracy: 0.9333333333333333\n","Epoch: 276, Model Loss: 0.3523 Accuracy: 0.9533333333333334\n","Epoch: 277, Model Loss: 0.3689 Accuracy: 0.9466666666666667\n","Epoch: 278, Model Loss: 0.3948 Accuracy: 0.9266666666666666\n","Epoch: 279, Model Loss: 0.3731 Accuracy: 0.9333333333333333\n","Epoch: 280, Model Loss: 0.3864 Accuracy: 0.9266666666666666\n","Epoch: 281, Model Loss: 0.3768 Accuracy: 0.94\n","Epoch: 282, Model Loss: 0.3813 Accuracy: 0.9333333333333333\n","Epoch: 283, Model Loss: 0.3609 Accuracy: 0.9533333333333334\n","Epoch: 284, Model Loss: 0.3711 Accuracy: 0.94\n","Epoch: 285, Model Loss: 0.3890 Accuracy: 0.94\n","Epoch: 286, Model Loss: 0.3766 Accuracy: 0.9466666666666667\n","Epoch: 287, Model Loss: 0.3565 Accuracy: 0.96\n","Epoch: 288, Model Loss: 0.3702 Accuracy: 0.9533333333333334\n","Epoch: 289, Model Loss: 0.4073 Accuracy: 0.9066666666666666\n","Epoch: 290, Model Loss: 0.3570 Accuracy: 0.9533333333333334\n","Epoch: 291, Model Loss: 0.3515 Accuracy: 0.9666666666666667\n","Epoch: 292, Model Loss: 0.4135 Accuracy: 0.9133333333333333\n","Epoch: 293, Model Loss: 0.3595 Accuracy: 0.9533333333333334\n","Epoch: 294, Model Loss: 0.3637 Accuracy: 0.96\n","Epoch: 295, Model Loss: 0.3699 Accuracy: 0.94\n","Epoch: 296, Model Loss: 0.3515 Accuracy: 0.9666666666666667\n","Epoch: 297, Model Loss: 0.3871 Accuracy: 0.9466666666666667\n","Epoch: 298, Model Loss: 0.3780 Accuracy: 0.9333333333333333\n","Epoch: 299, Model Loss: 0.3793 Accuracy: 0.9333333333333333\n","Epoch: 300, Model Loss: 0.3751 Accuracy: 0.94\n"]}]},{"cell_type":"code","source":["print(gin_model_example.state_dict()['gin_mlp_layers.0.gin_mlp_layers.0.weight'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"u4vZ3JkJMHXb","executionInfo":{"status":"ok","timestamp":1714079792454,"user_tz":-120,"elapsed":10,"user":{"displayName":"Ehsan Mobaraki","userId":"06314958514195310048"}},"outputId":"dc778d1c-5ad1-43b3-dec3-7ee2550250c2"},"execution_count":65,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[-0.2844, -0.3623, -0.1200,  0.1063,  0.2084, -0.1380,  0.0061,  0.2829],\n","        [ 0.1412,  0.1092,  0.0201,  0.3368,  0.1260,  0.3107,  0.0436,  0.3992],\n","        [-0.0406,  0.0394, -0.0252, -0.2404,  0.3206,  0.2871,  0.3359, -0.0339],\n","        [-0.2699, -0.2893, -0.0423,  0.1710,  0.2888,  0.0964,  0.1118, -0.2965],\n","        [ 0.1566,  0.4170,  0.1765, -0.1816, -0.2968,  0.0655,  0.0974, -0.3794],\n","        [-0.0023,  0.0654,  0.3780,  0.1698, -0.0272,  0.2466,  0.2234, -0.1896],\n","        [ 0.2285, -0.2109, -0.2160,  0.2672, -0.1464, -0.2578, -0.0213, -0.0217],\n","        [-0.1469, -0.4470, -0.2574, -0.2453, -0.1932, -0.0271,  0.1182,  0.0147]])\n"]}]},{"cell_type":"code","source":["GNN_Model_test_predicted_labels = []\n","def GNN_Model_test(test_loader):\n","    gin_model_example.eval()\n","    correct = 0\n","    for batch_of_graphs in test_loader:\n","        mlps_output_embeds, mlp_outputs_globalSUMpooled, lin1_output, lin1_output_dropouted, lin2_output, soft = gin_model_example(batch_of_graphs, None)\n","        GNN_Model_test_pred = soft.argmax(dim=1)\n","\n","        GNN_Model_test_predicted_labels.append(GNN_Model_test_pred.tolist()[0])\n","        correct += int((GNN_Model_test_pred == batch_of_graphs.y).sum())\n","    return correct / len(test_loader), GNN_Model_test_predicted_labels"],"metadata":{"id":"Cq-TipP5kWXj","executionInfo":{"status":"ok","timestamp":1714079805716,"user_tz":-120,"elapsed":247,"user":{"displayName":"Ehsan Mobaraki","userId":"06314958514195310048"}}},"execution_count":68,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import roc_auc_score\n","GNN_Model_test_acc, predicted_labels = GNN_Model_test(mutag_test_dataset)\n","print(f'Test Accuracy: {GNN_Model_test_acc:.4f}')\n","print(predicted_labels)\n","real_labels = []\n","for graph in mutag_test_dataset:\n","    real_labels.append(graph.y.tolist()[0])\n","print(real_labels)\n","micro_roc_auc_ovr = roc_auc_score(np.array(real_labels), np.array(predicted_labels), multi_class=\"ovr\", average=\"micro\")\n","micro_roc_auc_ovr = roc_auc_score(np.array(real_labels), np.array(predicted_labels), multi_class=\"ovr\", average=\"micro\")\n","print(\"micro_roc_auc_ovr: \", micro_roc_auc_ovr)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tFpHXvBekYhE","executionInfo":{"status":"ok","timestamp":1714079807044,"user_tz":-120,"elapsed":355,"user":{"displayName":"Ehsan Mobaraki","userId":"06314958514195310048"}},"outputId":"100d3f57-9686-47ac-9dae-363617c98919"},"execution_count":69,"outputs":[{"output_type":"stream","name":"stdout","text":["Test Accuracy: 0.7895\n","[0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1]\n","[1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0]\n","micro_roc_auc_ovr:  0.7564102564102566\n"]}]},{"cell_type":"code","source":["'''\n","from torch_geometric.utils import dropout\n","from torch_geometric.loader import DataLoader\n","import torch\n","import torch.nn as nn\n","from torch.autograd import Variable\n","import math\n","import torch.nn.functional as F\n","from torch.nn.parameter import Parameter\n","from torch_geometric.utils.convert import to_scipy_sparse_matrix\n","from torch_geometric.utils.train_test_split_edges import torch_geometric\n","import networkx as nx\n","import numpy as np\n","from torch_geometric.nn import GCNConv\n","import sys\n","from torch_geometric.datasets import TUDataset\n","from torch_geometric.nn import global_add_pool\n","from scipy.sparse import csr_matrix\n","py_path = '/content/drive/MyDrive/Explainability Methods/Models/Script/Layers/'\n","sys.path.insert(0,py_path)\n","import GIN_MLP_Layers as gin_mlp_layers\n","\n","\n","class GlobalSUMPool(nn.Module):\n","\n","    def __init__(self):\n","        super().__init__()\n","\n","    def forward(self, x, batch):\n","        return torch_geometric.nn.global_add_pool(x, batch)\n","################################################################################\n","class IdenticalPool(nn.Module):\n","\n","    def __init__(self):\n","        super().__init__()\n","\n","    def forward(self, x, batch):\n","        return x\n","\n","################################################################################\n","class GIN_Model(nn.Module):\n","    def __init__(self, num_mlp_layers, Bias, num_slp_layers, mlp_input_dim, mlp_hid_dim, mlp_output_dim, mlp_act_fun, num_classes, dropout_rate, Weight_Initializer):\n","        super(GIN_Model, self).__init__()\n","\n","        self.mlp_input_dim = mlp_input_dim\n","        self.mlp_hid_dim = mlp_hid_dim\n","        self.mlp_output_dim = mlp_output_dim\n","        self.num_slp_layers = num_slp_layers\n","        self.mlp_act_fun = mlp_act_fun\n","        self.lin_act_fun = mlp_act_fun\n","        self.num_classes = num_classes\n","        self.dropout_rate = dropout_rate\n","        self.Weight_Initializer = Weight_Initializer\n","\n","        self.num_mlp_layers = num_mlp_layers\n","        self.Bias = Bias\n","\n","        self.eps = nn.Parameter(torch.zeros(self.num_mlp_layers))\n","        self.gin_mlp_layers = nn.ModuleList()\n","        self.global_summing = GlobalSUMPool()\n","\n","        self.lin1 = nn.Linear(in_features=self.mlp_output_dim * (self.num_mlp_layers + 1), out_features=self.mlp_input_dim * (self.num_mlp_layers + 1))\n","        self.lin2 = nn.Linear(in_features=self.mlp_input_dim * (self.num_mlp_layers + 1), out_features=self.num_classes)\n","        self.dorpout = nn.Dropout(p=dropout_rate)\n","        self.act_fun_softmax = F.softmax\n","\n","        for i in range(self.num_mlp_layers):\n","            self.gin_mlp_layers.append(gin_mlp_layers.GIN_MLPs(num_slp_layers=self.num_slp_layers, mlp_input_dim=self.mlp_input_dim, mlp_hid_dim=self.mlp_hid_dim, mlp_output_dim=self.mlp_output_dim, mlp_act_fun=self.mlp_act_fun, Bias=self.Bias))\n","\n","        if self.lin_act_fun == 'ReLu':\n","            self.lin_act_fun = F.relu\n","            #print('ReLu is Selected.')\n","        elif self.lin_act_fun == 'eLu':\n","            self.lin_act_fun = nn.functional.elu\n","            #print('eLu is Selected.')\n","        elif self.lin_act_fun == 'tanh':\n","            self.lin_act_fun = torch.tanh\n","            #print('tanh is Selected.')\n","\n","        mean = 0\n","        std = 0.1\n","        self.initialize_weights(self.Weight_Initializer, Bias, mean, std)\n","\n","    def initialize_weights(model, Weight_Initializer, Bias, mean, std):\n","        # 1. Xavier Normal_.  2. Kaiming Normal_.  3. Uniform (0,0.1std)\n","        if Weight_Initializer == 1:                                             #.      1. Xavier Normal_.\n","            for i, modules in enumerate(model.children()):\n","                if isinstance(modules, torch.nn.ModuleList):\n","                    for module in modules:\n","                        if isinstance(module, gin_mlp_layers.GIN_MLPs):\n","                            for final_module in module.children():\n","                                if isinstance(final_module, torch.nn.ModuleList):\n","                                    for layers in final_module:\n","                                        if isinstance(layers, torch.nn.Linear):\n","                                            torch.nn.init.xavier_normal_(layers.weight)\n","                                            #layers.weight.data.zero_()\n","                                            #print(layers.weight)\n","                                            if Bias:\n","                                                layers.bias.data.zero_()\n","                                                #print(\"ok\")\n","                                        elif isinstance(layers, torch.nn.BatchNorm1d):\n","                                            #print(\"ok\")\n","                                            pass\n","                elif isinstance(modules, torch.nn.Linear):\n","                    #print(\"predict layer\")\n","                    #modules.weight.data.zero_()\n","                    #print(modules.weight)\n","                    torch.nn.init.xavier_normal_(modules.weight)\n","                elif isinstance(modules, GlobalSUMPool):\n","                    #print(\"GlobalSUMPool\")\n","                    #print(modules)\n","                    pass\n","                elif isinstance(modules, nn.Dropout):\n","                    #print(\"Dropout\")\n","                    #print(modules)\n","                    pass\n","                else:\n","                    pass\n","\n","        if Weight_Initializer == 2:                                             #.      2. Kaiming Normal_.\n","            for i, modules in enumerate(model.children()):\n","                if isinstance(modules, torch.nn.ModuleList):\n","                    for module in modules:\n","                        if isinstance(module, gin_mlp_layers.GIN_MLPs):\n","                            for final_module in module.children():\n","                                if isinstance(final_module, torch.nn.ModuleList):\n","                                    for layers in final_module:\n","                                        if isinstance(layers, torch.nn.Linear):\n","                                            torch.nn.init.kaiming_normal_(layers.weight)\n","                                            #layers.weight.data.zero_()\n","                                            #print(layers.weight)\n","                                            if Bias:\n","                                                layers.bias.data.zero_()\n","                                                #print(\"ok\")\n","                                        elif isinstance(layers, torch.nn.BatchNorm1d):\n","                                            #print(\"ok\")\n","                                            pass\n","                elif isinstance(modules, torch.nn.Linear):\n","                    #print(\"predict layer\")\n","                    #modules.weight.data.zero_()\n","                    #print(modules.weight)\n","                    torch.nn.init.kaiming_normal_(modules.weight)\n","                elif isinstance(modules, GlobalSUMPool):\n","                    #print(\"GlobalSUMPool\")\n","                    #print(modules)\n","                    pass\n","                elif isinstance(modules, nn.Dropout):\n","                    #print(\"Dropout\")\n","                    #print(modules)\n","                    pass\n","                else:\n","                    pass\n","\n","        if Weight_Initializer == 3:                                             #.      3. Uniform (0,0.1std)\n","            for i, modules in enumerate(model.children()):\n","                if isinstance(modules, torch.nn.ModuleList):\n","                    for module in modules:\n","                        if isinstance(module, gin_mlp_layers.GIN_MLPs):\n","                            for final_module in module.children():\n","                                if isinstance(final_module, torch.nn.ModuleList):\n","                                    for layers in final_module:\n","                                        if isinstance(layers, torch.nn.Linear):\n","                                            torch.nn.init.normal_(layers.weight, mean=mean, std=std)\n","                                            #layers.weight.data.zero_()\n","                                            #print(layers.weight)\n","                                            if Bias:\n","                                                layers.bias.data.zero_()\n","                                                #print(\"ok\")\n","                                        elif isinstance(layers, torch.nn.BatchNorm1d):\n","                                            #print(\"ok\")\n","                                            pass\n","                elif isinstance(modules, torch.nn.Linear):\n","                    #print(\"predict layer\")\n","                    #modules.weight.data.zero_()\n","                    #print(modules.weight)\n","                    torch.nn.init.normal_(modules.weight, mean=mean, std=std)\n","                elif isinstance(modules, GlobalSUMPool):\n","                    #print(\"GlobalSUMPool\")\n","                    #print(modules)\n","                    pass\n","                elif isinstance(modules, nn.Dropout):\n","                    #print(\"Dropout\")\n","                    #print(modules)\n","                    pass\n","                else:\n","                    pass\n","\n","\n","    def gin_neighborhood_aggregation(self, h, batched_graphs, edge_mask):\n","\n","        #joint_tilda_adjacency_matrix = torch.tensor(to_scipy_sparse_matrix(batched_graphs.edge_index).todense())# + torch.eye(len(torch.tensor(to_scipy_sparse_matrix(batched_graphs.edge_index).todense())))\n","        if edge_mask == None:\n","            joint_tilda_adjacency_matrix = torch.tensor(to_scipy_sparse_matrix(batched_graphs.edge_index).todense())\n","        else:\n","            joint_tilda_adjacency_matrix = torch.tensor(csr_matrix((np.array(edge_mask), (np.array(batched_graphs.edge_index[0]), np.array(batched_graphs.edge_index[1])))).todense())\n","\n","        joint_tilda_adjacency_matrix = joint_tilda_adjacency_matrix.type(torch.float32)\n","        if batched_graphs.batch == None:\n","            batch_size = 1\n","        else:\n","            batch_size = batched_graphs.num_graphs\n","\n","        pooled = torch.mm(joint_tilda_adjacency_matrix, batched_graphs.x)\n","\n","\n","        return joint_tilda_adjacency_matrix, pooled\n","\n","\n","    def gin_layer_process_eps(self, h, layer, batched_graphs, edge_mask):\n","\n","        joint_tilda_adjacency_matrix, pooled = self.gin_neighborhood_aggregation(h, batched_graphs, edge_mask)\n","\n","        pooled = pooled + (1 + self.eps[layer])*h\n","        pooled_rep = self.gin_mlp_layers[layer](pooled)\n","\n","\n","        return pooled_rep\n","\n","    def merging_process(self, one_mlp, graph_sizes):\n","        new=[]\n","        start=0\n","        for j in range(len(graph_sizes)):\n","            end = start + graph_sizes[j]\n","            new.append(one_mlp[start:end])\n","            start = end\n","        return new\n","\n","    def reshape_mlps_outputs(self, mlps_output_embeds, graph_sizes):\n","        merged_mlps_output_embeds = []\n","        for i in range(len(graph_sizes)):\n","            merged_mlps_output_embeds.append([])\n","\n","        for i in range(len(mlps_output_embeds)):\n","            for j in range(len(mlps_output_embeds[i])):\n","                merged_mlps_output_embeds[j].extend(mlps_output_embeds[i][j])\n","        return merged_mlps_output_embeds\n","\n","\n","\n","\n","\n","    def forward(self, batched_graphs, edge_mask):\n","        #X_concatinated = [graph for graph in batched_graphs.x]\n","        #X_concatinated = torch.stack(X_concatinated, dim=0)\n","\n","        if batched_graphs.batch is not None:\n","            graph_sizes = [len(batched_graphs[i].x) for i in range(len(batched_graphs))]\n","        else:\n","            graph_sizes = [len(batched_graphs.x)]\n","\n","        mlps_output_embeds = []\n","        mlps_output_embeds.append(batched_graphs.x)\n","        hid_rep = batched_graphs.x\n","\n","        for layer in range(self.num_mlp_layers):\n","            hid_rep = self.gin_layer_process_eps(hid_rep, layer, batched_graphs, edge_mask)\n","            mlps_output_embeds.append(hid_rep)\n","        #print(\"the last mlp's: \",h.size())\n","        #h = torch.split(hid_rep, graph_sizes)\n","\n","\n","        mlps_output_embeds_stacked = torch.stack(mlps_output_embeds)\n","\n","        mlp_outputs_globalSUMpooled = self.global_summing(mlps_output_embeds_stacked, batched_graphs.batch)\n","\n","        #merged_mlps_output_embeds_reshaped = self.reshape_mlps_outputs(mlp_outputs_globalSUMpooled, graph_sizes)\n","        if batched_graphs.batch == None:\n","            #merged_mlps_output_embeds_reshaped = self.reshape_mlps_outputs(mlps_output_embeds, graph_sizes, batched_graphs.batch)\n","            merged_mlps_output_embeds_reshaped = self.reshape_mlps_outputs(torch.unsqueeze(mlp_outputs_globalSUMpooled, dim=1), graph_sizes)\n","        else:\n","            merged_mlps_output_embeds_reshaped = self.reshape_mlps_outputs(mlp_outputs_globalSUMpooled, graph_sizes)\n","        lin1_output = self.lin1(torch.tensor(merged_mlps_output_embeds_reshaped))\n","        lin1_output = self.lin_act_fun(lin1_output)\n","\n","        lin1_output_dropouted = self.dorpout(lin1_output)\n","\n","        lin2_output = self.lin2(lin1_output_dropouted)\n","        lin2_output_softmaxed = self.act_fun_softmax(lin2_output, dim=1)\n","\n","        return mlps_output_embeds, mlps_output_embeds_stacked, mlp_outputs_globalSUMpooled, merged_mlps_output_embeds_reshaped, lin1_output, lin1_output_dropouted, lin2_output, lin2_output_softmaxed\n","\n","\n","\n","#dataset = TUDataset(root='data/TUDataset', name='MUTAG')\n","#batch_size = 20\n","#node_feat_size = len(dataset[0].x[0])\n","#batched_dataset = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n","#gin_model_example = GIN_Model(num_mlp_layers=4, Bias=True, num_slp_layers=2, mlp_input_dim=node_feat_size, mlp_hid_dim=7, mlp_output_dim=7, #mlp_act_fun=\"ReLu\", num_classes=2, dropout_rate=0.5, Weight_Initializer=3)\n","#print(gin_model_example)\n","\n","#for batched_graphs in batched_dataset:\n","#    #x, edge_index, batch, y = batched_graphs.x, batched_graphs.edge_index, batched_graphs.batch, batched_graphs.y\n","#    mlps_output_embeds, mlps_output_embeds_stacked, mlp_outputs_globalSUMpooled, merged_mlps_output_embeds_reshaped, lin1_output, #lin1_output_dropouted, lin2_output, lin2_output_softmaxed = gin_model_example(batched_graphs)\n","#    print(\"lin2_output_softmaxed: \", lin2_output_softmaxed.size())\n","#    print(lin2_output_softmaxed)\n","#    break'''"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"o4bV5j79e-Tf","executionInfo":{"status":"ok","timestamp":1694978769768,"user_tz":-120,"elapsed":259,"user":{"displayName":"Ehsan Mobaraki","userId":"06314958514195310048"}},"outputId":"3751ce71-7eb3-47ab-8663-87e80ffcd46a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["GIN_Model(\n","  (gin_mlp_layers): ModuleList(\n","    (0-3): 4 x GIN_MLPs(\n","      (gin_mlp_layers): ModuleList(\n","        (0-1): 2 x Linear(in_features=7, out_features=7, bias=True)\n","      )\n","      (gin_batch_normalization): ModuleList(\n","        (0): BatchNorm1d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","  )\n","  (global_summing): GlobalSUMPool()\n","  (lin1): Linear(in_features=35, out_features=35, bias=True)\n","  (lin2): Linear(in_features=35, out_features=2, bias=True)\n","  (dorpout): Dropout(p=0.5, inplace=False)\n",")\n"]}]},{"cell_type":"code","source":["'''from IPython.core.display import deepcopy\n","class GlobalSUMPool(nn.Module):\n","\n","    def __init__(self):\n","        super().__init__()\n","\n","    def forward(self, x, batch):\n","        return torch_geometric.nn.global_add_pool(x, batch)\n","################################################################################\n","class IdenticalPool(nn.Module):\n","\n","    def __init__(self):\n","        super().__init__()\n","\n","    def forward(self, x, batch):\n","        return x\n","\n","################################################################################\n","class GIN_Model(nn.Module):\n","    def __init__(self, num_mlp_layers, Bias, num_slp_layers, mlp_input_dim, mlp_hid_dim, mlp_output_dim, mlp_act_fun, num_classes,\n","                 dropout_rate, Weight_Initializer):\n","        super(GIN_Model, self).__init__()\n","\n","        self.mlp_input_dim = mlp_input_dim\n","        self.mlp_hid_dim = mlp_hid_dim\n","        self.mlp_output_dim = mlp_output_dim\n","        self.num_slp_layers = num_slp_layers\n","        self.mlp_act_fun = mlp_act_fun\n","        self.lin_act_fun = mlp_act_fun\n","        self.num_classes = num_classes\n","        self.dropout_rate = dropout_rate\n","        self.Weight_Initializer = Weight_Initializer\n","\n","        self.num_mlp_layers = num_mlp_layers\n","        self.Bias = Bias\n","\n","        self.eps = nn.Parameter(torch.zeros(self.num_mlp_layers))\n","        self.gin_mlp_layers = nn.ModuleList()\n","        self.global_summing = GlobalSUMPool()\n","\n","        self.lin1 = nn.Linear(in_features=self.mlp_output_dim * (self.num_mlp_layers + 1), out_features=self.mlp_input_dim * (self.num_mlp_layers + 1))\n","        self.lin2 = nn.Linear(in_features=self.mlp_input_dim * (self.num_mlp_layers + 1), out_features=self.num_classes)\n","        self.dorpout = nn.Dropout(p=dropout_rate)\n","        self.act_fun_softmax = F.softmax\n","\n","        for i in range(self.num_mlp_layers):\n","            self.gin_mlp_layers.append(GIN_MLPs(num_slp_layers=self.num_slp_layers, mlp_input_dim=self.mlp_input_dim,\n","                                                               mlp_hid_dim=self.mlp_hid_dim, mlp_output_dim=self.mlp_output_dim,\n","                                                               mlp_act_fun=self.mlp_act_fun, Bias=self.Bias))\n","\n","        if self.lin_act_fun == 'ReLu':\n","            self.lin_act_fun = F.relu\n","            #print('ReLu is Selected.')\n","        elif self.lin_act_fun == 'eLu':\n","            self.lin_act_fun = nn.functional.elu\n","            #print('eLu is Selected.')\n","        elif self.lin_act_fun == 'tanh':\n","            self.lin_act_fun = torch.tanh\n","            #print('tanh is Selected.')\n","\n","        mean = 0\n","        std = 0.1\n","        self.initialize_weights(self.Weight_Initializer, Bias, mean, std)\n","\n","    def initialize_weights(model, Weight_Initializer, Bias, mean, std):\n","        # 1. Xavier Normal_.  2. Kaiming Normal_.  3. Uniform (0,0.1std)\n","        if Weight_Initializer == 1:                                             #.      1. Xavier Normal_.\n","            for i, modules in enumerate(model.children()):\n","                if isinstance(modules, torch.nn.ModuleList):\n","                    for module in modules:\n","                        if isinstance(module, gin_mlp_layers.GIN_MLPs):\n","                            for final_module in module.children():\n","                                if isinstance(final_module, torch.nn.ModuleList):\n","                                    for layers in final_module:\n","                                        if isinstance(layers, torch.nn.Linear):\n","                                            torch.nn.init.xavier_normal_(layers.weight)\n","                                            #layers.weight.data.zero_()\n","                                            #print(layers.weight)\n","                                            if Bias:\n","                                                layers.bias.data.zero_()\n","                                                #print(\"ok\")\n","                                        elif isinstance(layers, torch.nn.BatchNorm1d):\n","                                            #print(\"ok\")\n","                                            pass\n","                elif isinstance(modules, torch.nn.Linear):\n","                    #print(\"predict layer\")\n","                    #modules.weight.data.zero_()\n","                    #print(modules.weight)\n","                    torch.nn.init.xavier_normal_(modules.weight)\n","                elif isinstance(modules, GlobalSUMPool):\n","                    #print(\"GlobalSUMPool\")\n","                    #print(modules)\n","                    pass\n","                elif isinstance(modules, nn.Dropout):\n","                    #print(\"Dropout\")\n","                    #print(modules)\n","                    pass\n","                else:\n","                    pass\n","\n","        if Weight_Initializer == 2:                                             #.      2. Kaiming Normal_.\n","            for i, modules in enumerate(model.children()):\n","                if isinstance(modules, torch.nn.ModuleList):\n","                    for module in modules:\n","                        if isinstance(module, gin_mlp_layers.GIN_MLPs):\n","                            for final_module in module.children():\n","                                if isinstance(final_module, torch.nn.ModuleList):\n","                                    for layers in final_module:\n","                                        if isinstance(layers, torch.nn.Linear):\n","                                            torch.nn.init.kaiming_normal_(layers.weight)\n","                                            #layers.weight.data.zero_()\n","                                            #print(layers.weight)\n","                                            if Bias:\n","                                                layers.bias.data.zero_()\n","                                                #print(\"ok\")\n","                                        elif isinstance(layers, torch.nn.BatchNorm1d):\n","                                            #print(\"ok\")\n","                                            pass\n","                elif isinstance(modules, torch.nn.Linear):\n","                    #print(\"predict layer\")\n","                    #modules.weight.data.zero_()\n","                    #print(modules.weight)\n","                    torch.nn.init.kaiming_normal_(modules.weight)\n","                elif isinstance(modules, GlobalSUMPool):\n","                    #print(\"GlobalSUMPool\")\n","                    #print(modules)\n","                    pass\n","                elif isinstance(modules, nn.Dropout):\n","                    #print(\"Dropout\")\n","                    #print(modules)\n","                    pass\n","                else:\n","                    pass\n","\n","        if Weight_Initializer == 3:                                             #.      3. Uniform (0,0.1std)\n","            for i, modules in enumerate(model.children()):\n","                if isinstance(modules, torch.nn.ModuleList):\n","                    for module in modules:\n","                        if isinstance(module, gin_mlp_layers.GIN_MLPs):\n","                            for final_module in module.children():\n","                                if isinstance(final_module, torch.nn.ModuleList):\n","                                    for layers in final_module:\n","                                        if isinstance(layers, torch.nn.Linear):\n","                                            torch.nn.init.normal_(layers.weight, mean=mean, std=std)\n","                                            #layers.weight.data.zero_()\n","                                            #print(layers.weight)\n","                                            if Bias:\n","                                                layers.bias.data.zero_()\n","                                                #print(\"ok\")\n","                                        elif isinstance(layers, torch.nn.BatchNorm1d):\n","                                            #print(\"ok\")\n","                                            pass\n","                elif isinstance(modules, torch.nn.Linear):\n","                    #print(\"predict layer\")\n","                    #modules.weight.data.zero_()\n","                    #print(modules.weight)\n","                    torch.nn.init.normal_(modules.weight, mean=mean, std=std)\n","                elif isinstance(modules, GlobalSUMPool):\n","                    #print(\"GlobalSUMPool\")\n","                    #print(modules)\n","                    pass\n","                elif isinstance(modules, nn.Dropout):\n","                    #print(\"Dropout\")\n","                    #print(modules)\n","                    pass\n","                else:\n","                    pass\n","\n","\n","    def gin_neighborhood_aggregation(self, new_adjacecny, new_features):\n","\n","        pooled = torch.bmm(new_adjacecny, new_features)\n","\n","\n","        return pooled\n","\n","\n","    def gin_layer_process_eps(self, hid_rep, layer, new_adjacecny, new_features):\n","\n","        pooled = self.gin_neighborhood_aggregation(new_adjacecny, new_features)\n","\n","\n","        pooled = pooled + (1 + self.eps[layer])*hid_rep\n","        pooled_rep = self.gin_mlp_layers[layer](pooled)\n","\n","\n","        return pooled_rep\n","\n","    def merging_process(self, one_mlp, graph_sizes):\n","        new=[]\n","        start=0\n","        for j in range(len(graph_sizes)):\n","            end = start + graph_sizes[j]\n","            new.append(one_mlp[start:end])\n","            start = end\n","        return new\n","\n","    def reshape_mlps_outputs(self, mlps_output_embeds, graph_sizes):\n","        merged_mlps_output_embeds = []\n","        for i in range(len(graph_sizes)):\n","            merged_mlps_output_embeds.append([])\n","\n","        for i in range(len(mlps_output_embeds)):\n","            for j in range(len(mlps_output_embeds[i])):\n","                merged_mlps_output_embeds[j].extend(mlps_output_embeds[i][j])\n","        return merged_mlps_output_embeds\n","\n","\n","    def computational_matrix(self, batched_graphs, edge_mask):\n","\n","        if edge_mask == None:\n","            joint_tilda_adjacency_matrix = torch.tensor(to_scipy_sparse_matrix(batched_graphs.edge_index).todense())\n","        else:\n","            joint_tilda_adjacency_matrix = torch.tensor(csr_matrix((np.array(edge_mask), (np.array(batched_graphs.edge_index[0]), np.array(batched_graphs.edge_index[1])))).todense())\n","\n","        joint_tilda_adjacency_matrix = joint_tilda_adjacency_matrix.type(torch.float32)\n","\n","\n","        if batched_graphs.batch is not None:\n","            graph_sizes = [len(batched_graphs[i].x) for i in range(len(batched_graphs))]\n","            batch_size = batched_graphs.num_graphs\n","        else:\n","            graph_sizes = [len(batched_graphs.x)]\n","            batch_size = 1\n","        max_number_of_nodes_in_batch_of_graphs = max(graph_sizes)\n","\n","\n","        adjacency_list = []\n","        feature_list = []\n","        for i in range(batch_size):\n","            start = i * graph_sizes[i]\n","            end = (i + 1) * graph_sizes[i]\n","            un_padded_adj = joint_tilda_adjacency_matrix[start:end, start:end]\n","            off_set = max_number_of_nodes_in_batch_of_graphs - un_padded_adj.size()[0]\n","            if un_padded_adj.size()[0] <= max_number_of_nodes_in_batch_of_graphs:\n","                un_padded_adj = F.pad(un_padded_adj, (0, off_set, 0, off_set), mode='constant', value=0)\n","                un_padded_adj = un_padded_adj.type(torch.float32)\n","\n","            un_padded_adj = un_padded_adj.type(torch.float32)\n","            adjacency_list.append(un_padded_adj)\n","\n","            un_padded_feat = batched_graphs.x[start:end, :]\n","            un_padded_feat = F.pad(un_padded_feat, (0, 0, 0, off_set), mode='constant', value=0)\n","            un_padded_feat = un_padded_feat.type(torch.float32)\n","            un_padded_feat.require_grad = True\n","            feature_list.append(un_padded_feat)\n","\n","        adjacency_list = list(map(lambda x: torch.unsqueeze(x, 0), adjacency_list))\n","        feature_list = list(map(lambda x: torch.unsqueeze(x, 0), feature_list))\n","\n","        new_adjacecny = torch.cat(adjacency_list, dim=0)\n","        new_features = torch.cat(feature_list, dim=0)\n","\n","        return new_adjacecny, new_features\n","\n","\n","    def forward(self, batched_graphs, edge_mask):\n","\n","        if batched_graphs.batch is not None:\n","            graph_sizes = [len(batched_graphs[i].x) for i in range(len(batched_graphs))]\n","        else:\n","            graph_sizes = [len(batched_graphs.x)]\n","        batched_graphs2 = deepcopy(batched_graphs)\n","        print(batched_graphs.batch, batched_graphs.batch.size())\n","        new_adjacecny, new_features = self.computational_matrix(batched_graphs, edge_mask)\n","        mlps_output_embeds = []\n","        mlps_output_embeds.append(new_features)\n","\n","\n","\n","        source = new_features\n","        for layer in range(self.num_mlp_layers):\n","            new_features = self.gin_layer_process_eps(new_features, layer, new_adjacecny, new_features)\n","            source = torch.cat((source, new_features),0)\n","            mlps_output_embeds.append(new_features)\n","\n","        batch_list = []\n","        for i in range(source.size()[0]):\n","            for j in range(source.size()[1]):\n","                batch_list.append(i)\n","        batch_np = np.array(batch_list)\n","        batch_torch = torch.from_numpy(batch_np)\n","        source = source.view(source.size()[0]*source.size()[1], source.size()[2])\n","        print(\"here\", source.size())\n","        print(\"new_features: \", new_features.size())\n","\n","\n","        #mlps_output_embeds_stacked = torch.stack(mlps_output_embeds)\n","        #print(\"mlps_output_embeds_stacked: \", mlps_output_embeds_stacked.size())\n","\n","        mlp_outputs_globalSUMpooled = self.global_summing(source, batch_torch)\n","        print(\"mlp_outputs_globalSUMpooled: \", mlp_outputs_globalSUMpooled.size())\n","\n","        if batched_graphs.batch == None:\n","            merged_mlps_output_embeds_reshaped = self.reshape_mlps_outputs(torch.unsqueeze(mlp_outputs_globalSUMpooled, dim=1), graph_sizes)\n","        else:\n","            merged_mlps_output_embeds_reshaped = self.reshape_mlps_outputs(mlp_outputs_globalSUMpooled, graph_sizes)\n","        lin1_output = self.lin1(torch.tensor(merged_mlps_output_embeds_reshaped))\n","        lin1_output = self.lin_act_fun(lin1_output)\n","\n","        lin1_output_dropouted = self.dorpout(lin1_output)\n","\n","        lin2_output = self.lin2(lin1_output_dropouted)\n","        lin2_output_softmaxed = self.act_fun_softmax(lin2_output, dim=1)\n","\n","        return mlps_output_embeds, mlps_output_embeds_stacked, mlp_outputs_globalSUMpooled, merged_mlps_output_embeds_reshaped, lin1_output, lin1_output_dropouted, lin2_output, lin2_output_softmaxed\n","node_feat_size=7\n","gin_model_example = GIN_Model(num_mlp_layers=4, Bias=True, num_slp_layers=2, mlp_input_dim=node_feat_size, mlp_hid_dim=7,\n","                              mlp_output_dim=7, mlp_act_fun=\"ReLu\", num_classes=2, dropout_rate=0.5, Weight_Initializer=3)\n","#print(gin_model_example)\n","\n","'''"],"metadata":{"id":"J18Hj--7ZWDS"},"execution_count":null,"outputs":[]}]}