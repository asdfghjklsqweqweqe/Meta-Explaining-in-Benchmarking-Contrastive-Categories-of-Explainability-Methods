{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMSir13l7sUWbWxRhK9GAkl"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import os\n","import torch\n","os.environ['TORCH'] = torch.__version__\n","print(torch.__version__)\n","\n","!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n","!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n","!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TEaiwHxqUW9g","executionInfo":{"status":"ok","timestamp":1714035989848,"user_tz":-120,"elapsed":68592,"user":{"displayName":"Ehsan Mobaraki","userId":"06314958514195310048"}},"outputId":"32ba1c41-b028-42f1-a998-64ba665408a1"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["2.2.1+cu121\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for torch-geometric (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xFV4KPwApAQF","executionInfo":{"status":"ok","timestamp":1714036010226,"user_tz":-120,"elapsed":20385,"user":{"displayName":"Ehsan Mobaraki","userId":"06314958514195310048"}},"outputId":"933424af-4792-4daf-b3e7-a104518e66df"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["from torch_geometric.utils import dropout\n","from torch_geometric.loader import DataLoader\n","import torch\n","import torch.nn as nn\n","from torch.autograd import Variable\n","import math\n","import torch.nn.functional as F\n","from torch.nn.parameter import Parameter\n","from torch_geometric.utils.convert import to_scipy_sparse_matrix\n","import torch_geometric\n","import networkx as nx\n","import numpy as np\n","from torch_geometric.nn import GCNConv\n","import sys\n","from torch_geometric.datasets import TUDataset\n","from torch_geometric.nn import global_add_pool\n","py_path = '/content/drive/MyDrive/Explainability Methods/Models/Script/Layers/'\n","sys.path.insert(0,py_path)\n","import GIN_MLP_Layers as gin_mlp_layers"],"metadata":{"id":"XuN-w8xQUfay","executionInfo":{"status":"ok","timestamp":1714036018989,"user_tz":-120,"elapsed":8774,"user":{"displayName":"Ehsan Mobaraki","userId":"06314958514195310048"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DCpiwgdJTGNa","executionInfo":{"status":"ok","timestamp":1714036332241,"user_tz":-120,"elapsed":7,"user":{"displayName":"Ehsan Mobaraki","userId":"06314958514195310048"}},"outputId":"68654fc3-4411-40d7-e2f2-9b929512d76c"},"outputs":[{"output_type":"stream","name":"stdout","text":["lin2_output_softmaxed:  torch.Size([2, 2])\n","tensor([[0.6612, 0.3388],\n","        [0.9518, 0.0482]], grad_fn=<SoftmaxBackward0>)\n"]}],"source":["from torch_geometric.utils import dropout\n","from torch_geometric.loader import DataLoader\n","import torch\n","import torch.nn as nn\n","from torch.autograd import Variable\n","import math\n","import torch.nn.functional as F\n","from torch.nn.parameter import Parameter\n","from torch_geometric.utils.convert import to_scipy_sparse_matrix\n","import torch_geometric\n","import networkx as nx\n","import numpy as np\n","from torch_geometric.nn import GCNConv\n","import sys\n","from torch_geometric.datasets import TUDataset\n","from torch_geometric.nn import global_add_pool\n","from scipy.sparse import csr_matrix\n","py_path = '/content/drive/MyDrive/Explainability Methods/Models/Script/Layers/'\n","sys.path.insert(0,py_path)\n","import GIN_MLP_Layers as gin_mlp_layers\n","\n","\n","class GlobalSUMPool(nn.Module):\n","\n","    def __init__(self):\n","        super().__init__()\n","\n","    def forward(self, x, batch):\n","        return torch_geometric.nn.global_add_pool(x, batch)\n","################################################################################\n","class IdenticalPool(nn.Module):\n","\n","    def __init__(self):\n","        super().__init__()\n","\n","    def forward(self, x, batch):\n","        return x\n","\n","################################################################################\n","class GIN_Model(nn.Module):\n","    def __init__(self, num_mlp_layers, Bias, num_slp_layers, mlp_input_dim, mlp_hid_dim, mlp_output_dim, mlp_act_fun, num_classes,\n","                 dropout_rate, Weight_Initializer):\n","        super(GIN_Model, self).__init__()\n","\n","        self.mlp_input_dim = mlp_input_dim\n","        self.mlp_hid_dim = mlp_hid_dim\n","        self.mlp_output_dim = mlp_output_dim\n","        self.num_slp_layers = num_slp_layers\n","        self.mlp_act_fun = mlp_act_fun\n","        self.lin_act_fun = mlp_act_fun\n","        self.num_classes = num_classes\n","        self.dropout_rate = dropout_rate\n","        self.Weight_Initializer = Weight_Initializer\n","\n","        self.num_mlp_layers = num_mlp_layers\n","        self.Bias = Bias\n","\n","        self.eps = nn.Parameter(torch.zeros(self.num_mlp_layers),requires_grad=True)\n","        self.gin_mlp_layers = nn.ModuleList()\n","        self.global_summing = GlobalSUMPool()\n","\n","        self.lin1 = nn.Linear(in_features=self.mlp_output_dim * (self.num_mlp_layers + 1), out_features=self.mlp_input_dim * (self.num_mlp_layers + 1))\n","        self.lin2 = nn.Linear(in_features=self.mlp_input_dim * (self.num_mlp_layers + 1), out_features=self.num_classes)\n","        self.dorpout = nn.Dropout(p=dropout_rate)\n","        self.act_fun_softmax = F.softmax\n","        self.the_first_layer = nn.Linear(in_features=self.mlp_input_dim, out_features=self.mlp_output_dim, bias=self.Bias)\n","\n","        for i in range(self.num_mlp_layers):\n","            self.gin_mlp_layers.append(gin_mlp_layers.GIN_MLPs(num_slp_layers=self.num_slp_layers, mlp_input_dim=self.mlp_output_dim,\n","                                                               mlp_hid_dim=self.mlp_hid_dim, mlp_output_dim=self.mlp_output_dim,\n","                                                               mlp_act_fun=self.mlp_act_fun, Bias=self.Bias))\n","\n","        if self.lin_act_fun == 'ReLu':\n","            self.lin_act_fun = F.relu\n","            #print('ReLu is Selected.')\n","        elif self.lin_act_fun == 'eLu':\n","            self.lin_act_fun = nn.functional.elu\n","            #print('eLu is Selected.')\n","        elif self.lin_act_fun == 'tanh':\n","            self.lin_act_fun = torch.tanh\n","            #print('tanh is Selected.')\n","\n","        mean = 0\n","        std = 0.1\n","        self.initialize_weights(self.Weight_Initializer, Bias, mean, std)\n","\n","    def initialize_weights(model, Weight_Initializer, Bias, mean, std):\n","        # 1. Xavier Normal_.  2. Kaiming Normal_.  3. Uniform (0,0.1std)\n","        if Weight_Initializer == 1:                                             #.      1. Xavier Normal_.\n","            for i, modules in enumerate(model.children()):\n","                if isinstance(modules, torch.nn.ModuleList):\n","                    for module in modules:\n","                        if isinstance(module, gin_mlp_layers.GIN_MLPs):\n","                            for final_module in module.children():\n","                                if isinstance(final_module, torch.nn.ModuleList):\n","                                    for layers in final_module:\n","                                        if isinstance(layers, torch.nn.Linear):\n","                                            torch.nn.init.xavier_normal_(layers.weight)\n","                                            #layers.weight.data.zero_()\n","                                            #print(layers.weight)\n","                                            if Bias:\n","                                                layers.bias.data.zero_()\n","                                                #print(\"ok\")\n","                                        elif isinstance(layers, torch.nn.BatchNorm1d):\n","                                            #print(\"ok\")\n","                                            pass\n","                elif isinstance(modules, torch.nn.Linear):\n","                    #print(\"predict layer\")\n","                    #modules.weight.data.zero_()\n","                    #print(modules.weight)\n","                    torch.nn.init.xavier_normal_(modules.weight)\n","                elif isinstance(modules, GlobalSUMPool):\n","                    #print(\"GlobalSUMPool\")\n","                    #print(modules)\n","                    pass\n","                elif isinstance(modules, nn.Dropout):\n","                    #print(\"Dropout\")\n","                    #print(modules)\n","                    pass\n","                else:\n","                    pass\n","\n","        if Weight_Initializer == 2:                                             #.      2. Kaiming Normal_.\n","            for i, modules in enumerate(model.children()):\n","                if isinstance(modules, torch.nn.ModuleList):\n","                    for module in modules:\n","                        if isinstance(module, gin_mlp_layers.GIN_MLPs):\n","                            for final_module in module.children():\n","                                if isinstance(final_module, torch.nn.ModuleList):\n","                                    for layers in final_module:\n","                                        if isinstance(layers, torch.nn.Linear):\n","                                            torch.nn.init.kaiming_normal_(layers.weight)\n","                                            #layers.weight.data.zero_()\n","                                            #print(layers.weight)\n","                                            if Bias:\n","                                                layers.bias.data.zero_()\n","                                                #print(\"ok\")\n","                                        elif isinstance(layers, torch.nn.BatchNorm1d):\n","                                            #print(\"ok\")\n","                                            pass\n","                elif isinstance(modules, torch.nn.Linear):\n","                    #print(\"predict layer\")\n","                    #modules.weight.data.zero_()\n","                    #print(modules.weight)\n","                    torch.nn.init.kaiming_normal_(modules.weight)\n","                elif isinstance(modules, GlobalSUMPool):\n","                    #print(\"GlobalSUMPool\")\n","                    #print(modules)\n","                    pass\n","                elif isinstance(modules, nn.Dropout):\n","                    #print(\"Dropout\")\n","                    #print(modules)\n","                    pass\n","                else:\n","                    pass\n","\n","        if Weight_Initializer == 3:                                             #.      3. Uniform (0,0.1std)\n","            for i, modules in enumerate(model.children()):\n","                if isinstance(modules, torch.nn.ModuleList):\n","                    for module in modules:\n","                        if isinstance(module, gin_mlp_layers.GIN_MLPs):\n","                            for final_module in module.children():\n","                                if isinstance(final_module, torch.nn.ModuleList):\n","                                    for layers in final_module:\n","                                        if isinstance(layers, torch.nn.Linear):\n","                                            torch.nn.init.normal_(layers.weight, mean=mean, std=std)\n","                                            #layers.weight.data.zero_()\n","                                            #print(layers.weight)\n","                                            if Bias:\n","                                                layers.bias.data.zero_()\n","                                                #print(\"ok\")\n","                                        elif isinstance(layers, torch.nn.BatchNorm1d):\n","                                            #print(\"ok\")\n","                                            pass\n","                elif isinstance(modules, torch.nn.Linear):\n","                    #print(\"predict layer\")\n","                    #modules.weight.data.zero_()\n","                    #print(modules.weight)\n","                    torch.nn.init.normal_(modules.weight, mean=mean, std=std)\n","                elif isinstance(modules, GlobalSUMPool):\n","                    #print(\"GlobalSUMPool\")\n","                    #print(modules)\n","                    pass\n","                elif isinstance(modules, nn.Dropout):\n","                    #print(\"Dropout\")\n","                    #print(modules)\n","                    pass\n","                else:\n","                    pass\n","\n","\n","    def gin_neighborhood_aggregation(self, new_adjacecny, new_features):\n","        pooled = torch.mm(new_adjacecny, new_features)\n","        return pooled\n","\n","\n","    def gin_layer_process_eps(self, hid_rep, layer, new_adjacecny):\n","\n","        pooled = self.gin_neighborhood_aggregation(new_adjacecny, hid_rep)\n","        pooled = pooled + (1 + self.eps[layer])*hid_rep\n","        pooled_rep = self.gin_mlp_layers[layer](pooled)\n","\n","        return pooled_rep\n","\n","    def merging_process(self, one_mlp, graph_sizes):\n","        new=[]\n","        start=0\n","        for j in range(len(graph_sizes)):\n","            end = start + graph_sizes[j]\n","            new.append(one_mlp[start:end])\n","            start = end\n","        return new\n","\n","    def reshape_mlps_outputs(self, mlps_output_embeds, graph_sizes):\n","        merged_mlps_output_embeds = []\n","        for i in range(len(graph_sizes)):\n","            merged_mlps_output_embeds.append([])\n","\n","        for i in range(len(mlps_output_embeds)):\n","            for j in range(len(mlps_output_embeds[i])):\n","                merged_mlps_output_embeds[j].extend(mlps_output_embeds[i][j])\n","        return merged_mlps_output_embeds\n","\n","    def computational_matrix(self, batched_graphs, edge_mask):\n","\n","        if edge_mask == None:\n","            joint_tilda_adjacency_matrix = torch.tensor(to_scipy_sparse_matrix(batched_graphs.edge_index).todense())\n","        else:\n","            joint_tilda_adjacency_matrix = torch.tensor(csr_matrix((np.array(edge_mask), (np.array(batched_graphs.edge_index[0]), np.array(batched_graphs.edge_index[1])))).todense())\n","        joint_tilda_adjacency_matrix = joint_tilda_adjacency_matrix.type(torch.float32)\n","\n","        return joint_tilda_adjacency_matrix\n","\n","    def reshape_batchs_by_graph_sizes(self, hid_rep, batch_size, graph_sizes):\n","        batch_mlps_output_embeds_shape_saved = []\n","        start = 0\n","        for i in range(batch_size):\n","            end = start + graph_sizes[i]\n","            un_padded_feat = hid_rep[start:end, start:end]#.unsqueeze(0)\n","            batch_mlps_output_embeds_shape_saved.append(un_padded_feat)\n","            start = end\n","        return batch_mlps_output_embeds_shape_saved\n","\n","    def forward(self, batched_graphs, edge_mask):\n","\n","        if batched_graphs.batch is not None:\n","            graph_sizes = [len(batched_graphs[i].x) for i in range(len(batched_graphs))]\n","            batch_size = batched_graphs.num_graphs\n","        else:\n","            graph_sizes = [len(batched_graphs.x)]\n","            batch_size = 1\n","\n","        mlps_output_embeds = []\n","        mlps_output_embeds_shape_saved = []\n","\n","        new_adjacecny = self.computational_matrix(batched_graphs, edge_mask)\n","        hid_rep = self.the_first_layer(batched_graphs.x)\n","        mlps_output_embeds.append(hid_rep)\n","\n","        for layer in range(self.num_mlp_layers):\n","            hid_rep = self.gin_layer_process_eps(hid_rep, layer, new_adjacecny)\n","            mlps_output_embeds.append(hid_rep)\n","            mlps_output_embeds_shape_saved.append(self.reshape_batchs_by_graph_sizes(hid_rep, batch_size, graph_sizes))\n","\n","        mlps_output_embeds_stacked = torch.stack(mlps_output_embeds)\n","        mlp_outputs_globalSUMpooled = self.global_summing(mlps_output_embeds_stacked, batched_graphs.batch)\n","\n","        if batched_graphs.batch == None:\n","            merged_mlps_output_embeds_reshaped = torch.unsqueeze(mlp_outputs_globalSUMpooled, dim=1).view(1, -1)\n","        else:\n","            merged_mlps_output_embeds_reshaped = mlp_outputs_globalSUMpooled.view(mlp_outputs_globalSUMpooled.size()[1], -1)\n","\n","        lin1_output = self.lin1(merged_mlps_output_embeds_reshaped)\n","\n","        lin1_output = self.lin_act_fun(lin1_output)\n","\n","        lin1_output_dropouted = self.dorpout(lin1_output)\n","\n","        lin2_output = self.lin2(lin1_output_dropouted)\n","        lin2_output_softmaxed = self.act_fun_softmax(lin2_output, dim=1)\n","\n","        return mlps_output_embeds_shape_saved, mlps_output_embeds_stacked, mlp_outputs_globalSUMpooled, merged_mlps_output_embeds_reshaped, lin1_output, lin1_output_dropouted, lin2_output, lin2_output_softmaxed\n","\n","\n","dataset = TUDataset(root='data/TUDataset', name='MUTAG')\n","batch_size = 2\n","node_feat_size = len(dataset[0].x[0])\n","batched_dataset = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n","gin_model_example = GIN_Model(num_mlp_layers=4, Bias=True, num_slp_layers=2, mlp_input_dim=node_feat_size, mlp_hid_dim=10,\n","                              mlp_output_dim=17, mlp_act_fun=\"ReLu\", num_classes=2, dropout_rate=0.5, Weight_Initializer=3)\n","\n","\n","for batched_graphs in batched_dataset:\n","    #x, edge_index, batch, y = batched_graphs.x, batched_graphs.edge_index, batched_graphs.batch, batched_graphs.y\n","    mlps_output_embeds, mlps_output_embeds_stacked, mlp_outputs_globalSUMpooled, merged_mlps_output_embeds_reshaped, lin1_output, lin1_output_dropouted, lin2_output, lin2_output_softmaxed = gin_model_example(batched_graphs, None)\n","    print(\"lin2_output_softmaxed: \", lin2_output_softmaxed.size())\n","    print(lin2_output_softmaxed)\n","    break"]},{"cell_type":"code","source":["sum=0\n","for i in range(batch_size):\n","    sum = sum + len(dataset[i].x)\n","print(sum)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4v_11xrecrwh","executionInfo":{"status":"ok","timestamp":1678987488352,"user_tz":-60,"elapsed":6,"user":{"displayName":"Ehsan Mobaraki","userId":"06314958514195310048"}},"outputId":"8384fadd-304a-4b6e-8305-ffa7fd7fff10"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["338\n"]}]}]}