{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPE397WzGU5P2lzUyRfMJDT"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"r9pm1jIuc87M"},"outputs":[],"source":["\"\"\"\n","FileName: models.py\n","Description: GNN models' set\n","Time: 2020/7/30 9:01\n","Project: GNN_benchmark\n","Author: Shurui Gui\n","\"\"\"\n","\n","import torch\n","import torch.nn as nn\n","import torch_geometric.nn as gnn\n","from torch_geometric.utils.loop import add_self_loops, remove_self_loops\n","from torch_geometric.data.batch import Batch\n","\n","from typing import Callable, Union, Tuple\n","from torch_geometric.typing import OptPairTensor, Adj, OptTensor, Size\n","from torch import Tensor\n","\n","from torch_sparse import SparseTensor\n","\n","\n","class GNNBasic(torch.nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","\n","    def arguments_read(self, *args, **kwargs):\n","\n","        data: Batch = kwargs.get('data') or None\n","\n","        if not data:\n","            if not args:\n","                assert 'x' in kwargs\n","                assert 'edge_index' in kwargs\n","                x, edge_index = kwargs['x'], kwargs['edge_index'],\n","                batch = kwargs.get('batch')\n","                if batch is None:\n","                    batch = torch.zeros(kwargs['x'].shape[0], dtype=torch.int64, device=x.device)\n","            elif len(args) == 2:\n","                x, edge_index = args[0], args[1]\n","                batch = torch.zeros(args[0].shape[0], dtype=torch.int64, device=x.device)\n","            elif len(args) == 3:\n","                x, edge_index, batch = args[0], args[1], args[2]\n","            else:\n","                raise ValueError(f\"forward's args should take 2 or 3 arguments but got {len(args)}\")\n","        else:\n","            x, edge_index, batch = data.x, data.edge_index, data.batch\n","\n","        return x, edge_index, batch\n","\n","\n","class GCN_3l(GNNBasic):\n","\n","    def __init__(self, model_level, dim_node, dim_hidden, num_classes):\n","        super().__init__()\n","        num_layer = 3\n","\n","        self.conv1 = GCNConv(dim_node, dim_hidden)\n","        self.convs = nn.ModuleList(\n","            [\n","                GCNConv(dim_hidden, dim_hidden)\n","                for _ in range(num_layer - 1)\n","             ]\n","        )\n","        self.relu1 = nn.ReLU()\n","        self.relus = nn.ModuleList(\n","            [\n","                nn.ReLU()\n","                for _ in range(num_layer - 1)\n","            ]\n","        )\n","        if model_level == 'node':\n","            self.readout = IdenticalPool()\n","        else:\n","            self.readout = GlobalMeanPool()\n","\n","        self.ffn = nn.Sequential(*(\n","                [nn.Linear(dim_hidden, dim_hidden)] +\n","                [nn.ReLU(), nn.Dropout(), nn.Linear(dim_hidden, num_classes)]\n","        ))\n","\n","        self.dropout = nn.Dropout()\n","\n","    def forward(self, *args, **kwargs) -> torch.Tensor:\n","        \"\"\"\n","        :param Required[data]: Batch - input data\n","        :return:\n","        \"\"\"\n","        x, edge_index, batch = self.arguments_read(*args, **kwargs)\n","\n","        post_conv = self.relu1(self.conv1(x, edge_index))\n","        for conv, relu in zip(self.convs, self.relus):\n","            post_conv = relu(conv(post_conv, edge_index))\n","\n","        out_readout = self.readout(post_conv, batch)\n","\n","        out = self.ffn(out_readout)\n","        return out\n","\n","    def get_emb(self, *args, **kwargs) -> torch.Tensor:\n","        x, edge_index, batch = self.arguments_read(*args, **kwargs)\n","        post_conv = self.relu1(self.conv1(x, edge_index))\n","        for conv, relu in zip(self.convs, self.relus):\n","            post_conv = relu(conv(post_conv, edge_index))\n","        return post_conv\n","\n","class GCN_3l_BN(GCN_3l):\n","    def __init__(self, model_level, dim_node, dim_hidden, num_classes):\n","        super().__init__(model_level, dim_node, dim_hidden, num_classes)\n","        num_layer = 3\n","\n","        self.relu1 = nn.Sequential(\n","            nn.BatchNorm1d(dim_hidden),\n","            nn.ReLU()\n","        )\n","\n","        self.relus = nn.ModuleList(\n","            [\n","                nn.Sequential(\n","                    nn.BatchNorm1d(dim_hidden),\n","                    nn.ReLU(),\n","                )\n","                for _ in range(num_layer - 1)\n","            ]\n","        )\n","\n","class GCN_2l(GNNBasic):\n","\n","    def __init__(self, model_level, dim_node, dim_hidden, num_classes):\n","        super().__init__()\n","        num_layer = 2\n","\n","        self.conv1 = GCNConv(dim_node, dim_hidden)\n","        self.convs = nn.ModuleList(\n","            [\n","                GCNConv(dim_hidden, dim_hidden)\n","                for _ in range(num_layer - 1)\n","            ]\n","        )\n","        self.relu1 = nn.ReLU()\n","        self.relus = nn.ModuleList(\n","            [\n","                nn.ReLU()\n","                for _ in range(num_layer - 1)\n","            ]\n","        )\n","        if model_level == 'node':\n","            self.readout = IdenticalPool()\n","        else:\n","            self.readout = GlobalMeanPool()\n","\n","        self.ffn = nn.Sequential(*(\n","                [nn.Linear(dim_hidden, num_classes)]\n","        ))\n","\n","        self.dropout = nn.Dropout()\n","\n","    def forward(self, *args, **kwargs) -> torch.Tensor:\n","        \"\"\"\n","        :param Required[data]: Batch - input data\n","        :return:\n","        \"\"\"\n","        x, edge_index, batch = self.arguments_read(*args, **kwargs)\n","\n","        post_conv = self.relu1(self.conv1(x, edge_index))\n","        for conv, relu in zip(self.convs, self.relus):\n","            post_conv = relu(conv(post_conv, edge_index))\n","\n","        out_readout = self.readout(post_conv, batch)\n","\n","        out = self.ffn(out_readout)\n","\n","        return out\n","\n","    def get_emb(self, *args, **kwargs) -> torch.Tensor:\n","        x, edge_index, batch = self.arguments_read(*args, **kwargs)\n","        \n","        post_conv = self.relu1(self.conv1(x, edge_index))\n","        for conv, relu in zip(self.convs, self.relus):\n","            post_conv = relu(conv(post_conv, edge_index))\n","            \n","        return post_conv\n","\n","\n","class GIN_3l(GNNBasic):\n","\n","    def __init__(self, model_level, dim_node, dim_hidden, num_classes):\n","        super().__init__()\n","        num_layer = 3\n","\n","        self.conv1 = GINConv(nn.Sequential(nn.Linear(dim_node, dim_hidden), nn.ReLU(),\n","                                           nn.Linear(dim_hidden, dim_hidden), nn.ReLU()))#,\n","                                           # nn.BatchNorm1d(dim_hidden)))\n","        self.convs = nn.ModuleList(\n","            [\n","                GINConv(nn.Sequential(nn.Linear(dim_hidden, dim_hidden), nn.ReLU(),\n","                                      nn.Linear(dim_hidden, dim_hidden), nn.ReLU()))#,\n","                                      # nn.BatchNorm1d(dim_hidden)))\n","                for _ in range(num_layer - 1)\n","             ]\n","        )\n","        self.relu1 = nn.ReLU()\n","        self.relus = nn.ModuleList(\n","            [\n","                nn.ReLU()\n","                for _ in range(num_layer - 1)\n","            ]\n","        )\n","        if model_level == 'node':\n","            self.readout = IdenticalPool()\n","        else:\n","            self.readout = GlobalMeanPool()\n","\n","        self.ffn = nn.Sequential(*(\n","                [nn.Linear(dim_hidden, dim_hidden)] +\n","                [nn.ReLU(), nn.Dropout(), nn.Linear(dim_hidden, num_classes)]\n","        ))\n","\n","        self.dropout = nn.Dropout()\n","\n","    def forward(self, *args, **kwargs) -> torch.Tensor:\n","        \"\"\"\n","        :param Required[data]: Batch - input data\n","        :return:\n","        \"\"\"\n","        x, edge_index, batch = self.arguments_read(*args, **kwargs)\n","\n","\n","        post_conv = self.conv1(x, edge_index)\n","        for conv in self.convs:\n","            post_conv = conv(post_conv, edge_index)\n","\n","\n","        out_readout = self.readout(post_conv, batch)\n","\n","        out = self.ffn(out_readout)\n","        return out\n","\n","    def get_emb(self, *args, **kwargs) -> torch.Tensor:\n","        x, edge_index, batch = self.arguments_read(*args, **kwargs)\n","        post_conv = self.conv1(x, edge_index)\n","        for conv in self.convs:\n","            post_conv = conv(post_conv, edge_index)\n","        return post_conv\n","\n","\n","class GIN_2l(GNNBasic):\n","\n","    def __init__(self, model_level, dim_node, dim_hidden, num_classes):\n","        super().__init__()\n","        num_layer = 2\n","\n","        self.conv1 = GINConv(nn.Sequential(nn.Linear(dim_node, dim_hidden), nn.ReLU(),\n","                                           nn.Linear(dim_hidden, dim_hidden), nn.ReLU()))#,\n","                                           # nn.BatchNorm1d(dim_hidden)))\n","        self.convs = nn.ModuleList(\n","            [\n","                GINConv(nn.Sequential(nn.Linear(dim_hidden, dim_hidden), nn.ReLU(),\n","                                      nn.Linear(dim_hidden, dim_hidden), nn.ReLU()))#,\n","                                      # nn.BatchNorm1d(dim_hidden)))\n","                for _ in range(num_layer - 1)\n","             ]\n","        )\n","        self.relu1 = nn.ReLU()\n","        self.relus = nn.ModuleList(\n","            [\n","                nn.ReLU()\n","                for _ in range(num_layer - 1)\n","            ]\n","        )\n","        if model_level == 'node':\n","            self.readout = IdenticalPool()\n","        else:\n","            self.readout = GlobalMeanPool()\n","\n","        self.ffn = nn.Sequential(*(\n","                [nn.Linear(dim_hidden, dim_hidden)] +\n","                [nn.ReLU(), nn.Dropout(), nn.Linear(dim_hidden, num_classes)]\n","        ))\n","\n","        self.dropout = nn.Dropout()\n","\n","    def forward(self, *args, **kwargs) -> torch.Tensor:\n","        \"\"\"\n","        :param Required[data]: Batch - input data\n","        :return:\n","        \"\"\"\n","        x, edge_index, batch = self.arguments_read(*args, **kwargs)\n","\n","\n","        post_conv = self.conv1(x, edge_index)\n","        for conv in self.convs:\n","            post_conv = conv(post_conv, edge_index)\n","\n","\n","        out_readout = self.readout(post_conv, batch)\n","\n","        out = self.ffn(out_readout)\n","        return out\n","\n","    def get_emb(self, *args, **kwargs) -> torch.Tensor:\n","        x, edge_index, batch = self.arguments_read(*args, **kwargs)\n","        post_conv = self.conv1(x, edge_index)\n","        for conv in self.convs:\n","            post_conv = conv(post_conv, edge_index)\n","        return post_conv\n","\n","\n","class GCNConv(gnn.GCNConv):\n","\n","    def __init__(self, *args, **kwargs):\n","        super().__init__(*args, **kwargs)\n","        self.edge_weight = None\n","        self.weight = nn.Parameter(self.lin.weight.data.T.clone().detach())\n","\n","    def forward(self, x: Tensor, edge_index: Adj,\n","                edge_weight: OptTensor = None) -> Tensor:\n","        \"\"\"\"\"\"\n","\n","        if self.normalize and edge_weight is None:\n","            if isinstance(edge_index, Tensor):\n","                cache = self._cached_edge_index\n","                if cache is None:\n","                    edge_index, edge_weight = gnn.conv.gcn_conv.gcn_norm(  # yapf: disable\n","                        edge_index, edge_weight, x.size(self.node_dim),\n","                        self.improved, self.add_self_loops, dtype=x.dtype)\n","                    if self.cached:\n","                        self._cached_edge_index = (edge_index, edge_weight)\n","                else:\n","                    edge_index, edge_weight = cache[0], cache[1]\n","\n","            elif isinstance(edge_index, SparseTensor):\n","                cache = self._cached_adj_t\n","                if cache is None:\n","                    edge_index = gnn.conv.gcn_conv.gcn_norm(  # yapf: disable\n","                        edge_index, edge_weight, x.size(self.node_dim),\n","                        self.improved, self.add_self_loops, dtype=x.dtype)\n","                    if self.cached:\n","                        self._cached_adj_t = edge_index\n","                else:\n","                    edge_index = cache\n","\n","        # --- add require_grad ---\n","        edge_weight.requires_grad_(True)\n","\n","        x = torch.matmul(x, self.weight)\n","\n","        # propagate_type: (x: Tensor, edge_weight: OptTensor)\n","        out = self.propagate(edge_index, x=x, edge_weight=edge_weight,\n","                             size=None)\n","\n","        if self.bias is not None:\n","            out += self.bias\n","\n","        # --- My: record edge_weight ---\n","        self.edge_weight = edge_weight\n","\n","        return out\n","\n","    def propagate(self, edge_index: Adj, size: Size = None, **kwargs):\n","        size = self.__check_input__(edge_index, size)\n","\n","        # Run \"fused\" message and aggregation (if applicable).\n","        if (isinstance(edge_index, SparseTensor) and self.fuse\n","                and not self._explain):\n","            coll_dict = self.__collect__(self.__fused_user_args__, edge_index,\n","                                         size, kwargs)\n","\n","            msg_aggr_kwargs = self.inspector.distribute(\n","                'message_and_aggregate', coll_dict)\n","            out = self.message_and_aggregate(edge_index, **msg_aggr_kwargs)\n","\n","            update_kwargs = self.inspector.distribute('update', coll_dict)\n","            return self.update(out, **update_kwargs)\n","\n","        # Otherwise, run both functions in separation.\n","        elif isinstance(edge_index, Tensor) or not self.fuse:\n","            coll_dict = self.__collect__(self.__user_args__, edge_index, size,\n","                                         kwargs)\n","\n","            msg_kwargs = self.inspector.distribute('message', coll_dict)\n","            out = self.message(**msg_kwargs)\n","\n","            # For `GNNExplainer`, we require a separate message and aggregate\n","            # procedure since this allows us to inject the `edge_mask` into the\n","            # message passing computation scheme.\n","            if self._explain:\n","                edge_mask = self.__edge_mask__\n","                # Some ops add self-loops to `edge_index`. We need to do the\n","                # same for `edge_mask` (but do not train those).\n","                if out.size(self.node_dim) != edge_mask.size(0):\n","                    loop = edge_mask.new_ones(size[0])\n","                    edge_mask = torch.cat([edge_mask, loop], dim=0)\n","                assert out.size(self.node_dim) == edge_mask.size(0)\n","                out = out * edge_mask.view([-1] + [1] * (out.dim() - 1))\n","\n","            aggr_kwargs = self.inspector.distribute('aggregate', coll_dict)\n","            out = self.aggregate(out, **aggr_kwargs)\n","\n","            update_kwargs = self.inspector.distribute('update', coll_dict)\n","            return self.update(out, **update_kwargs)\n","\n","\n","class GINConv(gnn.GINConv):\n","\n","    def __init__(self, nn: Callable, eps: float = 0., train_eps: bool = False,\n","                 **kwargs):\n","        super().__init__(nn, eps, train_eps, **kwargs)\n","        self.edge_weight = None\n","        self.fc_steps = None\n","        self.reweight = None\n","\n","\n","    def forward(self, x: Union[Tensor, OptPairTensor], edge_index: Adj,\n","                edge_weight: OptTensor = None, task='explain', **kwargs) -> Tensor:\n","        \"\"\"\"\"\"\n","        self.num_nodes = x.shape[0]\n","        if isinstance(x, Tensor):\n","            x: OptPairTensor = (x, x)\n","\n","        # propagate_type: (x: OptPairTensor)\n","        if edge_weight is not None:\n","            self.edge_weight = edge_weight\n","            assert edge_weight.shape[0] == edge_index.shape[1]\n","            self.reweight = False\n","        else:\n","            edge_index, _ = remove_self_loops(edge_index)\n","            self_loop_edge_index, _ = add_self_loops(edge_index, num_nodes=self.num_nodes)\n","            if self_loop_edge_index.shape[1] != edge_index.shape[1]:\n","                edge_index = self_loop_edge_index\n","            self.reweight = True\n","        out = self.propagate(edge_index, x=x[0], size=None)\n","\n","        if task == 'explain':\n","            layer_extractor = []\n","            hooks = []\n","\n","            def register_hook(module: nn.Module):\n","                if not list(module.children()):\n","                    hooks.append(module.register_forward_hook(forward_hook))\n","\n","            def forward_hook(module: nn.Module, input: Tuple[Tensor], output: Tensor):\n","                # input contains x and edge_index\n","                layer_extractor.append((module, input[0], output))\n","\n","            # --- register hooks ---\n","            self.nn.apply(register_hook)\n","\n","            nn_out = self.nn(out)\n","\n","            for hook in hooks:\n","                hook.remove()\n","\n","            fc_steps = []\n","            step = {'input': None, 'module': [], 'output': None}\n","            for layer in layer_extractor:\n","                if isinstance(layer[0], nn.Linear):\n","                    if step['module']:\n","                        fc_steps.append(step)\n","                    # step = {'input': layer[1], 'module': [], 'output': None}\n","                    step = {'input': None, 'module': [], 'output': None}\n","                step['module'].append(layer[0])\n","                if kwargs.get('probe'):\n","                    step['output'] = layer[2]\n","                else:\n","                    step['output'] = None\n","\n","            if step['module']:\n","                fc_steps.append(step)\n","            self.fc_steps = fc_steps\n","        else:\n","            nn_out = self.nn(out)\n","\n","\n","        return nn_out\n","\n","    def message(self, x_j: Tensor) -> Tensor:\n","        if self.reweight:\n","            edge_weight = torch.ones(x_j.shape[0], device=x_j.device)\n","            edge_weight.data[-self.num_nodes:] += self.eps\n","            edge_weight = edge_weight.detach().clone()\n","            edge_weight.requires_grad_(True)\n","            self.edge_weight = edge_weight\n","        return x_j * self.edge_weight.view(-1, 1)\n","\n","    def propagate(self, edge_index: Adj, size: Size = None, **kwargs):\n","        size = self.__check_input__(edge_index, size)\n","\n","        # Run \"fused\" message and aggregation (if applicable).\n","        if (isinstance(edge_index, SparseTensor) and self.fuse\n","                and not self._explain):\n","            coll_dict = self.__collect__(self.__fused_user_args__, edge_index,\n","                                         size, kwargs)\n","\n","            msg_aggr_kwargs = self.inspector.distribute(\n","                'message_and_aggregate', coll_dict)\n","            out = self.message_and_aggregate(edge_index, **msg_aggr_kwargs)\n","\n","            update_kwargs = self.inspector.distribute('update', coll_dict)\n","            return self.update(out, **update_kwargs)\n","\n","        # Otherwise, run both functions in separation.\n","        elif isinstance(edge_index, Tensor) or not self.fuse:\n","            coll_dict = self.__collect__(self.__user_args__, edge_index, size,\n","                                         kwargs)\n","\n","            msg_kwargs = self.inspector.distribute('message', coll_dict)\n","            out = self.message(**msg_kwargs)\n","\n","            # For `GNNExplainer`, we require a separate message and aggregate\n","            # procedure since this allows us to inject the `edge_mask` into the\n","            # message passing computation scheme.\n","            if self._explain:\n","                edge_mask = self.__edge_mask__\n","                # Some ops add self-loops to `edge_index`. We need to do the\n","                # same for `edge_mask` (but do not train those).\n","                if out.size(self.node_dim) != edge_mask.size(0):\n","                    loop = edge_mask.new_ones(size[0])\n","                    edge_mask = torch.cat([edge_mask, loop], dim=0)\n","                assert out.size(self.node_dim) == edge_mask.size(0)\n","                out = out * edge_mask.view([-1] + [1] * (out.dim() - 1))\n","\n","            aggr_kwargs = self.inspector.distribute('aggregate', coll_dict)\n","            out = self.aggregate(out, **aggr_kwargs)\n","\n","            update_kwargs = self.inspector.distribute('update', coll_dict)\n","            return self.update(out, **update_kwargs)\n","\n","\n","class GNNPool(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","\n","\n","class GlobalMeanPool(GNNPool):\n","\n","    def __init__(self):\n","        super().__init__()\n","\n","    def forward(self, x, batch):\n","        return gnn.global_mean_pool(x, batch)\n","\n","\n","class IdenticalPool(GNNPool):\n","\n","    def __init__(self):\n","        super().__init__()\n","\n","    def forward(self, x, batch):\n","        return x\n","\n","\n","class GraphSequential(nn.Sequential):\n","\n","    def __init__(self, *args):\n","        super().__init__(*args)\n","\n","    def forward(self, *input) -> Tensor:\n","        for module in self:\n","            if isinstance(input, tuple):\n","                input = module(*input)\n","            else:\n","                input = module(input)\n","        return input\n","\n","\n","# explain_mask in propagation haven't pass sigmoid func\n","class GCN_2l_mask(GNNBasic):\n","\n","    def __init__(self, model_level, dim_node, dim_hidden, num_classes):\n","        super().__init__()\n","        num_layer = 2\n","\n","        self.conv1 = GCNConv_mask(dim_node, dim_hidden)\n","        self.convs = nn.ModuleList(\n","            [\n","                GCNConv_mask(dim_hidden, dim_hidden)\n","                for _ in range(num_layer - 1)\n","            ]\n","        )\n","        self.relu1 = nn.ReLU()\n","        self.relus = nn.ModuleList(\n","            [\n","                nn.ReLU()\n","                for _ in range(num_layer - 1)\n","            ]\n","        )\n","        if model_level == 'node':\n","            self.readout = IdenticalPool()\n","        else:\n","            self.readout = GlobalMeanPool()\n","\n","        self.ffn = nn.Sequential(*(\n","                [nn.Linear(dim_hidden, num_classes)]\n","        ))\n","\n","        self.dropout = nn.Dropout()\n","\n","    def forward(self, *args, **kwargs) -> torch.Tensor:\n","        \"\"\"\n","        :param Required[data]: Batch - input data\n","        :return:\n","        \"\"\"\n","        x, edge_index, batch = self.arguments_read(*args, **kwargs)\n","\n","        post_conv = self.relu1(self.conv1(x, edge_index))\n","        for conv, relu in zip(self.convs, self.relus):\n","            post_conv = relu(conv(post_conv, edge_index))\n","\n","        out_readout = self.readout(post_conv, batch)\n","\n","        out = self.ffn(out_readout)\n","\n","        return out\n","\n","    def get_emb(self, *args, **kwargs) -> torch.Tensor:\n","        x, edge_index, batch = self.arguments_read(*args, **kwargs)\n","        post_conv = self.conv1(x, edge_index)\n","        for conv in self.convs:\n","            post_conv = conv(post_conv, edge_index)\n","        return post_conv\n","\n","\n","class GIN_2l_mask(GNNBasic):\n","\n","    def __init__(self, model_level, dim_node, dim_hidden, num_classes):\n","        super().__init__()\n","        num_layer = 2\n","\n","        self.conv1 = GINConv_mask(nn.Sequential(nn.Linear(dim_node, dim_hidden), nn.ReLU(),\n","                                           nn.Linear(dim_hidden, dim_hidden), nn.ReLU()))#,\n","                                           # nn.BatchNorm1d(dim_hidden)))\n","        self.convs = nn.ModuleList(\n","            [\n","                GINConv_mask(nn.Sequential(nn.Linear(dim_hidden, dim_hidden), nn.ReLU(),\n","                                      nn.Linear(dim_hidden, dim_hidden), nn.ReLU()))#,\n","                                      # nn.BatchNorm1d(dim_hidden)))\n","                for _ in range(num_layer - 1)\n","             ]\n","        )\n","        self.relu1 = nn.ReLU()\n","        self.relus = nn.ModuleList(\n","            [\n","                nn.ReLU()\n","                for _ in range(num_layer - 1)\n","            ]\n","        )\n","        if model_level == 'node':\n","            self.readout = IdenticalPool()\n","        else:\n","            self.readout = GlobalMeanPool()\n","\n","        self.ffn = nn.Sequential(*(\n","                [nn.Linear(dim_hidden, dim_hidden)] +\n","                [nn.ReLU(), nn.Dropout(), nn.Linear(dim_hidden, num_classes)]\n","        ))\n","\n","        self.dropout = nn.Dropout()\n","\n","    def forward(self, *args, **kwargs) -> torch.Tensor:\n","        \"\"\"\n","        :param Required[data]: Batch - input data\n","        :return:\n","        \"\"\"\n","        x, edge_index, batch = self.arguments_read(*args, **kwargs)\n","\n","\n","        post_conv = self.conv1(x, edge_index)\n","        for conv in self.convs:\n","            post_conv = conv(post_conv, edge_index)\n","\n","\n","        out_readout = self.readout(post_conv, batch)\n","\n","        out = self.ffn(out_readout)\n","        return out\n","\n","    def get_emb(self, *args, **kwargs) -> torch.Tensor:\n","        x, edge_index, batch = self.arguments_read(*args, **kwargs)\n","        post_conv = self.conv1(x, edge_index)\n","        for conv in self.convs:\n","            post_conv = conv(post_conv, edge_index)\n","        return post_conv\n","\n","\n","class GCNConv_mask(gnn.GCNConv):\n","\n","    def __init__(self, *args, **kwargs):\n","        super().__init__(*args, **kwargs)\n","        self.edge_weight = None\n","        self.weight = nn.Parameter(self.lin.weight.data.T.clone().detach())\n","\n","    def forward(self, x: Tensor, edge_index: Adj,\n","                edge_weight: OptTensor = None) -> Tensor:\n","        \"\"\"\"\"\"\n","\n","        if self.normalize and edge_weight is None:\n","            if isinstance(edge_index, Tensor):\n","                cache = self._cached_edge_index\n","                if cache is None:\n","                    edge_index, edge_weight = gnn.conv.gcn_conv.gcn_norm(  # yapf: disable\n","                        edge_index, edge_weight, x.size(self.node_dim),\n","                        self.improved, self.add_self_loops, dtype=x.dtype)\n","                    if self.cached:\n","                        self._cached_edge_index = (edge_index, edge_weight)\n","                else:\n","                    edge_index, edge_weight = cache[0], cache[1]\n","\n","            elif isinstance(edge_index, SparseTensor):\n","                cache = self._cached_adj_t\n","                if cache is None:\n","                    edge_index = gnn.conv.gcn_conv.gcn_norm(  # yapf: disable\n","                        edge_index, edge_weight, x.size(self.node_dim),\n","                        self.improved, self.add_self_loops, dtype=x.dtype)\n","                    if self.cached:\n","                        self._cached_adj_t = edge_index\n","                else:\n","                    edge_index = cache\n","\n","        # --- add require_grad ---\n","        edge_weight.requires_grad_(True)\n","\n","        x = torch.matmul(x, self.weight)\n","\n","        # propagate_type: (x: Tensor, edge_weight: OptTensor)\n","        out = self.propagate(edge_index, x=x, edge_weight=edge_weight,\n","                             size=None)\n","\n","        if self.bias is not None:\n","            out += self.bias\n","\n","        # --- My: record edge_weight ---\n","        self.edge_weight = edge_weight\n","\n","        return out\n","\n","    def propagate(self, edge_index: Adj, size: Size = None, **kwargs):\n","        size = self.__check_input__(edge_index, size)\n","\n","        # Run \"fused\" message and aggregation (if applicable).\n","        if (isinstance(edge_index, SparseTensor) and self.fuse\n","                and not self._explain):\n","            coll_dict = self.__collect__(self.__fused_user_args__, edge_index,\n","                                         size, kwargs)\n","\n","            msg_aggr_kwargs = self.inspector.distribute(\n","                'message_and_aggregate', coll_dict)\n","            out = self.message_and_aggregate(edge_index, **msg_aggr_kwargs)\n","\n","            update_kwargs = self.inspector.distribute('update', coll_dict)\n","            return self.update(out, **update_kwargs)\n","\n","        # Otherwise, run both functions in separation.\n","        elif isinstance(edge_index, Tensor) or not self.fuse:\n","            coll_dict = self.__collect__(self.__user_args__, edge_index, size,\n","                                         kwargs)\n","\n","            msg_kwargs = self.inspector.distribute('message', coll_dict)\n","            out = self.message(**msg_kwargs)\n","\n","            # For `GNNExplainer`, we require a separate message and aggregate\n","            # procedure since this allows us to inject the `edge_mask` into the\n","            # message passing computation scheme.\n","            if self._explain:\n","                edge_mask = self.__edge_mask__\n","                # Some ops add self-loops to `edge_index`. We need to do the\n","                # same for `edge_mask` (but do not train those).\n","                if out.size(self.node_dim) != edge_mask.size(0):\n","                    loop = edge_mask.new_ones(size[0])\n","                    edge_mask = torch.cat([edge_mask, loop], dim=0)\n","                assert out.size(self.node_dim) == edge_mask.size(0)\n","                out = out * edge_mask.view([-1] + [1] * (out.dim() - 1))\n","\n","            aggr_kwargs = self.inspector.distribute('aggregate', coll_dict)\n","            out = self.aggregate(out, **aggr_kwargs)\n","\n","            update_kwargs = self.inspector.distribute('update', coll_dict)\n","            return self.update(out, **update_kwargs)\n","\n","\n","class GINConv_mask(gnn.GINConv):\n","\n","    def __init__(self, nn: Callable, eps: float = 0., train_eps: bool = False,\n","                 **kwargs):\n","        super().__init__(nn, eps, train_eps, **kwargs)\n","        self.edge_weight = None\n","        self.fc_steps = None\n","        self.reweight = None\n","\n","\n","    def forward(self, x: Union[Tensor, OptPairTensor], edge_index: Adj,\n","                edge_weight: OptTensor = None, task='explain', **kwargs) -> Tensor:\n","        \"\"\"\"\"\"\n","        self.num_nodes = x.shape[0]\n","        if isinstance(x, Tensor):\n","            x: OptPairTensor = (x, x)\n","\n","        # propagate_type: (x: OptPairTensor)\n","        if edge_weight is not None:\n","            self.edge_weight = edge_weight\n","            assert edge_weight.shape[0] == edge_index.shape[1]\n","            self.reweight = False\n","        else:\n","            edge_index, _ = remove_self_loops(edge_index)\n","            self_loop_edge_index, _ = add_self_loops(edge_index, num_nodes=self.num_nodes)\n","            if self_loop_edge_index.shape[1] != edge_index.shape[1]:\n","                edge_index = self_loop_edge_index\n","            self.reweight = True\n","        out = self.propagate(edge_index, x=x[0], size=None)\n","\n","        if task == 'explain':\n","            layer_extractor = []\n","            hooks = []\n","\n","            def register_hook(module: nn.Module):\n","                if not list(module.children()):\n","                    hooks.append(module.register_forward_hook(forward_hook))\n","\n","            def forward_hook(module: nn.Module, input: Tuple[Tensor], output: Tensor):\n","                # input contains x and edge_index\n","                layer_extractor.append((module, input[0], output))\n","\n","            # --- register hooks ---\n","            self.nn.apply(register_hook)\n","\n","            nn_out = self.nn(out)\n","\n","            for hook in hooks:\n","                hook.remove()\n","\n","            fc_steps = []\n","            step = {'input': None, 'module': [], 'output': None}\n","            for layer in layer_extractor:\n","                if isinstance(layer[0], nn.Linear):\n","                    if step['module']:\n","                        fc_steps.append(step)\n","                    # step = {'input': layer[1], 'module': [], 'output': None}\n","                    step = {'input': None, 'module': [], 'output': None}\n","                step['module'].append(layer[0])\n","                if kwargs.get('probe'):\n","                    step['output'] = layer[2]\n","                else:\n","                    step['output'] = None\n","\n","            if step['module']:\n","                fc_steps.append(step)\n","            self.fc_steps = fc_steps\n","        else:\n","            nn_out = self.nn(out)\n","\n","\n","        return nn_out\n","\n","    def message(self, x_j: Tensor) -> Tensor:\n","        if self.reweight:\n","            edge_weight = torch.ones(x_j.shape[0], device=x_j.device)\n","            edge_weight.data[-self.num_nodes:] += self.eps\n","            edge_weight = edge_weight.detach().clone()\n","            edge_weight.requires_grad_(True)\n","            self.edge_weight = edge_weight\n","        return x_j * self.edge_weight.view(-1, 1)\n","\n","    def propagate(self, edge_index: Adj, size: Size = None, **kwargs):\n","        size = self.__check_input__(edge_index, size)\n","\n","        # Run \"fused\" message and aggregation (if applicable).\n","        if (isinstance(edge_index, SparseTensor) and self.fuse\n","                and not self._explain):\n","            coll_dict = self.__collect__(self.__fused_user_args__, edge_index,\n","                                         size, kwargs)\n","\n","            msg_aggr_kwargs = self.inspector.distribute(\n","                'message_and_aggregate', coll_dict)\n","            out = self.message_and_aggregate(edge_index, **msg_aggr_kwargs)\n","\n","            update_kwargs = self.inspector.distribute('update', coll_dict)\n","            return self.update(out, **update_kwargs)\n","\n","        # Otherwise, run both functions in separation.\n","        elif isinstance(edge_index, Tensor) or not self.fuse:\n","            coll_dict = self.__collect__(self.__user_args__, edge_index, size,\n","                                         kwargs)\n","\n","            msg_kwargs = self.inspector.distribute('message', coll_dict)\n","            out = self.message(**msg_kwargs)\n","\n","            # For `GNNExplainer`, we require a separate message and aggregate\n","            # procedure since this allows us to inject the `edge_mask` into the\n","            # message passing computation scheme.\n","            if self._explain:\n","                edge_mask = self.__edge_mask__\n","                # Some ops add self-loops to `edge_index`. We need to do the\n","                # same for `edge_mask` (but do not train those).\n","                if out.size(self.node_dim) != edge_mask.size(0):\n","                    loop = edge_mask.new_ones(size[0])\n","                    edge_mask = torch.cat([edge_mask, loop], dim=0)\n","                assert out.size(self.node_dim) == edge_mask.size(0)\n","                out = out * edge_mask.view([-1] + [1] * (out.dim() - 1))\n","\n","            aggr_kwargs = self.inspector.distribute('aggregate', coll_dict)\n","            out = self.aggregate(out, **aggr_kwargs)\n","\n","            update_kwargs = self.inspector.distribute('update', coll_dict)\n","            return self.update(out, **update_kwargs)"]}]}