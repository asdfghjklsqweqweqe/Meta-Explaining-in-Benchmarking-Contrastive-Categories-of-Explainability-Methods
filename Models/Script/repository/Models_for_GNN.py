# -*- coding: utf-8 -*-
"""Models for GNN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NWBoQYS3CDbzDg4X-ZkmE7dPCFlxVzTk
"""

# Install required packages.
#import os
#import torch
#os.environ['TORCH'] = torch.__version__
#print(torch.__version__)

#!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html
#!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html
#!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git

from ast import mod
from torch_geometric.sampler.neighbor_sampler import torch_geometric


import argparse
import os
import torch as th
import torch
import torch.nn as nn
import matplotlib.pyplot as plt
from torch_geometric.nn import GCNConv
import torch.nn.functional as F
from torch.nn import Linear
from sklearn.model_selection import train_test_split
import numpy as np
from torch_geometric.datasets import TUDataset
from torch_geometric.nn import GCNConv, global_mean_pool
from torch_geometric.loader import DataLoader
from torch_geometric.explain import Explainer, GNNExplainer
import torch_geometric.nn as gnn


class GlobalMeanPool(nn.Module):

    def __init__(self):
        super().__init__()

    def forward(self, x, batch):
        return gnn.global_mean_pool(x, batch)
################################################################################
class IdenticalPool(nn.Module):

    def __init__(self):
        super().__init__()

    def forward(self, x, batch):
        return x

################################################################################
class Graph_Net(torch.nn.Module):
    def __init__(self, model_name, model_level, input_dim, hidden_dim, output_dim, num_hid_layers, Bias, Weight_Initializer, dropout_rate,
                 pred_hidden_dims=[], concat=True, bn=True,
                 add_self=False, args=None):
        if model_name == 'GCN+GAP':
            super(Graph_Net, self).__init__()
            self.input_dim = input_dim
            print ('Graph_Net Input_Dimension:', self.input_dim)

            self.hidden_dim = hidden_dim
            print ('Graph_Net Hidden_Dimension:', self.hidden_dim)

            self.output_dim = output_dim
            print ('Graph_Net Output_Dimension:', self.output_dim)

            self.num_hid_layers = num_hid_layers
            print ('Graph_Net Number_of_Hidden_Layers:', self.num_hid_layers)


            self.args = args
            self.act = F.relu


            self.GConvs = torch.nn.ModuleList()
            #self.gcn1 = GCNConv(self.input_dim, self.hidden_dim, bias=Bias)
            #torch.nn.init.ones_(self.gcn1.lin.weight)
            #self.GConvs.append(self.gcn1)

            for layer in range(self.num_hid_layers):
                self.GConvs.append(GCNConv(self.hidden_dim, self.hidden_dim, bias=Bias))

            #self.GConvs.append(GCNConv(self.hidden_dim, self.output_dim))
            print('len(self.GConvs):', len(self.GConvs))
            self.dropout = nn.Dropout(p=dropout_rate)

            if model_level == 'node':
                self.readout = IdenticalPool()
            else:
                self.readout = GlobalMeanPool()

            #self.ffn = nn.Linear(self.output_dim, self.output_dim, bias=False)
            self.ffn = nn.Linear(self.hidden_dim, self.output_dim, bias=Bias)
            torch.nn.init.ones_(self.ffn.weight)

            mean = 0
            std = 0.1
            self.initialize_weights(Weight_Initializer, Bias, mean, std)



        else:
            print('Model is not defined well.')

    def initialize_weights(model, Weight_Initializer, Bias, mean, std):
        # 1. Xavier Normal_.  2. Kaiming Normal_.  3. Uniform (0,0.1std)
        if Weight_Initializer == 1:                                             #.      1. Xavier Normal_.
            for i,layers in enumerate(model.children()):
                #print("no condition: ", i, layers)
                if isinstance(layers, torch.nn.ModuleList):
                    for j, layer in enumerate(layers.modules()):
                        if isinstance(layer, GCNConv):
                            torch.nn.init.xavier_normal_(layer.lin.weight)
                            #print(i, j, "   GCN modules:    ",layer)
                            #print("done")
                            if Bias:
                                layer.bias.data.zero_()
                            #print("layer weights: ", layer.state_dict().get('lin.weight'))
                            #print("layer bias: ", layer.bias)
                        else: 
                            #print(i, j, "   components of a GCN :     ", layer)
                            pass
                if isinstance(layers, torch.nn.Linear):
                    #print("Dense before update: ", layers.weight)
                    torch.nn.init.xavier_normal_(layers.weight)
                    if Bias:
                        layers.bias.data.zero_()
                    #print(i, "   Dense:     ", layers)
                    #print("Dense after update: ", layers.weight)
                    #print("layer bias: ", layers.bias)
                elif isinstance(layers, (GlobalMeanPool)):
                    #print(i, "   GAP:      ",  layers)
                    pass
                elif isinstance(layers, (IdenticalPool)):
                    #print(i, "   IAP:      ",  layers)
                    pass

        if Weight_Initializer == 2:                                             #.      2. Kaiming Normal_.
            for i,layers in enumerate(model.children()):
                #print("no condition: ", i, layers)
                if isinstance(layers, torch.nn.ModuleList):
                    for j, layer in enumerate(layers.modules()):
                        if isinstance(layer, GCNConv):
                            torch.nn.init.kaiming_normal_(layer.lin.weight)
                            #print(i, j, "   GCN modules:    ",layer)
                            #print("done")
                            if Bias:
                                layer.bias.data.zero_()
                            #print(layer.state_dict().get('lin.weight'))
                            #print("layer bias: ", layer.bias)
                        else: 
                            #print(i, j, "   components of a GCN :     ", layer)
                            pass
                if isinstance(layers, torch.nn.Linear):
                    #print("Dense before update: ", layers.weight)
                    torch.nn.init.kaiming_normal_(layers.weight)
                    if Bias:
                        layers.bias.data.zero_()
                    #print(i, "   Dense:     ", layers)
                    #print("Dense after update: ", layers.weight)
                    #print("layer bias: ", layers.bias)
                elif isinstance(layers, (GlobalMeanPool)):
                    #print(i, "   GAP:      ",  layers)
                    pass
                elif isinstance(layers, (IdenticalPool)):
                    #print(i, "   IAP:      ",  layers)
                    pass
                            
        if Weight_Initializer == 3:                                             #.      3. Uniform (0,0.1std)
            for i,layers in enumerate(model.children()):
                #print("no condition: ", i, layers)
                if isinstance(layers, torch.nn.ModuleList):
                    for j, layer in enumerate(layers.modules()):
                        if isinstance(layer, GCNConv):
                            torch.nn.init.normal_(layer.lin.weight.data, mean, std)
                            #print(i, j, "   GCN modules:    ",layer)
                            #print("done")
                            if Bias:
                                layer.bias.data.zero_()
                            #print(layer.state_dict().get('lin.weight'))
                            #print("layer bias: ", layer.bias)
                        else: 
                            #print(i, j, "   components of a GCN :     ", layer)
                            pass
                if isinstance(layers, torch.nn.Linear):
                    #print("Dense before update: ", layers.weight)
                    torch.nn.init.normal_(layers.weight, mean, std)
                    if Bias:
                        layers.bias.data.zero_()
                    #print(i, "   Dense:     ", layers)
                    #print("Dense after update: ", layers.weight)
                    #print("layer bias: ", layers.bias)
                elif isinstance(layers, (GlobalMeanPool)):
                    #print(i, "   GAP:      ",  layers)
                    pass
                elif isinstance(layers, (IdenticalPool)):
                    #print(i, "   IAP:      ",  layers)
                    pass



    def forward(self, data):
        x, edge_index, batch = data.x, data.edge_index, data.batch
        Output_of_Hidden_Layers = []
        for i in range(self.num_hid_layers):
            x = self.GConvs[i](x, edge_index)
            x = F.relu(x)
            x = self.dropout(x)
            Output_of_Hidden_Layers.append(x)

        pooling_layer_output = self.readout(x, batch)
        ffn_output = self.ffn(pooling_layer_output)
        ffn_output = F.relu(ffn_output)

        log_soft = F.log_softmax(ffn_output, dim=1)
        soft = F.softmax(log_soft, dim=1)

        return Output_of_Hidden_Layers, pooling_layer_output, ffn_output, log_soft, soft

#GNN_Model = Graph_Net(model_name='GCN+GAP', model_level='graph', input_dim=7, 
#                      hidden_dim=7, output_dim=2, num_hid_layers=2, Bias=True, 
#                      Weight_Initializer=1, dropout_rate=0.1)
#print('=====================================================================')
#print(GNN_Model)
#print('=====================================================================')






#dataset = TUDataset(root='data/TUDataset', name='MUTAG')
#Output_of_Hidden_Layers, pooling_layer_output, ffn_output, log_soft, soft = GNN_Model(dataset[0])
#print(Output_of_Hidden_Layers, pooling_layer_output, ffn_output, log_soft, soft)