# -*- coding: utf-8 -*-
"""PGExplainer on Graph Classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1k_IYIs2ABzES743hQKuWSespZbufyKiT

## ***PGExplainer***


> Moduled: Accpeting the four GNNs (GCN+GAP, DGCNN, DIFFPOOL, and GIN)


---
"""

import numpy as np
from math import sqrt
from statistics import mean
import torch_geometric
from torch_geometric.datasets import TUDataset
import torch
import torch.nn as nn
from torch.nn.parameter import Parameter
from torch_geometric.nn import GCNConv
import torch.nn.functional as F
from torch.nn import Linear, ReLU, Sequential
from torch.autograd import graph
from typing import Any, Dict, Optional, Union
from IPython.core.display import deepcopy
from torch_geometric.nn import MessagePassing
import copy

class PGExplainer:
    coeffs = {'edge_size': 0.05,
              'edge_ent': 1.0,
              'temp': [5.0, 2.0],
              'bias': 0.0,
              }
    def __init__(self, GNN_Model, explainer_save_index, Exp_Epoch, Exp_lr, node_feat_dim,):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.GNN_Model = GNN_Model
        self.node_feat_dim = node_feat_dim
        self.explainer_lr = Exp_lr
        self.explainer_hid_dim = 64
        self.pgexp_mlp = Sequential(Linear(self.node_feat_dim * 2, self.explainer_hid_dim), ReLU(), Linear(self.explainer_hid_dim, 1)).to(self.device)
        self.pgexp_mlp_optimizer = torch.optim.Adam(self.pgexp_mlp.parameters(), lr=self.explainer_lr)
        self.explainer_epochs = Exp_Epoch
        self.criterion = F.binary_cross_entropy_with_logits
        self.explainer_save_index = explainer_save_index
        self.Explainability_name = 'PGExplainer'
        self.Task_name = 'Graph Classification'

    def explainer_loss(self, By_Perturbation_predicted_label, predicted_label):
        loss_per_epoch = self.criterion(By_Perturbation_predicted_label, predicted_label)
        return loss_per_epoch

    def train_step_explainer(self, merged_embeddings_list_batchs, GNN_Model, your_dataset, temperature, GNN_Model_preds_NOT_MASKED, target_class):

        self.pgexp_mlp.train()
        self.pgexp_mlp.zero_grad()
        for batched_merged_embeddings, batched_preds_NOT_MASKED, batched_graphs in zip(merged_embeddings_list_batchs, GNN_Model_preds_NOT_MASKED, your_dataset):

            explaier_outputs = self.pgexp_mlp(batched_merged_embeddings).view(-1)
            edge_mask = self.binary_concrete(explaier_outputs, temperature)
            batched_graphs = batched_graphs.to(self.device)

            # if self.GNN_Model.__class__.__name__ == "GCN_plus_GAP_Model":
            #     self.apply_masks(GNN_Model, edge_mask, batched_graphs.edge_index, apply_sigmoid=True)

            if self.GNN_Model.__class__.__name__ == "GCN_plus_GAP_Model":
                Output_of_Hidden_Layers, pooling_layer_output, ffn_output, soft_MASKED = self.GNN_Model(batched_graphs, edge_mask)
            elif self.GNN_Model.__class__.__name__ == "DGCNN_Model":
                final_GNN_layer_output, sortpooled_embedings, output_conv1d_1, maxpooled_output_conv1d_1, output_conv1d_2, to_dense, output_h1, dropout_output_h1, output_h2, soft_MASKED = self.GNN_Model(batched_graphs, edge_mask)
            elif self.GNN_Model.__class__.__name__ == "DIFFPOOL_Model":
                concatination_list_of_poolings, pred_torch_without_soft, soft_MASKED = self.GNN_Model(batched_graphs, edge_mask)
            elif self.GNN_Model.__class__.__name__ == "GIN_Model":
                mlps_output_embeds, mlp_outputs_globalSUMpooled, lin1_output, lin1_output_dropouted, lin2_output, soft_MASKED = self.GNN_Model(batched_graphs, edge_mask)

            batch_loss = self.explainer_loss(soft_MASKED.detach()[:,target_class].to(torch.float32), batched_preds_NOT_MASKED.detach()[:,target_class].to(torch.float32))

            batch_loss.requires_grad = True
            batch_loss.backward(retain_graph=True)
            self.pgexp_mlp_optimizer.step()

        return edge_mask

    def train_explainer(self, GNN_Model, your_dataset, target_class):
        edge_masks_per_epoch = []
        #self.clear_masks(GNN_Model)
        merged_embeddings_list = self.get_merged_embeddings(GNN_Model, your_dataset)
        GNN_Model_preds_NOT_MASKED = []
        for batch_of_graphs in your_dataset:
            batch_of_graphs = batch_of_graphs.to(self.device)

            if self.GNN_Model.__class__.__name__ == "GCN_plus_GAP_Model":
                Output_of_Hidden_Layers, pooling_layer_output, ffn_output, soft_NOT_MASKED = self.GNN_Model(batch_of_graphs, None)
            elif self.GNN_Model.__class__.__name__ == "DGCNN_Model":
                final_GNN_layer_output, sortpooled_embedings, output_conv1d_1, maxpooled_output_conv1d_1, output_conv1d_2, to_dense, output_h1, dropout_output_h1, output_h2, soft_NOT_MASKED = self.GNN_Model(batch_of_graphs, None)
            elif self.GNN_Model.__class__.__name__ == "DIFFPOOL_Model":
                concatination_list_of_poolings, pred_torch_without_soft, soft_NOT_MASKED = self.GNN_Model(batch_of_graphs, None)
            elif self.GNN_Model.__class__.__name__ == "GIN_Model":
                mlps_output_embeds, mlp_outputs_globalSUMpooled, lin1_output, lin1_output_dropouted, lin2_output, soft_NOT_MASKED = self.GNN_Model(batch_of_graphs, None)
            GNN_Model_preds_NOT_MASKED.append(soft_NOT_MASKED)

        for epoch in range(self.explainer_epochs):
            #print("Epoch: ", epoch)
            temperature = self.compute_temp(epoch)
            edge_mask = self.train_step_explainer(merged_embeddings_list, GNN_Model, your_dataset, temperature, GNN_Model_preds_NOT_MASKED, target_class)
            edge_masks_per_epoch.append(edge_mask)

            if (epoch + 1) == self.explainer_save_index:
                torch.save({'epoch': epoch+1, 'model_state_dict': self.pgexp_mlp.state_dict(), 'optimizer_state_dict': self.pgexp_mlp_optimizer.state_dict()}, "/data/cs.aau.dk/ey33jw/Explainability_Methods/" + str(self.Explainability_name) + " on " + str(self.Task_name) + "/Model/" + str(self.GNN_Model.__class__.__name__) + "_Model_classifier_PGExplainer_MLP_" + str(epoch + 1) + "_epochs_" + str(target_class) + ".pt")
        # self.clear_masks(GNN_Model)

    def test_explainer(self, GNN_Model, your_dataset, pgexp_mlp):
        predicted_labels_MASKED = []
        edge_masks = []
        merged_embeddings_list_batchs = self.get_merged_embeddings(GNN_Model, your_dataset)
        GNN_Model_preds_NOT_MASKED = []
        pgexp_mlp.eval()
        for batch_of_graphs in your_dataset:
            batch_of_graphs = batch_of_graphs.to(self.device)

            if self.GNN_Model.__class__.__name__ == "GCN_plus_GAP_Model":
                Output_of_Hidden_Layers, pooling_layer_output, ffn_output, soft_NOT_MASKED = self.GNN_Model(batch_of_graphs, None)
            elif self.GNN_Model.__class__.__name__ == "DGCNN_Model":
                final_GNN_layer_output, sortpooled_embedings, output_conv1d_1, maxpooled_output_conv1d_1, output_conv1d_2, to_dense, output_h1, dropout_output_h1, output_h2, soft_NOT_MASKED = self.GNN_Model(batch_of_graphs, None)
            elif self.GNN_Model.__class__.__name__ == "DIFFPOOL_Model":
                concatination_list_of_poolings, pred_torch_without_soft, soft_NOT_MASKED = self.GNN_Model(batch_of_graphs, None)
            elif self.GNN_Model.__class__.__name__ == "GIN_Model":
                mlps_output_embeds, mlp_outputs_globalSUMpooled, lin1_output, lin1_output_dropouted, lin2_output, soft_NOT_MASKED = self.GNN_Model(batch_of_graphs, None)
            GNN_Model_preds_NOT_MASKED.append(soft_NOT_MASKED)

        for batched_merged_embeddings, batched_preds_NOT_MASKED, batched_graphs in zip(merged_embeddings_list_batchs, GNN_Model_preds_NOT_MASKED, your_dataset):
            explaier_outputs = pgexp_mlp(batched_merged_embeddings).view(-1)
            batched_graphs = batched_graphs.to(self.device)

            temperature = 1
            edge_mask = self.binary_concrete(explaier_outputs, temperature)
            edge_masks.append(edge_mask)

            # if self.GNN_Model.__class__.__name__ == "GCN_plus_GAP_Model":
            #     self.apply_masks(GNN_Model, edge_mask, batched_graphs.edge_index, apply_sigmoid=True)

            if self.GNN_Model.__class__.__name__ == "GCN_plus_GAP_Model":
                Output_of_Hidden_Layers, pooling_layer_output, ffn_output, soft_MASKED = self.GNN_Model(batched_graphs, edge_mask)
            elif self.GNN_Model.__class__.__name__ == "DGCNN_Model":
                final_GNN_layer_output, sortpooled_embedings, output_conv1d_1, maxpooled_output_conv1d_1, output_conv1d_2, to_dense, output_h1, dropout_output_h1, output_h2, soft_MASKED = self.GNN_Model(batched_graphs, edge_mask)
            elif self.GNN_Model.__class__.__name__ == "DIFFPOOL_Model":
                concatination_list_of_poolings, pred_torch_without_soft, soft_MASKED = self.GNN_Model(batched_graphs, edge_mask)
            elif self.GNN_Model.__class__.__name__ == "GIN_Model":
                mlps_output_embeds, mlp_outputs_globalSUMpooled, lin1_output, lin1_output_dropouted, lin2_output, soft_MASKED = self.GNN_Model(batched_graphs, edge_mask)

            predicted_labels_MASKED.append(torch.squeeze(soft_MASKED.argmax(dim=1)).tolist())
        # self.clear_masks(GNN_Model)
        return predicted_labels_MASKED, edge_masks

    def apply_masks(self, model, mask, edge_index, apply_sigmoid):
        loop_mask = edge_index[0] != edge_index[1]

        for module in model.modules():
            if isinstance(module, MessagePassing):

                if (not isinstance(mask, Parameter)
                        and '_edge_mask' in module._parameters):
                    mask = Parameter(mask)

                module.explain = True
                module._edge_mask = mask
                module._loop_mask = loop_mask
                module._apply_sigmoid = apply_sigmoid
                #print(module._edge_mask)

    def clear_masks(self, model):

        for module in model.modules():
            if isinstance(module, MessagePassing):
                module.explain = False
                module._edge_mask = None
                module._loop_mask = None
                module._apply_sigmoid = True
        return module

    def batch_handler_for_embeddings(self, batched_graphs, new_embeddings):
        unpaded_data = []
        #if batched_graphs.batch is not None:
        #    graph_sizes = [batched_graphs[i].x.size()[1] for i in range(len(batched_graphs))]
        #else:
        #    graph_sizes = [len(batched_graphs.x)]

        #for i, graph in enumerate(new_embeddings):
        #    unpaded_data.append(graph[:graph_sizes[i]])
        #new_graph_embeddings_merged = torch.cat(unpaded_data, dim=0)
        for i, graph in enumerate(new_embeddings):
            unpaded_data.append(graph)
        new_graph_embeddings_merged = torch.cat(unpaded_data, dim=0)
        return new_graph_embeddings_merged

    def get_merged_embeddings(self, GNN_Model, your_dataset):
        new_embeddings_list = []
        merged_embeddings_list = []
        for batched_graphs in your_dataset:
            new_graph_by_masks = deepcopy(batched_graphs.detach())

            new_embeddings, GNN_Model_explain_predicted_labels = self.get_hopped_embeddings(GNN_Model, new_graph_by_masks)
            #print("new_embeddings: ", new_embeddings.size())
            #print("batched_graphs: ", batched_graphs.x.size())
            new_embeddings_list.append(new_embeddings)
            if len(new_embeddings.size()) == 3:
                new_embeddings = self.batch_handler_for_embeddings(batched_graphs, new_embeddings)
                #print("size is batch")

            merged_embeddings = self.edge_embeddings(new_embeddings, new_graph_by_masks.edge_index)

            merged_embeddings_list.append(merged_embeddings)

        return merged_embeddings_list

    def edge_embeddings(self, batched_embedding, edge_index):
        Zs = [batched_embedding[edge_index[0]], batched_embedding[edge_index[1]]]
        return torch.cat(Zs, dim=-1)

    def get_hopped_embeddings(self, GNN_Model, batch_of_graphs):
        batch_of_graphs = batch_of_graphs.to(self.device)
        GNN_Model.eval()

        if self.GNN_Model.__class__.__name__ == "GCN_plus_GAP_Model":
            Output_of_Hidden_Layers, pooling_layer_output, ffn_output, soft = self.GNN_Model(batch_of_graphs, None)
            return Output_of_Hidden_Layers[-1], soft.argmax(dim=1)

        elif self.GNN_Model.__class__.__name__ == "DGCNN_Model":
            final_GNN_layer_output, sortpooled_embedings, output_conv1d_1, maxpooled_output_conv1d_1, output_conv1d_2, to_dense, output_h1, dropout_output_h1, output_h2, soft = self.GNN_Model(batch_of_graphs, None)
            return final_GNN_layer_output, soft.argmax(dim=1)

        elif self.GNN_Model.__class__.__name__ == "DIFFPOOL_Model":
            concatination_list_of_poolings, pred_torch_without_soft, soft = self.GNN_Model(batch_of_graphs, None)
            return batch_of_graphs.x, soft.argmax(dim=1)

        elif self.GNN_Model.__class__.__name__ == "GIN_Model":
            mlps_output_embeds, mlp_outputs_globalSUMpooled, lin1_output, lin1_output_dropouted, lin2_output, soft = self.GNN_Model(batch_of_graphs, None)
            return torch.squeeze(mlps_output_embeds[-1], dim=0), soft.argmax(dim=1)

    def compute_temp(self, epoch):
        temp = self.coeffs['temp']
        return temp[0] * pow(temp[1] / temp[0], epoch / self.explainer_epochs)

    def binary_concrete(self, explaier_outputs, temperature):
        bias = self.coeffs['bias']
        eps = (1 - 2 * bias) * torch.rand_like(explaier_outputs) + bias
        return torch.sigmoid((eps.log() - (1 - eps).log() + explaier_outputs) / temperature)
    def load_explainer_mlp(self, Exp_Load_index, target_class):
        pgexp_mlp = Sequential(Linear(self.node_feat_dim * 2, self.explainer_hid_dim), ReLU(), Linear(self.explainer_hid_dim, 1))
        pgexp_mlp_optimizer = torch.optim.Adam(self.pgexp_mlp.parameters(), lr=self.explainer_lr)
        checkpoint = torch.load("/data/cs.aau.dk/ey33jw/Explainability_Methods/" + str(self.Explainability_name) + " on " + str(self.Task_name) + "/Model/" + str(self.GNN_Model.__class__.__name__) + "_Model_classifier_PGExplainer_MLP_" + str(Exp_Load_index)+"_epochs_" + str(target_class) + ".pt")
        pgexp_mlp.load_state_dict(checkpoint['model_state_dict'])
        pgexp_mlp_optimizer.load_state_dict(checkpoint['optimizer_state_dict'])

        return pgexp_mlp, pgexp_mlp_optimizer

    def __call__(self, ExTrain_or_ExTest,  Exp_Load_index, your_dataset, target_class):
        if ExTrain_or_ExTest == "train":
            self.train_explainer(self.GNN_Model, your_dataset, target_class)
        elif ExTrain_or_ExTest == "test":
            pgexp_mlp, pgexp_mlp_optimizer = self.load_explainer_mlp(Exp_Load_index=Exp_Load_index, 
                                                                     target_class=target_class)
            pgexp_mlp = pgexp_mlp.to(self.device)
            predicted_labels, edge_masks = self.test_explainer(self.GNN_Model, your_dataset, pgexp_mlp)
            #print("Class: ", target_class, " predicted_labels: ", predicted_labels)
            #print("edge_masks: ", edge_masks)
            
            return edge_masks
        else:
            print("recheck")

#target_class = 0
#ExTrain_or_ExTest = 'test'

#EXP = PGExplainer(GNN_Model=GNN_Model, explainer_save_index=10, Exp_Epoch=10, Exp_lr=0.001, node_feat_dim=13)
#print(mutag_train_dataset[0])
#EXP(ExTrain_or_ExTest=ExTrain_or_ExTest, Exp_Load_index=10, your_dataset=[fake_test_dataset[-1]], target_class=target_class)
