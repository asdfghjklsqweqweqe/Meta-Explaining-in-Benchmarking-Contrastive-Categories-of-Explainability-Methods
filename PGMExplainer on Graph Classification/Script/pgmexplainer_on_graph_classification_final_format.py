# -*- coding: utf-8 -*-
"""PGMExplainer on Graph Classification Final Format.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rRNlkRMQQoQLDmqBbOB6fs7C55sUa-R2

## ***PGMExplainer***


> Moduled: Accpeting the four GNNs (GCN+GAP, DGCNN, DIFFPOOL, and GIN)


---
"""



import argparse
import os
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
import numpy as np
from math import sqrt
import math
from torch_geometric.datasets import TUDataset
import torch as th
import torch
import torch.nn as nn
from torch import Tensor
from torch.nn.parameter import Parameter
from torch_geometric.nn import GCNConv
import torch.nn.functional as F
from torch.nn import Linear, LayerNorm
from sklearn import metrics
from scipy.spatial.distance import hamming
import statistics
import pandas
from time import perf_counter
from IPython.core.display import deepcopy
from torch_geometric.nn import MessagePassing
import copy
from torch.nn import ReLU, Sequential
from torch import sigmoid
from itertools import chain
from time import perf_counter
from torch_geometric.data import Data, Batch, Dataset
from functools import partial
from torch_geometric.utils import to_networkx
from torch_geometric.utils import remove_self_loops
from typing import Callable, Union, Optional
#from torch_geometric.utils.num_nodes import maybe_num_nodes
import networkx as nx
from typing import List, Tuple, Dict
from collections import Counter
import statistics
from scipy import stats
import logging
import pandas as pd
import csv
from statistics import mean
from torch_geometric.nn import GCNConv, global_mean_pool
from torch_geometric.loader import DataLoader
import torch_geometric.nn as gnn
# from google.colab import drive

class PGM_Graph_Explainer(object):
    def __init__(self, GNN_Model, graph, perturb_feature_list, perturb_mode, perturb_indicator):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.Task_name = 'Graph Classification'
        self.Explainability_name = "PGMExplainer"
        self.GNN_Model = GNN_Model.to(self.device)
        self.GNN_Model.eval()

        self.graph = graph
        self.graph = self.graph.to(self.device)
        self.num_layers = 2
        self.perturb_feature_list = perturb_feature_list
        self.perturb_mode = perturb_mode
        self.perturb_indicator = perturb_indicator
        # self.node_feat = graph.x.numpy()
        self.node_feat = graph.x



    def cressie_read(self, X, Y, Z, data_pertubed_Samples, significance_level):
        return self.power_divergence(X=X, Y=Y, Z=Z, data_pertubed_Samples=data_pertubed_Samples, lambda_="cressie-read",
                                     significance_level=significance_level)

    def power_divergence(self, X, Y, Z, data_pertubed_Samples, lambda_, significance_level):
        if hasattr(Z, "__iter__"):
            Z = list(Z)
        else:
            raise (f"Z must be an iterable. Got object type: {type(Z)}")

        if (X in Z) or (Y in Z):
            raise ValueError(f"The variables X or Y can't be in Z. Found {X if X in Z else Y} in Z.")
        if len(Z) == 0:
            chi, p_value, dof, expected = stats.chi2_contingency(
                data_pertubed_Samples.groupby([X, Y]).size().unstack(Y, fill_value=0), lambda_=lambda_
            )
        else:
            chi = 0
            dof = 0
            for z_state, df in data_pertubed_Samples.groupby(Z):
                try:
                    c, _, d, _ = stats.chi2_contingency(df.groupby([X, Y]).size().unstack(Y, fill_value=0),
                                                        lambda_=lambda_)
                    chi += c
                    dof += d
                except ValueError:
                    if isinstance(z_state, str):
                        logging.info(f"Skipping the test {X} \u27C2 {Y} | {Z[0]}={z_state}. Not enough samples")
                    else:
                        z_str = ", ".join([f"{var}={state}" for var, state in zip(Z, z_state)])
                        logging.info(f"Skipping the test {X} \u27C2 {Y} | {z_str}. Not enough samples")
            p_value = 1 - stats.chi2.cdf(chi, df=dof)
        return chi, p_value, dof


    def perturb_node_features(self, node_feature_matrix, targeted_node_idx, random_perturbation_permission):

        graph_node_features = deepcopy(node_feature_matrix)
        targeted_node_feat_to_perturb_array = deepcopy(graph_node_features[targeted_node_idx])
        # print("targeted_node_feat_to_perturb_array: ", targeted_node_feat_to_perturb_array)
        # epsilon = 0.05 * np.max(self.node_feat, axis=0)
        epsilon = 0.05 * torch.max(self.node_feat, dim=0).values

        if random_perturbation_permission == 1:
            for i in range(targeted_node_feat_to_perturb_array.shape[0]):
                if i in self.perturb_feature_list:
                    if self.perturb_mode == "mean":
                        targeted_node_feat_to_perturb_array[i] = np.mean(node_feature_matrix[:,i])
                    elif self.perturb_mode == "zero":
                        targeted_node_feat_to_perturb_array[i] = 0
                    elif self.perturb_mode == "max":
                        targeted_node_feat_to_perturb_array[i] = np.max(node_feature_matrix[:,i])
                    elif self.perturb_mode == "uniform":
                        targeted_node_feat_to_perturb_array[i] = targeted_node_feat_to_perturb_array[i] + np.random.uniform(low=-epsilon[i], high=epsilon[i])
                        if targeted_node_feat_to_perturb_array[i] < 0:
                            targeted_node_feat_to_perturb_array[i] = 0
                        elif targeted_node_feat_to_perturb_array[i] > np.max(self.node_feat, axis=0)[i]:
                            targeted_node_feat_to_perturb_array[i] = np.max(self.node_feat, axis=0)[i]

        graph_node_features[targeted_node_idx] = targeted_node_feat_to_perturb_array

        return graph_node_features

    def gather_perturbed_node_features(self, sampling_count, index_to_perturb, noise_offset_percentage,
                                       p_value_threshold, class_index):
        if self.GNN_Model.__class__.__name__ == "GCN_plus_GAP_Model":
            Output_of_Hidden_Layers, pooling_layer_output, ffn_output, pred_torch = self.GNN_Model(self.graph, None)
        elif self.GNN_Model.__class__.__name__ == "DGCNN_Model":
            final_GNN_layer_output, sortpooled_embedings, output_conv1d_1, maxpooled_output_conv1d_1, output_conv1d_2, to_dense, output_h1, dropout_output_h1, output_h2, pred_torch = self.GNN_Model(self.graph, None)
        elif self.GNN_Model.__class__.__name__ == "DIFFPOOL_Model":
            concatination_list_of_poolings, pred_torch_without_soft, pred_torch = self.GNN_Model(self.graph, None)
        elif self.GNN_Model.__class__.__name__ == "GIN_Model":
            mlps_output_embeds, mlp_outputs_globalSUMpooled, lin1_output, lin1_output_dropouted, lin2_output, pred_torch = self.GNN_Model(self.graph, None)

        #pred_label = pred_torch.argmax(dim=1)
        pred_label = pred_torch.detach()[:, class_index].to(torch.float32).tolist()[0]

        num_nodes_in_graph = self.node_feat.shape[0]

        Samples = []
        for iteration in range(sampling_count):
            graph_original_features = deepcopy(self.node_feat)
            sample = []
            for node_index in range(num_nodes_in_graph):
                if node_index in index_to_perturb:
                    seed = np.random.randint(100)
                    if seed < noise_offset_percentage:
                        random_perturbation_permission = 1
                        graph_perturbed_features = self.perturb_node_features(
                            node_feature_matrix=graph_original_features, targeted_node_idx=node_index,
                            random_perturbation_permission=random_perturbation_permission)
                    else:
                        random_perturbation_permission = 0
                else:
                    random_perturbation_permission = 0
                sample.append(random_perturbation_permission)

                perturbed_graph = deepcopy(self.graph)
                if random_perturbation_permission:
                    # graph_perturbed_features_torch = torch.tensor(graph_perturbed_features, dtype=torch.float)
                    graph_perturbed_features_torch = graph_perturbed_features.clone().detach().float()
                    perturbed_graph.x = graph_perturbed_features_torch
                perturbed_graph = perturbed_graph.to(self.device)

                if self.GNN_Model.__class__.__name__ == "GCN_plus_GAP_Model":
                    Output_of_Hidden_Layers, pooling_layer_output, ffn_output, pred_perturb_torch = self.GNN_Model(perturbed_graph, None)
                elif self.GNN_Model.__class__.__name__ == "DGCNN_Model":
                    final_GNN_layer_output, sortpooled_embedings, output_conv1d_1, maxpooled_output_conv1d_1, output_conv1d_2, to_dense, output_h1, dropout_output_h1, output_h2, pred_perturb_torch = self.GNN_Model(perturbed_graph, None)
                elif self.GNN_Model.__class__.__name__ == "DIFFPOOL_Model":
                    concatination_list_of_poolings, pred_torch_without_soft, pred_perturb_torch = self.GNN_Model(perturbed_graph, None)
                elif self.GNN_Model.__class__.__name__ == "GIN_Model":
                    mlps_output_embeds, mlp_outputs_globalSUMpooled, lin1_output, lin1_output_dropouted, lin2_output, pred_perturb_torch = self.GNN_Model(perturbed_graph, None)

                pred_change = pred_torch.detach()[:, class_index].to(torch.float32).tolist()[0] - \
                              pred_perturb_torch.detach()[:, class_index].to(torch.float32).tolist()[0]

                sample.append(pred_change)
            Samples.append(sample)

        Samples = np.asarray(Samples)
        if self.perturb_indicator == "abs":
            Samples = np.abs(Samples)

        top = int(sampling_count / 8)
        top_idx = np.argsort(Samples[:, num_nodes_in_graph])[-top:]

        for i in range(sampling_count):
            if i in top_idx:
                Samples[i, num_nodes_in_graph] = 1
            else:
                Samples[i, num_nodes_in_graph] = 0

        return Samples

    def explain(self, num_samples, noise_offset_percentage, top_node, p_value_threshold, class_index):

        if top_node == None:
            top_node = int(self.node_feat.shape[0]/8)

#         Round 1
        Samples = self.gather_perturbed_node_features(sampling_count=num_samples, index_to_perturb=range(self.node_feat.shape[0]),
                                                      noise_offset_percentage=noise_offset_percentage,
                                                      p_value_threshold=p_value_threshold, class_index=class_index)

        data_pertubed_Samples1 = pd.DataFrame(Samples)

        p_values = []
        candidate_nodes = []
        # The entry for the graph classification data is at "num_nodes"
        for node in range(self.node_feat.shape[0]):
            chi2, p, dof = self.cressie_read(X=node, Y=self.node_feat.shape[0], Z=[],
                                             data_pertubed_Samples=data_pertubed_Samples1, significance_level=0.05)
            #print("this is returned P: ", p)
            p_values.append(p)


        number_candidates = top_node
        if len(p_values) <= number_candidates:
            number_candidates = len(p_values)-1
        candidate_nodes = np.argpartition(p_values, number_candidates)[0:number_candidates]

#         Round 2
        Samples = self.gather_perturbed_node_features(sampling_count=num_samples, index_to_perturb=candidate_nodes,
                                                      noise_offset_percentage=noise_offset_percentage,
                                                      p_value_threshold=p_value_threshold, class_index=class_index)
        data = pd.DataFrame(Samples)
        #est = ConstraintBasedEstimator(data)

        p_values = []
        dependent_nodes = []

        for node in range(self.node_feat.shape[0]):
            chi2, p, dof = self.cressie_read(X=node, Y=self.node_feat.shape[0], Z=[], data_pertubed_Samples=data,
                                             significance_level=0.05)
            #chi2, p = chi_square(node, target, [], data)
            p_values.append(p)
            if p < p_value_threshold:
                dependent_nodes.append(node)


        top_p = np.min((top_node, self.node_feat.shape[0]-1))
        ind_top_p = np.argpartition(p_values, top_p)[0:top_p]
        pgm_nodes = list(ind_top_p)

        return pgm_nodes, p_values, candidate_nodes, dependent_nodes

#input_graph = fake_test_dataset[0]
#pgmx = PGM_Graph_Explainer(GNN_Model=GNN_Model, graph=input_graph, perturb_feature_list=[None], perturb_mode = "mean",
#                           perturb_indicator = "abs")



#pgm_nodes, p_values, candidate_nodes, dependent_nodes = pgmx.explain(num_samples=len(input_graph.x), noise_offset_percentage=50,
#                                                                     top_node=5, p_value_threshold=0.05, class_index=2)
#print("pgm_nodes: ", pgm_nodes, " p_values: ", p_values, " candidate_nodes: ", candidate_nodes, "dependent_nodes: ", dependent_nodes)

#pgm_nodes_for_each_graph_correct = []
#pgm_nodes_for_each_graph_incorrect = []
#time_consumption = []

#for i in range(len(test_dataset)):

#    pgmx = PGM_Graph_Explainer(Model_Name="GCN_plus_GAP_Model", classifier_load_index=200, input_dim=7, hid_dim=7, output_dim=2, graph=test_dataset[i],
#                           perturb_feature_list=[None], perturb_mode = "mean", perturb_indicator = "abs")
#    start_time = perf_counter()
#    pgm_node_correct, p_values_correct, candidate_nodes_correct, dependent_nodes_correct = pgmx.explain(num_samples=len(test_dataset[i].x), percentage=50,
#                                                                                                        top_node=3, p_value_threshold=0.05, pred_threshold=0.1,
#                                                                                                        ctg='correct')
#    pgm_nodes_for_each_graph_correct.append(pgm_node_correct)
#    print(pgm_node_correct)
#    time_consumption.append(perf_counter() - start_time)
#    pgm_node_incorrect, p_values_incorrect, candidate_nodes_incorrect, dependent_nodes_incorrect = pgmx.explain(num_samples=len(test_dataset[i].x), percentage=50,
#                                                                                                                top_node=3, p_value_threshold=0.05, pred_threshold=0.1,
#                                                                                                                ctg='incorrect')
#    pgm_nodes_for_each_graph_incorrect.append(pgm_node_incorrect)
#    print(pgm_node_incorrect)

#print(len(time_consumption))
#print("time_consumption: ", time_consumption)
#print(statistics.mean(time_consumption))

#test_dataset_dropped_correct = deepcopy(test_dataset)
#test_dataset_dropped_incorrect = deepcopy(test_dataset)


#for i in range(len(pgm_nodes_for_each_graph_correct)):
#    for j in range(len(test_dataset_dropped_correct[i].x)):
#        if j not in pgm_nodes_for_each_graph_correct[i]:
#            test_dataset_dropped_correct[i].x[j] = torch.zeros_like(test_dataset_dropped_correct[i].x[j])

#for i in range(len(pgm_nodes_for_each_graph_incorrect)):
#    for j in range(len(test_dataset_dropped_incorrect[i].x)):
#        if j not in pgm_nodes_for_each_graph_incorrect[i]:
#            test_dataset_dropped_incorrect[i].x[j] = torch.zeros_like(test_dataset_dropped_incorrect[i].x[j])

#print(pgm_nodes_for_each_graph_correct[0])
#print(test_dataset_dropped_correct[0].x)

