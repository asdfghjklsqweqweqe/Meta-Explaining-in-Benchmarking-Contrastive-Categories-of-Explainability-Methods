{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["AD5rtmGThXDT","bC9Dju7lK0Za","Q94lhvEwZB-b"],"machine_shape":"hm","gpuType":"L4","authorship_tag":"ABX9TyPFrB4o9SG9S4qKxKRDvpPq"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["# Install required packages.\n","import os\n","\n","#!pip install torch==1.7.0\n","import torch\n","os.environ['TORCH'] = torch.__version__\n","print(torch.__version__)\n","\n","\n","!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n","!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n","!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git\n","!pip install git+https://github.com/rusty1s/pytorch_geometric.git"],"metadata":{"id":"xR3MBu2fCzCM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1716205469344,"user_tz":-120,"elapsed":43781,"user":{"displayName":"Ehsan Mobaraki","userId":"06314958514195310048"}},"outputId":"b2deb546-291b-43fe-8c56-3555b329879f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2.2.1+cu121\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m48.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for torch-geometric (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Collecting git+https://github.com/rusty1s/pytorch_geometric.git\n","  Cloning https://github.com/rusty1s/pytorch_geometric.git to /tmp/pip-req-build-7qehxh1q\n","  Running command git clone --filter=blob:none --quiet https://github.com/rusty1s/pytorch_geometric.git /tmp/pip-req-build-7qehxh1q\n","  Resolved https://github.com/rusty1s/pytorch_geometric.git to commit e9648df16dcb6dde0e09b5736b1b2da5d68db2ad\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch-geometric==2.6.0) (3.9.5)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch-geometric==2.6.0) (2023.6.0)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch-geometric==2.6.0) (3.1.4)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch-geometric==2.6.0) (1.25.2)\n","Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch-geometric==2.6.0) (5.9.5)\n","Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch-geometric==2.6.0) (3.1.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch-geometric==2.6.0) (2.31.0)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch-geometric==2.6.0) (1.2.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-geometric==2.6.0) (1.11.4)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch-geometric==2.6.0) (4.66.4)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric==2.6.0) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric==2.6.0) (23.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric==2.6.0) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric==2.6.0) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric==2.6.0) (1.9.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric==2.6.0) (4.0.3)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch-geometric==2.6.0) (2.1.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric==2.6.0) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric==2.6.0) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric==2.6.0) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric==2.6.0) (2024.2.2)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric==2.6.0) (1.4.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric==2.6.0) (3.5.0)\n"]}]},{"cell_type":"code","source":["import argparse\n","import os\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split\n","import numpy as np\n","from math import sqrt\n","from statistics import mean\n","import torch_geometric\n","from torch_geometric.datasets import TUDataset\n","import torch\n","import torch.nn as nn\n","from torch.nn.parameter import Parameter\n","from torch_geometric.nn import GCNConv\n","import torch.nn.functional as F\n","from torch.nn import Linear, ReLU, Sequential\n","from sklearn import metrics\n","from scipy.spatial.distance import hamming\n","import statistics\n","import pandas\n","import csv\n","from time import perf_counter\n","from torch_geometric.nn import GCNConv, global_mean_pool\n","from torch_geometric.loader import DataLoader\n","import torch_geometric.nn as gnn\n","from torch.autograd import graph\n","from typing import Any, Dict, Optional, Union\n","from IPython.core.display import deepcopy\n","from torch_geometric.nn import MessagePassing\n","import copy\n","from importlib import reload\n","import pickle\n","from sklearn.preprocessing import label_binarize\n","from tqdm import tqdm\n","from torch_geometric.data import Data, Batch, Dataset"],"metadata":{"id":"_ehFqoYDQM62"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"maNEuq_vQO3t","executionInfo":{"status":"ok","timestamp":1716205500168,"user_tz":-120,"elapsed":26329,"user":{"displayName":"Ehsan Mobaraki","userId":"06314958514195310048"}},"outputId":"86c7f29c-9b48-479e-b13f-c4acb274f0b2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["nci1_dataset = TUDataset(root='data/TUDataset', name='NCI1')"],"metadata":{"id":"W7wkuhITQUSw","executionInfo":{"status":"ok","timestamp":1716205503664,"user_tz":-120,"elapsed":3499,"user":{"displayName":"Ehsan Mobaraki","userId":"06314958514195310048"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"8a25ebcd-323f-4cac-9849-4cba91754846"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading https://www.chrsmrrs.com/graphkerneldatasets/NCI1.zip\n","Processing...\n","Done!\n"]}]},{"cell_type":"code","source":["Explainability_name = 'Run All Methods'\n","Task_name = 'Graph Classification'\n","checkpoint_directory_Classifier = \"/content/drive/My Drive/Explainability Methods/\" + str(Explainability_name) + \" on \" + str(Task_name) + \"/Model/model_classifier.pt\"\n","classifier_lr = 0.001\n","classifier_dropout = 0.1\n","classifier_weight_decay = 1e-6\n","classifier_bias = True\n","DataSet_name = \"NCI1\"\n","\n","#File_Name = Model_name + \" \" + Explainability_name + \" \" + Task_name + \" \" + DataSet_name + \" \""],"metadata":{"id":"_kB6dTaS01Tx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df = pandas.read_csv(\"/content/drive/My Drive/Explainability Methods/Train and Test Indexes on Graph Classification/Experimental Results/train_test_indexes_NCI1.csv\")\n","\n","read_training_list_indexes__ = df['Train Indexes']\n","read_test_list_indexes__ = df['Test Indexes']\n","read_test_list_indexes__ = read_test_list_indexes__.dropna()\n","read_test_list_indexes = []\n","read_training_list_indexes = []\n","for element in read_test_list_indexes__:\n","    read_test_list_indexes.append(int(element))\n","for element in read_training_list_indexes__:\n","    read_training_list_indexes.append(int(element))\n","\n","\n","print(read_training_list_indexes)\n","print(read_test_list_indexes)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Yt9T5zo9-O-7","executionInfo":{"status":"ok","timestamp":1716205504901,"user_tz":-120,"elapsed":1239,"user":{"displayName":"Ehsan Mobaraki","userId":"06314958514195310048"}},"outputId":"a629a03e-37d1-40a1-c3ba-96ba27d803d9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[1769, 2293, 1110, 3802, 1484, 3715, 3654, 2414, 1099, 48, 2863, 1024, 3580, 2363, 4084, 3557, 1975, 589, 807, 2228, 2346, 2743, 1425, 1883, 597, 727, 1357, 797, 3780, 2367, 1039, 1111, 1267, 3667, 608, 3539, 277, 3999, 3907, 3860, 2648, 3457, 702, 1417, 398, 3826, 2318, 1662, 162, 3046, 3817, 1302, 3963, 3400, 858, 2172, 462, 956, 1286, 976, 216, 4028, 3339, 3043, 3665, 3535, 550, 674, 1291, 3013, 3349, 3964, 2224, 1818, 2181, 1473, 96, 785, 3336, 1587, 2728, 1232, 1771, 1354, 3641, 4069, 2467, 3850, 2581, 3369, 2956, 3442, 2617, 3481, 2604, 3123, 291, 3548, 987, 2820, 3022, 1433, 1893, 3590, 1085, 1886, 2455, 2182, 214, 357, 3182, 1488, 3492, 1646, 3956, 3044, 1123, 1850, 3456, 2031, 3804, 3894, 3951, 3725, 2393, 1361, 59, 1197, 1486, 1126, 1912, 1787, 197, 3272, 1884, 4052, 2205, 3560, 53, 2977, 50, 2557, 1789, 2575, 3703, 2531, 3247, 535, 3622, 3403, 891, 449, 1896, 3923, 3613, 399, 1561, 1004, 1610, 121, 1822, 119, 699, 2849, 1377, 1276, 2256, 3282, 861, 2006, 497, 2194, 2725, 3196, 2796, 3786, 2814, 3323, 3095, 888, 2632, 381, 180, 1546, 425, 4036, 2246, 2787, 1149, 1398, 175, 782, 1577, 2413, 3839, 3460, 3448, 953, 2807, 1682, 1251, 1612, 635, 1047, 3583, 768, 846, 4073, 3796, 143, 3515, 193, 2987, 1639, 584, 409, 2689, 3787, 3579, 1103, 914, 1026, 2286, 2054, 1731, 1689, 2603, 4089, 3733, 3714, 1225, 2852, 205, 4015, 2924, 3184, 2457, 2919, 2577, 2993, 716, 1273, 1705, 3822, 729, 2871, 2748, 922, 3900, 2070, 3978, 3146, 1062, 2673, 936, 11, 2336, 220, 1821, 847, 2441, 2111, 606, 3985, 3394, 1593, 1346, 2580, 2465, 2179, 548, 1823, 1503, 1549, 3865, 3953, 1430, 2146, 3150, 2980, 743, 1307, 2765, 1865, 1010, 280, 3552, 269, 3630, 3824, 1048, 2237, 394, 385, 1764, 1958, 152, 288, 672, 995, 1685, 1079, 403, 3962, 2845, 1432, 2123, 3066, 2630, 2428, 275, 1312, 1045, 1934, 2633, 2662, 1504, 3174, 65, 1256, 977, 3521, 2124, 715, 2501, 3021, 223, 3597, 499, 908, 3255, 3469, 1930, 3076, 3395, 1095, 686, 3628, 2770, 2020, 843, 3734, 2912, 423, 1967, 2090, 2416, 2259, 3673, 1353, 559, 3355, 4090, 2444, 2962, 2039, 3493, 3563, 1028, 1341, 2570, 3698, 2033, 2353, 621, 1356, 800, 1620, 920, 3882, 3108, 2453, 1980, 844, 128, 1961, 3527, 47, 2265, 2310, 1638, 448, 2301, 2836, 1963, 2620, 668, 3303, 974, 86, 3856, 647, 641, 3397, 127, 204, 889, 1350, 2645, 1747, 349, 1386, 528, 2439, 1426, 901, 3015, 245, 2522, 2545, 3416, 1550, 2361, 706, 3065, 3269, 3692, 1536, 1525, 2167, 319, 1698, 302, 1222, 1939, 758, 1993, 132, 3624, 1396, 1263, 3312, 1613, 2425, 1767, 4061, 852, 1900, 750, 4033, 2300, 1092, 1727, 1947, 3441, 2715, 3390, 3005, 3838, 1551, 3487, 2598, 2337, 1649, 3405, 3386, 34, 3861, 215, 1002, 2307, 3364, 3884, 1494, 600, 3910, 64, 1489, 992, 3877, 735, 2481, 1899, 841, 138, 3719, 1897, 2696, 234, 1277, 1279, 1482, 1917, 391, 42, 3036, 491, 2781, 244, 3438, 379, 1333, 159, 3365, 3651, 3143, 4017, 3160, 1269, 1375, 955, 2105, 829, 1745, 677, 3026, 2422, 883, 935, 1852, 3086, 3084, 1915, 2243, 598, 1397, 3235, 1038, 3201, 3183, 993, 3993, 2323, 2036, 3298, 1922, 2097, 728, 4091, 2311, 1815, 2876, 516, 2156, 794, 2943, 203, 1792, 3584, 996, 185, 1541, 1477, 2682, 4024, 3512, 1446, 1081, 2494, 625, 98, 3463, 3516, 3608, 3863, 2108, 2065, 4, 3690, 2862, 122, 2403, 1687, 2822, 1795, 3751, 3700, 4096, 2811, 2933, 1772, 565, 3202, 949, 1928, 3852, 1133, 3404, 3862, 435, 3033, 1244, 3619, 918, 168, 1414, 2601, 2227, 963, 793, 1379, 2532, 171, 3595, 2572, 1074, 1583, 3383, 1106, 153, 2388, 929, 1223, 812, 267, 1101, 3335, 229, 713, 3285, 3300, 506, 145, 1112, 3547, 19, 3534, 2204, 3636, 578, 3275, 406, 2297, 2955, 1314, 3380, 2893, 3136, 3929, 1008, 1445, 952, 104, 326, 2313, 3301, 2296, 2492, 3711, 1906, 151, 91, 1389, 867, 27, 1206, 594, 3517, 465, 46, 3170, 137, 1340, 258, 224, 2830, 1973, 799, 2316, 3872, 2917, 2389, 3632, 2509, 2857, 3924, 1777, 1972, 3574, 1202, 2385, 1721, 3846, 334, 2721, 1929, 81, 776, 1691, 2731, 2462, 2816, 2232, 3060, 1653, 2920, 3940, 2665, 1148, 1171, 503, 2149, 1859, 417, 3407, 3682, 1676, 352, 654, 316, 1453, 2762, 950, 62, 1944, 3694, 2927, 833, 1025, 1254, 2975, 1252, 649, 576, 1001, 1065, 2421, 732, 3551, 1427, 687, 56, 2968, 367, 3321, 2473, 3051, 1807, 179, 3042, 3173, 1090, 643, 3129, 519, 1127, 1542, 893, 3645, 477, 1066, 1651, 3500, 2875, 3866, 1497, 3842, 1296, 2638, 2915, 2639, 2543, 3023, 2443, 2704, 634, 2454, 1868, 2769, 3587, 2371, 3784, 757, 2084, 4107, 3596, 3367, 3466, 1059, 2711, 3919, 2477, 3853, 1399, 1107, 1575, 3374, 1838, 2274, 3163, 3704, 2692, 1679, 3998, 3702, 3976, 520, 2690, 1393, 1183, 2624, 2407, 1322, 2254, 3200, 898, 3249, 1555, 131, 3370, 3879, 1034, 513, 3316, 1661, 3565, 809, 2018, 2380, 979, 2087, 2278, 2874, 1846, 1870, 3152, 626, 851, 514, 3478, 1212, 3178, 2699, 508, 1686, 2843, 2983, 3199, 731, 3137, 3600, 3468, 2641, 89, 3680, 2406, 4092, 505, 1072, 261, 1051, 2972, 3100, 2266, 1169, 1680, 453, 2584, 2285, 3274, 2609, 1952, 796, 2947, 3130, 85, 710, 1570, 2974, 1950, 632, 3729, 1237, 2461, 4066, 2909, 3798, 1268, 2585, 761, 2884, 1581, 1690, 3209, 3025, 441, 3519, 3867, 1250, 1303, 324, 1506, 2395, 407, 476, 3888, 2106, 325, 2907, 2223, 2215, 2341, 293, 3016, 3398, 3753, 474, 2134, 2841, 1858, 502, 1650, 2805, 2535, 1924, 3994, 1384, 515, 412, 3228, 3399, 3524, 663, 3259, 3290, 2702, 3883, 838, 2189, 2842, 256, 4086, 322, 2076, 38, 661, 1218, 599, 1652, 2427, 3788, 1481, 1617, 644, 1987, 3544, 3732, 3815, 1642, 1021, 276, 82, 3717, 1841, 3373, 697, 777, 1067, 243, 1869, 390, 586, 3032, 1744, 2551, 1323, 2340, 3208, 3258, 278, 4106, 2019, 3348, 533, 885, 3112, 2571, 1748, 1715, 1532, 1321, 2544, 3873, 1720, 1465, 3351, 2606, 1137, 1082, 2622, 1538, 452, 2764, 260, 3647, 2526, 1803, 1758, 2643, 2644, 990, 4011, 2755, 1936, 4002, 3504, 364, 1290, 146, 2818, 217, 3476, 3592, 1125, 3191, 2279, 2133, 213, 2426, 819, 340, 4006, 3555, 1009, 2759, 21, 1556, 3705, 192, 285, 463, 2996, 2823, 3473, 681, 1310, 1597, 2667, 3047, 413, 333, 1672, 1564, 737, 3388, 1176, 1308, 3841, 1219, 2486, 3927, 436, 2881, 4074, 3440, 1568, 1535, 3825, 501, 2719, 1006, 1633, 1779, 3695, 249, 2651, 351, 2083, 2208, 763, 2496, 527, 2954, 3443, 3417, 1740, 1843, 1156, 2844, 3965, 2815, 3393, 3614, 2063, 2883, 3938, 112, 3874, 1938, 437, 3382, 1988, 534, 468, 3615, 4099, 496, 2027, 2051, 341, 2469, 2959, 2594, 3408, 1760, 722, 3772, 3922, 719, 3752, 1837, 3509, 2348, 3260, 1185, 3283, 3554, 1187, 3111, 766, 263, 1531, 2649, 656, 1770, 208, 2890, 1816, 3827, 2326, 804, 3612, 3350, 2969, 2474, 3085, 3778, 2891, 3119, 2038, 2088, 1675, 2678, 2419, 3623, 3002, 1983, 2472, 2963, 1131, 3284, 149, 3774, 4030, 4078, 1943, 2352, 58, 2666, 1600, 257, 1797, 2449, 368, 3775, 2436, 602, 3061, 3422, 2886, 2077, 1828, 1196, 3421, 688, 2127, 3230, 3916, 1618, 108, 4080, 3713, 620, 251, 1630, 3969, 3586, 1455, 1409, 3313, 3837, 1364, 287, 4070, 3000, 637, 2916, 1778, 3738, 2991, 3655, 3970, 3507, 1029, 2687, 1937, 332, 3030, 458, 1706, 169, 461, 1678, 3914, 3266, 2491, 2345, 1052, 2600, 1885, 2879, 3381, 3921, 4023, 3057, 161, 933, 614, 3167, 2530, 445, 1150, 2898, 4053, 97, 2631, 3017, 1616, 3785, 2244, 2847, 456, 2720, 3724, 1925, 3681, 1124, 3770, 905, 2432, 1220, 1522, 2724, 2655, 3357, 2714, 1974, 3375, 2355, 2829, 2853, 3354, 3449, 2549, 835, 3955, 1752, 3392, 1359, 1692, 2589, 2382, 4059, 1416, 4046, 2429, 1520, 494, 2410, 2877, 781, 1591, 615, 712, 1471, 2674, 489, 2560, 147, 2099, 3318, 3234, 3122, 4013, 2634, 3578, 967, 2686, 2478, 347, 1339, 1493, 2952, 1261, 1845, 2637, 3093, 536, 3320, 1960, 2249, 3250, 2889, 2904, 1204, 3245, 1857, 2404, 3144, 3177, 783, 3484, 3803, 1499, 1615, 2118, 2683, 2626, 3133, 902, 3771, 2636, 1448, 4055, 3211, 1729, 2653, 3317, 1086, 887, 2236, 1154, 4068, 2671, 874, 342, 1315, 2303, 1889, 3718, 3168, 1985, 636, 738, 1146, 3220, 2817, 3151, 3981, 3117, 1808, 3067, 2239, 2563, 2451, 3106, 3331, 2165, 1478, 1968, 3687, 493, 1167, 1800, 3588, 4010, 3765, 771, 2489, 2925, 265, 4014, 938, 2151, 3131, 1480, 1420, 604, 2284, 3293, 3192, 2798, 3522, 2812, 2976, 3029, 680, 2423, 1444, 707, 2147, 658, 2973, 1132, 3792, 2612, 3307, 3385, 2906, 3966, 3286, 166, 160, 1627, 2190, 3001, 1440, 3311, 1265, 1782, 2255, 80, 555, 2523, 1814, 2534, 537, 509, 2184, 431, 39, 225, 2793, 3701, 30, 566, 2468, 2261, 1164, 971, 1076, 102, 3140, 2902, 2663, 1946, 480, 2497, 2498, 1138, 3240, 1901, 3989, 2561, 44, 344, 2411, 970, 338, 3429, 2101, 315, 2618, 3947, 824, 1236, 524, 298, 376, 3851, 860, 117, 2951, 155, 170, 1226, 1964, 2046, 2034, 395, 2937, 1829, 4079, 629, 2288, 1200, 2085, 1791, 577, 188, 3128, 3764, 2043, 469, 541, 3648, 2652, 1765, 1313, 3222, 1246, 2727, 973, 2744, 228, 235, 3634, 1534, 790, 764, 988, 837, 3831, 3881, 3620, 2485, 3541, 1840, 184, 4095, 3810, 2271, 3819, 54, 2574, 3756, 1469, 3812, 173, 1434, 3594, 2196, 1882, 3603, 3010, 2206, 2948, 886, 2379, 126, 3432, 733, 3332, 2159, 3576, 191, 3885, 734, 660, 1435, 3281, 3932, 2358, 1368, 3377, 1592, 1049, 426, 410, 1155, 3536, 13, 311, 2621, 3757, 568, 71, 631, 3959, 1580, 1439, 1152, 2012, 1867, 3816, 1274, 1030, 1502, 1878, 3640, 472, 1170, 1351, 3486, 1451, 3660, 4094, 4038, 380, 1404, 3606, 3414, 1833, 2339, 129, 4102, 3135, 3289, 427, 2488, 1301, 2080, 2650, 2061, 2321, 721, 642, 374, 1631, 682, 2059, 2067, 3310, 2833, 2747, 3523, 3342, 3925, 2139, 2866, 1177, 624, 1371, 2806, 563, 3818, 1523, 4067, 3068, 1888, 3950, 1253, 723, 2553, 3878, 1553, 454, 2960, 2885, 1526, 237, 3823, 1216, 1704, 3903, 2201, 1325, 1806, 3434, 553, 3080, 3041, 2538, 401, 3893, 471, 1887, 3176, 521, 3221, 3906, 3508, 1441, 1735, 978, 2383, 622, 3297, 14, 1358, 552, 3197, 1363, 3304, 181, 3028, 2706, 2026, 538, 1174, 3821, 2559, 3204, 4058, 297, 3532, 1919, 3420, 1969, 3501, 2635, 3808, 2330, 26, 1916, 2095, 3892, 3864, 2684, 67, 318, 3979, 1545, 579, 3811, 1595, 842, 4009, 1070, 144, 623, 2736, 2275, 1830, 1483, 3664, 3658, 3598, 1598, 1160, 1068, 2153, 2500, 1628, 3502, 2524, 2712, 2220, 253, 2282, 3214, 3843, 2233, 2150, 330, 1023, 1411, 1238, 2145, 1543, 1759, 2710, 2688, 2459, 2935, 165, 457, 1671, 3378, 114, 2250, 363, 2661, 1560, 3004, 2211, 3233, 3430, 3356, 2729, 2640, 3139, 1957, 226, 2490, 3943, 1572, 1419, 1407, 1109, 1911, 695, 2801, 1069, 283, 383, 3844, 16, 940, 3490, 4082, 3402, 68, 4027, 2212, 666, 998, 1848, 630, 1824, 1492, 4035, 845, 2360, 348, 4031, 1369, 2855, 3104, 1073, 582, 2896, 1825, 821, 1116, 1666, 3267, 1391, 1942, 3935, 3124, 507, 3338, 2957, 4048, 3789, 986, 2946, 1450, 1533, 1466, 3186, 1629, 3121, 840, 1234, 353, 439, 422, 3768, 2878, 745, 196, 4101, 3213, 966, 201, 3187, 356, 2004, 2450, 2269, 2777, 2292, 919, 664, 639, 3674, 2433, 569, 813, 28, 164, 4047, 4008, 139, 3019, 3876, 853, 2132, 2378, 3697, 1738, 1635, 3834, 2359, 1383, 1288, 881, 60, 1189, 2958, 1221, 1668, 1810, 5, 558, 339, 369, 2365, 1517, 485, 831, 2096, 55, 2810, 221, 3324, 1304, 3359, 575, 1722, 12, 2705, 2595, 440, 1040, 1907, 703, 1061, 3506, 1724, 926, 4057, 3037, 1461, 2464, 292, 187, 2774, 18, 551, 1228, 3431, 2726, 1056, 2176, 3688, 3387, 3986, 1518, 2517, 1470, 1324, 2888, 2399, 2730, 2778, 17, 2614, 299, 3629, 1751, 2858, 684, 951, 1634, 1733, 3869, 3971, 1331, 3567, 3048, 767, 3561, 2035, 2247, 2178, 939, 2554, 2008, 3458, 1530, 1835, 3268, 1707, 1640, 3056, 657, 765, 2424, 815, 869, 1914, 1400, 3538, 1031, 2691, 393, 540, 755, 2171, 3644, 3602, 3226, 4049, 142, 2676, 1609, 2767, 3791, 2756, 1388, 1168, 1515, 4093, 2387, 2757, 2990, 2548, 3913, 1529, 730, 1648, 1280, 2819, 1443, 3246, 1842, 2202, 1360, 3909, 2642, 2647, 3854, 2374, 3930, 2044, 2613, 3848, 3156, 2901, 1866, 1231, 1485, 1284, 2163, 1392, 2062, 2408, 1696, 2588, 1552, 3915, 2390, 3360, 1558, 3807, 3550, 1596, 2314, 2565, 1667, 1608, 101, 1192, 2709, 3639, 3832, 2177, 1367, 2400, 157, 3795, 45, 4019, 3649, 725, 1019, 607, 392, 1736, 3868, 3957, 210, 2082, 418, 1449, 2804, 1108, 36, 3609, 1694, 2160, 3154, 2735, 787, 1805, 1362, 742, 397, 3813, 2610, 2102, 1193, 1507, 3542, 408, 3593, 899, 2926, 947, 530, 3510, 2170, 2799, 3276, 289, 1462, 2415, 692, 3960, 3581, 1992, 2809, 202, 3996, 3761, 3847, 3396, 1015, 1374, 194, 870, 294, 957, 3845, 238, 1498, 2679, 2773, 2386, 4056, 1817, 2119, 2009, 1776, 2519, 2470, 1898, 2332, 3353, 3779, 2656, 2550, 3758, 1997, 1890, 1872, 1191, 359, 1732, 3287, 3083, 1569, 2372, 3480, 2599, 2627, 2831, 4022, 2180, 3743, 24, 1209, 3346, 2936, 3767, 2556, 653, 539, 3546, 2003, 1472, 593, 3720, 1091, 2381, 1521, 811, 482, 1413, 567, 61, 1151, 3766, 2512, 1755, 1786, 2325, 3126, 2193, 1338, 3776, 2042, 3710, 3570, 1382, 2312, 3018, 542, 1582, 3568, 3889, 2350, 1463, 4001, 3147, 1017, 1145, 2258, 2540, 459, 1203, 1871, 1336, 2539, 3314, 798, 350, 271, 1055, 3341, 3575, 2562, 123, 2850, 2939, 1783, 2057, 3577, 3363, 1235, 1475, 2950, 700, 1248, 1057, 2320, 1190, 932, 3708, 1293, 3474, 2911, 1996, 2448, 1659, 2049, 2586, 1594, 2168, 75, 1576, 1115, 2072, 3984, 1719, 1337, 2161, 3116, 1098, 1270, 3072, 2795, 2897, 295, 2391, 2052, 1965, 3003, 2658, 3562, 3689, 2681, 268, 3830, 3141, 120, 2334, 1632, 792, 3553, 3039, 810, 1799, 4088, 3031, 396, 3371, 1761, 3410, 1297, 3908, 212, 100, 2241, 1405, 814, 3750, 2797, 2669, 1895, 495, 3120, 2698, 177, 3809, 3736, 1656, 3611, 3166, 892, 2092, 791, 1163, 2010, 3526, 2608, 2737, 1100, 1456, 307, 2143, 2802, 2242, 2333, 1910, 708, 4063, 3540, 3291, 959, 1904, 561, 464, 2058, 3975, 442, 3127, 1381, 1044, 2169, 2078, 1962, 1903, 478, 1785, 2335, 3491, 2214, 1050, 1053, 1643, 1604, 3891, 3063, 1257, 1157, 2864, 3325, 2516, 1058, 795, 3977, 1215, 2605, 627, 3762, 1159, 2768, 1953, 694, 4043, 2800, 282, 2934, 248, 801, 500, 1120, 655, 2308, 2342, 3635, 4018, 1626, 1114, 1158, 198, 1054, 2732, 2110, 4032, 1599, 3452, 3428, 1165, 1811, 3206, 825, 1428, 3783, 3829, 3241, 2369, 1982, 2209, 1511, 1516, 3050, 1827, 1442, 1342, 3511, 2910, 3503, 2611, 219, 3190, 1637, 1874, 1344, 605, 388, 808, 296, 4098, 3172, 2409, 2882, 254, 611, 2745, 517, 1467, 2693, 4109, 904, 163, 3427, 3081, 66, 1932, 1153, 1688, 1820, 70, 2148, 274, 110, 1084, 3090, 2115, 2235, 3637, 2328, 446, 1891, 1567, 2675, 3712, 925, 4097, 1573, 618, 1739, 3075, 3880, 1660, 83, 3739, 2248, 720, 43, 1693, 475, 3333, 6, 1894, 3898, 387, 2287, 1875, 827, 1994, 2913, 1711, 3205, 981, 443, 1763, 2865, 4054, 930, 3294, 667, 378, 2126, 921, 3858, 3376, 1349, 266, 230, 512, 432, 2949, 240, 1508, 3661, 3227, 2476, 592, 903, 3034, 3256, 985, 1684, 726, 3912, 470, 1654, 1000, 233, 2048, 1999, 2195, 1118, 2672, 2779, 1173, 308, 450, 1717, 1589, 3754, 3455, 3709, 23, 778, 2064, 3735, 1372, 2138, 2226, 3142, 1418, 2140, 2370, 3309, 1624, 3243, 421, 1926, 3737, 2191, 2536, 1011, 3252, 1793, 1880, 546, 2733, 991, 252, 3238, 2495, 3543, 1080, 3968, 2056, 3465, 2905, 3814, 2839, 2596, 770, 3958, 3114, 2808, 2992, 2995, 583, 3231, 460, 2602, 3224, 868, 3454, 4037, 3676, 2761, 2930, 92, 3251, 3961, 3650, 2547, 3828, 3302, 1063, 2821, 1677, 3219, 2480, 2, 2870, 1129, 74, 301, 1121, 1750, 2417, 2504, 1500, 3946, 880, 2989, 740, 99, 789, 3801, 2267, 2471, 616, 2880, 1447, 1955, 3840, 780, 806, 118, 1565, 3939, 113, 3225, 1097, 696, 3098, 3643, 2068, 3505, 2487, 498, 154, 2994, 1328, 361, 2166, 2518, 1188, 3616, 2136, 1117, 2158, 3604, 1514, 3277, 3887, 2117, 958, 2510, 511, 1527, 1365, 404, 2185, 2069, 2783, 2304, 633, 1147, 246, 2234, 924, 997, 1454, 1242, 2828, 3467, 1809, 1281, 1335, 2338, 877, 4026, 2981, 31, 3781, 601, 1853, 1043, 596, 670, 1394, 2245, 1491, 2135, 2592, 1438, 1933, 1172, 2785, 759, 1849, 736, 3328, 788, 820, 2752, 866, 3836, 2024, 1766, 1387, 2929, 2985, 2999, 3686, 3498, 3564, 1657, 2298, 3904, 3471, 323, 3216, 7, 1864, 182, 2362, 1143, 329, 968, 8, 1863, 1851, 1979, 2331, 3901, 1927, 3089, 1275, 2066, 2597, 3607, 3218, 3585, 711, 255, 1695, 317, 2856, 2430, 3773, 189, 29, 305, 239, 784, 2283, 402, 943, 2466, 1390, 377, 2593, 2928, 1311, 1326, 1584, 1214, 3797, 1703, 3652, 2892, 2887, 2739, 1300, 1271, 3058, 1831, 84, 2112, 557, 2685, 279, 1078, 2002, 1756, 1182, 1726, 250, 1813, 911, 63, 3558, 3008, 3436, 3760, 3859, 748, 400, 1089, 3627, 232, 549, 69, 617, 3069, 2418, 2749, 4108, 2047, 3610, 830, 88, 3638, 405, 259, 832, 1801, 945, 3559, 1140, 2979, 1647, 372, 1022, 4034, 3446, 2738, 912, 2050, 1014, 4041, 354, 3633, 2493, 2103, 2030, 1460, 2377, 572, 2566, 3625, 2447, 673, 2965, 3435, 360, 167, 2646, 3038, 3279, 1096, 3345, 22, 1601, 2629, 3482, 483, 562, 2760, 2120, 2079, 3514, 1712, 3078, 504, 3495, 2582, 3423, 2953, 1141, 3088, 1775, 705, 1718, 3769, 1298, 2742, 856, 1476, 2938, 2707, 3159, 4060, 2741, 2219, 183, 510, 2164, 1548, 3833, 775, 1305, 2142, 1287, 1436, 701, 671, 107, 3731, 3744, 2722, 3472, 343, 2188, 2086, 3799, 2578, 3263, 1241, 2075, 1376, 2200, 1784, 1989, 2573, 3305, 2253, 1970, 645, 3726, 612, 1410, 1198, 3871, 3793, 3948, 574, 1723, 704, 284, 4072, 2141, 4004, 965, 3693, 2521, 3520, 2513, 3450, 1402, 2104, 2945, 3179, 3145, 648, 1134, 87, 3330, 1762, 1908, 106, 2162, 2025, 1102, 3099, 585, 3513, 1701, 1892, 2032, 1013, 3175, 3572, 1161, 1645, 25, 1033, 3343, 3937, 3954, 336, 1366, 2854, 1139, 419, 3162, 4007, 2013, 2306, 1780, 2347, 2089, 2567, 871, 762, 2932, 2357, 2587, 3569, 303, 2654, 1804, 570, 1879, 484, 3270, 370, 1406, 3531, 749, 1945, 4065, 1918, 2700, 839, 961, 3684, 1681, 2750, 2144, 335, 774, 1826, 3755, 3188, 2718, 2028, 290, 1458, 3389, 4087, 3897, 2499, 312, 1728, 573, 1528, 1948, 4012, 2514, 2434, 2552, 3189, 2405, 2971, 300, 773, 2029, 2754, 3462, 865, 4021, 3077, 4062, 2716, 946, 779, 270, 2942, 709, 3415, 3198, 2198, 1184, 1431, 523, 718, 4040, 3210, 544, 1931, 1505, 2055, 3488, 1981, 156, 651, 1105, 3659, 2210, 2354, 941, 3153, 490, 753, 2445, 3165, 2895, 3571, 2315, 264, 3911, 1923, 756, 698, 3683, 2397, 3995, 3052, 79, 2723, 2623, 313, 1162, 2619, 3642, 1746, 3886, 3727, 429, 2483, 1991, 2238, 1708, 879, 3573, 595, 1343, 816, 590, 3464, 3319, 358, 1623, 3800, 3662, 2094, 3340, 3253, 2628, 420, 2343, 4025, 3020, 3193, 628, 818, 746, 35, 650, 1768, 1655, 2225, 3530, 3237, 2508, 2394, 2940, 3092, 2412, 3194, 1060, 3653, 2734, 2772, 805, 2327, 1607, 1749, 444, 1135, 1674, 878, 1278, 2155, 176, 3265, 195, 1142, 928, 3582, 962, 3794, 3677, 2252, 1513, 2203, 3308, 1012, 1020, 3372, 4029, 972, 2616, 3997, 2113, 438, 2541, 3280, 3358, 2351, 3646, 1403, 3905, 3617, 744, 3212, 897, 2396, 3053, 2660, 3489, 2349, 2677, 3007, 1876, 3232, 2708, 1327, 1644, 3980, 760, 1239, 3059, 3079, 2122, 969, 2921, 3413, 556, 907, 3591, 3790, 890, 2045, 486, 1027, 964, 103, 1249, 1976, 2537, 10, 1590, 3699, 93, 2270, 3329, 2872, 3941, 1495, 174, 3011, 3949, 414, 980, 3835, 447, 975, 3035, 140, 306, 3244, 2835, 2824, 2827, 1794, 751, 3707, 3366, 2782, 2281, 178, 640, 2005, 1949, 3545, 1753, 1605, 1046, 321, 134, 105, 222, 2199, 200, 2776, 545, 1986, 2364, 2290, 492, 231, 1920, 2073, 1258, 2221, 2260, 1554, 822, 2859, 2435, 337, 3155, 2344, 613, 1839, 906, 2502, 416, 2356, 1347, 488, 944, 817, 2376, 190, 1217, 1563, 2961, 1136, 3264, 580, 3118, 1479, 2309, 872, 2402, 2997, 2128, 518, 2503]\n","[1544, 2456, 2525, 2840, 1378, 1714, 3678, 209, 2116, 3070, 2452, 150, 3327, 3295, 1064, 691, 1395, 3362, 896, 3671, 382, 2121, 2186, 2294, 4081, 3185, 455, 1940, 2437, 487, 2546, 2900, 40, 3967, 1283, 3933, 1207, 1032, 2789, 983, 49, 1292, 3215, 1699, 826, 1790, 473, 2873, 2130, 227, 2825, 2230, 1971, 3439, 3679, 2659, 3138, 3132, 2869, 2231, 320, 109, 3740, 389, 3361, 1834, 2529, 1977, 1016, 3352, 479, 3973, 2964, 1832, 554, 3759, 2291, 3062, 3902, 3433, 859, 916, 1229, 2268, 3109, 3164, 1700, 1319, 741, 3855, 2131, 373, 2515, 136, 433, 2093, 2129, 1037, 1429, 415, 999, 2442, 1877, 41, 1459, 3670, 2431, 3470, 2375, 2867, 3161, 1452, 3125, 3896, 4000, 3027, 37, 581, 934, 638, 2668, 1295, 529, 1464, 1956, 4045, 1737, 1144, 2832, 1316, 3409, 2834, 3931, 371, 3306, 2482, 2100, 662, 304, 3278, 1457, 3148, 989, 135, 2775, 1585, 2368, 1083, 587, 1774, 1812, 2758, 3379, 1294, 78, 4085, 1211, 2187, 2157, 1487, 2173, 386, 1566, 2458, 849, 836, 130, 3730, 309, 917, 15, 1186, 2479, 3453, 1658, 1788, 3064, 2484, 3485, 1802, 3134, 1289, 2174, 77, 2780, 141, 2670, 218, 346, 1309, 3745, 466, 2694, 3763, 2305, 1665, 1036, 3105, 1035, 3556, 786, 1208, 2846, 894, 3626, 1913, 1669, 1697, 3746, 3875, 3009, 1195, 3668, 434, 2273, 3991, 1557, 242, 3315, 2607, 1873, 2507, 603, 1855, 685, 1230, 3262, 2175, 1959, 3723, 1317, 4003, 2373, 927, 1088, 909, 803, 1501, 1181, 2564, 4100, 1861, 3006, 2868, 3657, 2183, 2071, 355, 1966, 1586, 4077, 236, 33, 365, 2438, 2791, 1179, 2918, 931, 3236, 1071, 2986, 1836, 1266, 3101, 3103, 873, 2216, 51, 857, 2037, 3248, 1524, 3074, 3171, 1741, 3685, 850, 2848, 3223, 1352, 3618, 2664, 923, 3040, 3426, 1178, 4051, 1119, 32, 2923, 854, 3113, 481, 3203, 652, 646, 4050, 3217, 1622, 3115, 855, 1571, 802, 2695, 2392, 910, 3549, 3656, 2908, 1247, 4020, 1415, 314, 1205, 669, 9, 3945, 3944, 2717, 4105, 4042, 90, 3589, 3257, 2970, 3749, 1998, 2625, 2277, 3406, 3696, 3157, 1334, 1213, 328, 1329, 862, 4103, 1282, 547, 1243, 1262, 3870, 1984, 3424, 665, 411, 1621, 1345, 2014, 3337, 610, 1355, 3741, 3926, 689, 3566, 262, 1990, 2861, 3082, 158, 3496, 1619, 3229, 3411, 2697, 2280, 2011, 3097, 3952, 3149, 1224, 2894, 1625, 2931, 2091, 1860, 2763, 2222, 2440, 133, 0, 2922, 1113, 1602, 3437, 52, 3483, 3920, 3934, 2022, 331, 3451, 769, 3110, 2978, 3890, 937, 2520, 747, 2568, 1935, 2366, 186, 3936, 327, 2021, 864, 3533, 1128, 3849, 3073, 1199, 2583, 1509, 1490, 772, 2017, 1423, 2505, 2016, 3820, 1710, 1299, 273, 915, 984, 1474, 1087, 3180, 272, 2527, 1510, 3777, 3384, 2982, 2786, 3091, 1603, 3899, 1166, 2324, 3675, 148, 1559, 2826, 3296, 1978, 1259, 2264, 1574, 3722, 1042, 3024, 115, 591, 2533, 2463, 1606, 900, 1255, 2295, 3254, 2137, 3181, 3477, 1007, 1233, 3972, 1862, 1670, 1954, 3169, 2701, 3107, 1401, 375, 675, 2272, 3158, 3347, 1245, 2555, 2053, 3102, 2941, 4083, 310, 1578, 560, 1847, 1018, 1272, 2558, 1547, 2041, 2152, 1421, 714, 2218, 1512, 1104, 1077, 1702, 2074, 3605, 451, 1318, 3497, 1180, 2217, 2262, 4039, 2207, 4075, 2475, 2590, 1320, 3271, 1664, 1122, 345, 3459, 172, 2023, 2040, 281, 1408, 2299, 211, 1798, 2251, 3273, 2803, 3917, 2899, 1995, 531, 2771, 659, 2966, 1713, 2114, 1641, 1757, 3805, 76, 2197, 3748, 3096, 1614, 206, 3391, 2967, 1709, 1844, 1611, 207, 678, 2579, 1773, 1683, 3242, 1264, 1743, 1856, 1781, 1075, 3857, 2615, 3631, 2576, 1130, 2657, 2753, 1951, 4005, 4071, 3261, 3207, 875, 3806, 3322, 2098, 2125, 1175, 57, 3742, 2081, 1562, 954, 72, 2746, 571, 1716, 1306, 3942, 1754, 73, 2998, 3599, 683, 3054, 4044, 522, 3928, 1380, 3782, 4076, 3747, 2001, 1424, 2192, 2914, 3344, 532, 2154, 1496, 3974, 3621, 247, 3045, 125, 2401, 3326, 384, 2000, 2398, 2276, 3, 752, 884, 2740, 3479, 3299, 1003, 2766, 3445, 1370, 526, 1373, 848, 1905, 913, 3401, 3672, 1332, 116, 20, 199, 3049, 3529, 754, 1194, 2903, 424, 2569, 676, 3895, 3094, 1796, 739, 1636, 366, 3288, 4064, 286, 3691, 3419, 1227, 3728, 2713, 2109, 2240, 1422, 3475, 690, 1437, 724, 3663, 619, 823, 2790, 2837, 3368, 1260, 1005, 2302, 1819, 3412, 834, 1, 942, 362, 2384, 2460, 1537, 1921, 2792, 2060, 679, 1519, 4016, 3716, 95, 1742, 3014, 2322, 2213, 1579, 1588, 1902, 2329, 3982, 2506, 1725, 2838, 3706, 3518, 2289, 3425, 948, 693, 428, 2813, 2680, 3334, 3669, 3525, 863, 2229, 3071, 3292, 3666, 111, 3418, 609, 2107, 3983, 3992, 1468, 1734, 2263, 467, 2446, 3988, 2984, 1210, 1539, 3494, 3239, 2319, 2257, 1093, 876, 828, 1201, 2788, 895, 994, 2851, 124, 1240, 1330, 3537, 2784, 3195, 2015, 1881, 3447, 430, 3990, 543, 1730, 2988, 2860, 2591, 4104, 3012, 1663, 2007, 3499, 1854, 3601, 1540, 564, 3987, 2420, 2542, 1941, 1385, 1673, 1285, 2751, 1909, 3528, 2528, 2317, 525, 588, 3721, 1094, 94, 3087, 2511, 241, 3444, 717, 982, 960, 3461, 1412, 1348, 3918, 2794, 3055, 882, 2944, 1041, 2703]\n"]}]},{"cell_type":"code","source":["#train_dataset, test_dataset = train_test_split(dataset, test_size=0.3, random_state=0, shuffle=True)\n","#print(\"Number of Training Graphs: \", len(train_dataset))\n","#print(\"Number of Test Graphs: \", len(test_dataset))\n","\n","nci1_train_dataset = []\n","nci1_test_dataset = []\n","for index in read_training_list_indexes:\n","    nci1_train_dataset.append(nci1_dataset[index])\n","for index in read_test_list_indexes:\n","    nci1_test_dataset.append(nci1_dataset[index])\n","\n","\n","print(f'Number of training graphs: {len(nci1_train_dataset)}')\n","print(f'Number of test graphs: {len(nci1_test_dataset)}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"z03xSW56QXBg","executionInfo":{"status":"ok","timestamp":1716205505176,"user_tz":-120,"elapsed":276,"user":{"displayName":"Ehsan Mobaraki","userId":"06314958514195310048"}},"outputId":"878aeebf-2dac-41bb-d2f5-e5a4581dfa23"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of training graphs: 3288\n","Number of test graphs: 822\n"]}]},{"cell_type":"code","source":["print(nci1_train_dataset[0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-I7GGLROwfQo","executionInfo":{"status":"ok","timestamp":1716205505176,"user_tz":-120,"elapsed":3,"user":{"displayName":"Ehsan Mobaraki","userId":"06314958514195310048"}},"outputId":"714b87b8-a290-4fd0-9525-bd0029262353"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Data(edge_index=[2, 54], x=[25, 37], y=[1])\n"]}]},{"cell_type":"code","source":["BATCH_SIZE = 64\n","nci1_train_dataloader = DataLoader(nci1_train_dataset, batch_size=BATCH_SIZE, shuffle=False) # important to be false\n","nci1_test_dataloader = DataLoader(nci1_test_dataset, batch_size=1, shuffle=False)"],"metadata":{"id":"0GvOiXOCQcCw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(nci1_train_dataloader.batch_size)\n","batch = next(iter(nci1_train_dataloader))\n","print(batch.y)\n","print(len(nci1_train_dataloader))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"i_foPNxLQeth","executionInfo":{"status":"ok","timestamp":1716205534716,"user_tz":-120,"elapsed":264,"user":{"displayName":"Ehsan Mobaraki","userId":"06314958514195310048"}},"outputId":"41081bef-3c21-4990-ae6a-9c851015769c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["64\n","tensor([1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1,\n","        0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1,\n","        0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1])\n","52\n"]}]},{"cell_type":"code","source":["# from torch_geometric.datasets import FakeDataset\n","\n","# num_graphs = 100\n","# avg_num_nodes = 20\n","# avg_degree = 15\n","# node_feat_dim = 13\n","# edge_feat_dim = 0\n","# num_classes = 3\n","\n","# fake_dataset = FakeDataset(num_graphs=num_graphs, avg_num_nodes=avg_num_nodes, avg_degree=avg_degree, num_channels=node_feat_dim,\n","#                            edge_dim=edge_feat_dim, num_classes=num_classes,)\n","# Explainability_name = 'PGExplainer'\n","# Task_name = 'Graph Classification'\n","# classifier_bias = True\n","# DataSet_name = \"Fake\"\n","# BATCH_SIZE = 10\n","# classifier_lr = 0.001\n","# classifier_dropout = 0.1\n","# classifier_weight_decay = 1e-6\n","\n","# train_ratio = 0.8\n","\n","# fake_train_dataset = fake_dataset[:int(len(fake_dataset) * train_ratio)]\n","# fake_test_dataset = fake_dataset[len(fake_dataset) - int(len(fake_dataset) * train_ratio):]\n","\n","# fake_train_dataloader = DataLoader(fake_train_dataset, batch_size=BATCH_SIZE, shuffle=False)\n","# fake_test_dataloader = DataLoader(fake_test_dataset, batch_size=BATCH_SIZE, shuffle=False)"],"metadata":{"id":"vgt09LzvZEtW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["    #   node_feat_size = self.input_dataset[0].x.size()[-1]\n","    #   if self.GNN_Model_name == \"GCN_plus_GAP\":\n","    #     return gcn_plus_gap_model.GCN_plus_GAP(model_name=\"GCN_plus_GAP\", act_fun=act_fun, model_level=\"graph\", input_dim=node_feat_size, hidden_dim=hidden_dim, output_dim=self.num_classes, num_hid_layers=3, Bias=Bias, Weight_Initializer=winit, dropout_rate=dropout_rate)\n","    #   elif self.GNN_Model_name == 'DGCNN':\n","    #     k = 17\n","    #     if self.dataset_name == 'NCI1' or self.dataset_name == 'ENZYMES':\n","    #       k = 32\n","    #     elif self.dataset_name == 'Graph-SST5':\n","    #       k = 19\n","\n","    #     return dgcnn_model.DGCNN_Model(GNN_layers=[32, 32, 32, 13], num_classes=self.num_classes, mlp_act_fun=\"ReLu\", dgcnn_act_fun=\"tanh\", mlp_dropout_rate=dropout_rate, Weight_Initializer=winit, Bias=Bias, dgcnn_k=k, node_feat_size=node_feat_size, hid_channels=[128,128], conv1d_kernels=[2,5], ffn_layer_size=128, strides=[2,1])\n","\n","    #   elif self.GNN_Model_name == 'DIFFPOOL':\n","    #     return diffpool_model.DIFFPOOL_Model(embedding_input_dim=node_feat_size, embedding_num_block_layers=1, embedding_hid_dim=64, new_feature_size=64, assignment_input_dim=node_feat_size,\n","    #                                   assignment_num_block_layers=1, assignment_hid_dim=64, max_number_of_nodes=256, prediction_hid_layers=[50], concat_neighborhood=False, num_classes=self.num_classes, Weight_Initializer=winit, Bias=Bias, dropout_rate=dropout_rate, normalize_graphsage=False, aggregation=\"mean\", act_fun=act_fun, concat_diffpools_outputs=True, num_pooling=1, pooling=\"mean\")\n","    #   elif self.GNN_Model_name == 'GIN':\n","    #     return gin_model.GIN_Model(num_mlp_layers=3, Bias=Bias, num_slp_layers=2, mlp_input_dim=node_feat_size, mlp_hid_dim=hidden_dim, mlp_output_dim=self.num_classes, mlp_act_fun=act_fun, dropout_rate=dropout_rate, Weight_Initializer=winit, joint_embeddings= None)"],"metadata":{"id":"1QYdWPMft_Pv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# NCI1,DGCNN,4 * tanh(DGCNN) + 2 * relu(Conv1d) + Sortpooling + 2 * relu(Linear),kaiming_normal_,5,1000,0.0001,0.3,True,32,CrossEntropyLoss\n","# NCI1,DIFFPOOL,2 * DIFFPOOL + 2 * GRAPHSAGE + 3 * Linear,xavier_normal_,2,1000,0.0001,0.3,True,32,CrossEntropyLoss\n","# NCI1,GCN_plus_GAP,ReLu(GCN1)+ReLu(GCN2)+GAP+FFN,xavier_normal_,1,1000,0.0001,0.3,True,32,CrossEntropyLoss\n","# NCI1,GIN,kaiming_normal_,5,200,0.001,0.3,True,32,CrossEntropyLoss"],"metadata":{"id":"-ZV0jO0IxA9u"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import sys\n","py_path = '/content/drive/MyDrive/Explainability Methods/Models/Script/'\n","sys.path.insert(0,py_path)\n","\n","import GCN_plus_GAP as Graph_Network\n","GNN_Model = Graph_Network.GCN_plus_GAP(model_name='GCN_plus_GAP', model_level='graph', input_dim=37, hidden_dim=128, output_dim=2,\n","                                       num_hid_layers=3, Bias=classifier_bias, act_fun='ReLu', Weight_Initializer=1,\n","                                       dropout_rate=0.3)\n","\n","# import DGCNN as dgcnn_model\n","# GNN_Model = dgcnn_model.DGCNN_Model(GNN_layers=[32, 32, 32, 7], num_classes=2, node_feat_size=7, mlp_act_fun='ReLu',\n","#                                    dgcnn_act_fun='tanh', mlp_dropout_rate=0.5, Weight_Initializer=3, Bias=False, dgcnn_k=17,\n","#                                    hid_channels=[16,32], conv1d_kernels=[2,5], ffn_layer_size=128, strides=[2,1])\n","\n","# import DIFFPOOL as diffpool_model\n","# GNN_Model = diffpool_model.DIFFPOOL_Model(embedding_input_dim=7, embedding_num_block_layers=1, embedding_hid_dim=64,\n","#                                          new_feature_size=64, assignment_input_dim=7, assignment_num_block_layers=1,\n","#                                          assignment_hid_dim=64, max_number_of_nodes=256, prediction_hid_layers=[50],\n","#                                          concat_neighborhood=False, num_classes=2, Weight_Initializer=1, Bias=classifier_bias,\n","#                                          dropout_rate=0, normalize_graphsage=False, aggregation=\"mean\", act_fun=\"ReLu\",\n","#                                          concat_diffpools_outputs=True, num_pooling=1, pooling=\"mean\")\n","\n","# import GIN as gin_model\n","# GNN_Model = gin_model.GIN_Model(num_mlp_layers=4, mlp_input_dim=37, mlp_hid_dim=37, mlp_output_dim=2, num_slp_layers=2,\n","#                                 Bias=classifier_bias, mlp_act_fun=\"ReLu\", dropout_rate=classifier_dropout, Weight_Initializer=1,\n","#                                 joint_embeddings=False)\n","\n","\n","\n","Model_name = GNN_Model.__class__.__name__"],"metadata":{"id":"D2W-jRaZzu2G","executionInfo":{"status":"ok","timestamp":1716206429878,"user_tz":-120,"elapsed":281,"user":{"displayName":"Ehsan Mobaraki","userId":"06314958514195310048"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"20edfe93-60ea-46e0-9e24-cf90d9f263c3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["GCN_plus_GAP Input_Dimension: 37\n","GCN_plus_GAP Hidden_Dimension: 128\n","GCN_plus_GAP Output_Dimension: 2\n","GCN_plus_GAP Number_of_Hidden_Layers: 3\n","ReLu is Selected.\n"]}]},{"cell_type":"code","source":["print(GNN_Model)"],"metadata":{"id":"K3DZ5nHbSkxN","executionInfo":{"status":"ok","timestamp":1716206432756,"user_tz":-120,"elapsed":548,"user":{"displayName":"Ehsan Mobaraki","userId":"06314958514195310048"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"8d9c81b7-633c-4680-8ada-7e101be50ed0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["GCN_plus_GAP(\n","  (GConvs): ModuleList(\n","    (0): GCNConv(37, 128)\n","    (1-2): 2 x GCNConv(128, 128)\n","  )\n","  (dropout): Dropout(p=0.3, inplace=False)\n","  (readout): GlobalMeanPool()\n","  (ffn): Linear(in_features=128, out_features=2, bias=True)\n",")\n"]}]},{"cell_type":"code","source":["GNN_Model_state_dict = torch.load(\"/content/drive/MyDrive/Explainability Methods/Run All Methods on Graph Classification/Model/GCN_plus_GAP_model_classifier_NCI1.pt\")"],"metadata":{"id":"6hLUGaiIee-U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["GNN_Model.load_state_dict(GNN_Model_state_dict['model_state_dict'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OkV6LsaffVkl","executionInfo":{"status":"ok","timestamp":1716206443507,"user_tz":-120,"elapsed":240,"user":{"displayName":"Ehsan Mobaraki","userId":"06314958514195310048"}},"outputId":"db21958f-b90d-4a98-9a26-fbb44efb05b6"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{},"execution_count":45}]},{"cell_type":"code","source":["GNN_Model_Optimizer = torch.optim.Adam(GNN_Model.parameters(), lr=classifier_lr, weight_decay=classifier_weight_decay)"],"metadata":{"id":"r2WaHCuOQqmA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["criterion = torch.nn.CrossEntropyLoss()\n","def loss_calculations(preds, gtruth):\n","    loss_per_epoch = criterion(preds, gtruth)\n","    return loss_per_epoch"],"metadata":{"id":"N4HPVX1xQsUC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def visualize_losses(GNN_Model_losses, epoch_history):\n","    GNN_Model_losses_list = torch.stack(GNN_Model_losses).cpu().detach().numpy()\n","\n","    fig = plt.figure(figsize=(27,20))\n","\n","    ax = plt.subplot2grid((3, 1), (0, 0), colspan=1)\n","    plt.xlabel('Epochs')\n","    plt.ylabel('Loss')\n","    plt.title(\" Loss in Epoch: \" + str(epoch_history))\n","\n","    ax.plot(GNN_Model_losses_list, color='r')\n","\n","    #plt.savefig('/content/drive/My Drive/Explainability Methods/' + str(Explainability_name)+' on ' + str(Task_name) + '/Experimental Results/' + File_Name + 'Loss_til_epoch_{:04d}.png'.format(epoch_history))\n","    plt.show()"],"metadata":{"id":"kWp9o5a3Shi7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train_step(data):\n","    GNN_Model_loss_batch = []\n","    Pred_Labels = []\n","    Real_Labels = []\n","\n","    GNN_Model.train()\n","    GNN_Model.zero_grad()\n","    for batch_of_graphs in data:\n","        if GNN_Model.__class__.__name__ == \"GCN_plus_GAP\":\n","            Output_of_Hidden_Layers, pooling_layer_output, ffn_output, soft = GNN_Model(batch_of_graphs)\n","            batch_loss = loss_calculations(soft, batch_of_graphs.y)\n","            Pred_Labels.extend(soft.argmax(dim=1).detach().tolist())\n","\n","        elif GNN_Model.__class__.__name__ == \"DGCNN_Model\":\n","            final_GNN_layer_output, sortpooled_embedings, output_conv1d_1, maxpooled_output_conv1d_1, output_conv1d_2, to_dense, output_h1, dropout_output_h1, output_h2, softmaxed_h2 = GNN_Model(batch_of_graphs, None)\n","            batch_loss = loss_calculations(softmaxed_h2, batch_of_graphs.y)\n","            Pred_Labels.extend(softmaxed_h2.argmax(dim=1).detach().tolist())\n","\n","        elif GNN_Model.__class__.__name__ == \"DIFFPOOL_Model\":\n","            concatination_list_of_poolings, prediction_output_not_softed, prediction_output = GNN_Model(batch_of_graphs, None)\n","            Pred_Labels.extend(prediction_output.argmax(dim=1).detach().tolist())\n","            batch_loss = loss_calculations(prediction_output, batch_of_graphs.y)\n","\n","        elif GNN_Model.__class__.__name__ == \"GIN_Model\":\n","            mlps_output_embeds, mlp_outputs_globalSUMpooled, lin1_output, lin1_output_dropouted, lin2_output, lin2_output_softmaxed = GNN_Model(batch_of_graphs, None)\n","            batch_loss = loss_calculations(lin2_output_softmaxed, batch_of_graphs.y)\n","            Pred_Labels.extend(lin2_output_softmaxed.argmax(dim=1).detach().tolist())\n","        else:\n","            raise Exception(\"We cover GCN_plus_GAP, DGCNN, DIFFPOOL, and GIN.\")\n","\n","        Real_Labels.extend(batch_of_graphs.y.detach().tolist())\n","        GNN_Model_loss_batch.append(batch_loss)\n","\n","        batch_loss.backward()\n","        GNN_Model_Optimizer.step()\n","\n","    return torch.mean(torch.tensor(GNN_Model_loss_batch)), metrics.accuracy_score(Real_Labels, Pred_Labels)"],"metadata":{"id":"fc6qCN-zQwDa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["GNN_Model_training_Acc_per_epoch = []\n","GNN_Model_training_time_per_epoch = []\n","def train(EPOCHS, load_index, data):\n","    GNN_Model_training_loss_per_epoch = []\n","\n","    for epoch in range(EPOCHS):\n","        t1 = perf_counter()\n","        GNN_Model_training_loss, training_acc = train_step(data)\n","        GNN_Model_training_time_per_epoch.append(perf_counter()-t1)\n","        print(f'Epoch: {epoch+1:03d}, Model Loss: {GNN_Model_training_loss:.4f}, Accuracy: {training_acc:.2f}')\n","\n","        GNN_Model_training_loss_per_epoch.append(GNN_Model_training_loss)\n","        GNN_Model_training_Acc_per_epoch.append(training_acc)\n","        #break\n","\n","        if (epoch + load_index + 1) % 50 == 0 and epoch > 0:\n","            visualize_losses(GNN_Model_training_loss_per_epoch, epoch + load_index + 1)\n","        #if (epoch + load_index + 1) % 100 == 0 and epoch > 0:\n","        #  torch.save({'epoch': epoch+load_index+1, 'model_state_dict': GNN_Model.state_dict(), 'optimizer_state_dict': GNN_Model_Optimizer.state_dict(), 'loss': GNN_Model_training_loss_per_epoch,}, \"/content/drive/My Drive/Explainability Methods/\" + str(Explainability_name) + \" on \" + str(Task_name) + \"/Model/\" + File_Name + str(epoch + load_index + 1)+\".pt\")\n","\n"],"metadata":{"id":"-K4246wIQy5g"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["EPOCHS = 1\n","load_index = 0\n","\n","train(EPOCHS, load_index, nci1_train_dataloader)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IaD4cuYBQ1La","executionInfo":{"status":"ok","timestamp":1716206397979,"user_tz":-120,"elapsed":275,"user":{"displayName":"Ehsan Mobaraki","userId":"06314958514195310048"}},"outputId":"9847173f-e180-4808-f8c5-f8924e6beef1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 001, Model Loss: 0.8144, Accuracy: 0.50\n"]}]},{"cell_type":"code","source":["class one_model_one_explainer:\n","    def __init__(self, dataset_name):\n","\n","        self.dataset_name = dataset_name\n","        self.fid_plus_threshold = 0.01\n","        self.fid_minus_threshold = 0.01\n","        self.contrastivity_threshold = 0.5\n","        self.sparsity_threshold = 0.5\n","        self.stability_threshold = 0.5\n","        self.stability_perturbation_mean = 0.1\n","        self.stability_perturbation_std = 0.1\n","\n","\n","    def __call__(self, a_trained_GNN_Model, explainer_name, test_dataset, num_classes, explainer_epoch, explainer_learning_rate):\n","\n","\n","        print(\"Name of the Dataset: \", self.dataset_name)\n","        print(\"Number of Classes: \", num_classes)\n","        print(\"Test Dataset Size: \", len(test_dataset))\n","        print(\"Explainer: \", explainer_name)\n","        print(\"Explainer Training Epochs: \", explainer_epoch)\n","        print(\"Explainer Learning Rate: \", explainer_learning_rate)\n","\n","        ##########################################################################################################################\n","\n","        if explainer_name == \"GNNExplainer\":\n","            import sys\n","            py_path = '/content/drive/MyDrive/Explainability Methods/GNNExplainer on Graph Classification/Script/'\n","            sys.path.insert(0,py_path)\n","            import gnnexplainer_on_graph_classification as GNNExplainer\n","\n","\n","            explanations = {}\n","            for i in range(num_classes):\n","                explanations[i] = []\n","            t1 = perf_counter()\n","            for i, graph in tqdm(enumerate(test_dataset)):\n","                for class_index in range(num_classes):\n","                    EXP = GNNExplainer.GNNExplainer(a_trained_GNN_Model, explainer_epoch, explainer_learning_rate)\n","                    node_mask, edge_mask = EXP(graph, class_index)\n","                    explanations[class_index].append(node_mask)\n","\n","            timing = perf_counter()-t1\n","            average_explanation_time = (timing)/(explainer_epoch*len(test_dataset))\n","            print(a_trained_GNN_Model.__class__.__name__, \" Model by \",  explainer_name, \" average_explanation_time: \", average_explanation_time)\n","\n","            #  Fidelity Plus\n","            py_path = '/content/drive/MyDrive/Explainability Methods/Evaluation of Explainability Methods/Script/'\n","            sys.path.insert(0,py_path)\n","            import evaluation_of_xmethods_fidelity_plus as eval_xai_fid_plus\n","            eval_xai_fid_plus = reload(eval_xai_fid_plus)\n","\n","            fid_plus_xmethod_example = eval_xai_fid_plus.evalaution_of_xmethods_fidelity_plus(a_trained_model=a_trained_GNN_Model,\n","                                                                                              test_data=test_dataset)\n","            fid_plus_score = fid_plus_xmethod_example.my_fidelity(saliencies_for_multiple_classes=explanations,\n","                                                                  importance_threshold=self.fid_plus_threshold, style=\"Node\")\n","            print(a_trained_GNN_Model.__class__.__name__, \" Model by \",explainer_name, \" Fid+: \", fid_plus_score)\n","\n","\n","            #  Fidelity Minus\n","            py_path = '/content/drive/MyDrive/Explainability Methods/Evaluation of Explainability Methods/Script/'\n","            sys.path.insert(0,py_path)\n","            import evaluation_of_xmethods_fidelity_minus as eval_xai_fid_minus\n","            eval_xai_fid_minus = reload(eval_xai_fid_minus)\n","\n","            fid_minus_xmethod_example = eval_xai_fid_minus.evalaution_of_xmethods_fidelity_minus(a_trained_model=a_trained_GNN_Model,\n","                                                                                                 test_data=test_dataset)\n","            fid_minus_score = fid_minus_xmethod_example.my_fidelity(saliencies_for_multiple_classes=explanations,\n","                                                                    importance_threshold=self.fid_minus_threshold, style=\"Node\")\n","            print(a_trained_GNN_Model.__class__.__name__, \" Model by \",explainer_name, \" Fid-: \", fid_minus_score)\n","\n","\n","            #  Contrastivity\n","            py_path = '/content/drive/MyDrive/Explainability Methods/Evaluation of Explainability Methods/Script/'\n","            sys.path.insert(0,py_path)\n","            import evaluation_of_xmethods_contrastivity as eval_xai_contrastivity\n","            eval_xai_contrastivity = reload(eval_xai_contrastivity)\n","\n","            contrastivity_xmethod_example = eval_xai_contrastivity.evalaution_of_xmethods_contrastivity(a_trained_model=a_trained_GNN_Model,\n","                                                                                                        test_data=test_dataset)\n","            contrastivity_score = contrastivity_xmethod_example.my_contrastivity(your_dataset=test_dataset,\n","                                                                                 saliencies_for_multiple_classes=explanations,\n","                                                                                 importance_threshold=self.contrastivity_threshold, contrast_coeff=1e+11)\n","            print(a_trained_GNN_Model.__class__.__name__, \" Model by \",explainer_name, \" Contrastivity_Score: \", contrastivity_score)\n","\n","\n","            #  Sparsity\n","            py_path = '/content/drive/MyDrive/Explainability Methods/Evaluation of Explainability Methods/Script/'\n","            sys.path.insert(0,py_path)\n","            import evaluation_of_xmethods_sparsity as eval_xai_sparsity\n","            eval_xai_sparsity = reload(eval_xai_sparsity)\n","\n","            sparsity_xmethod_example = eval_xai_sparsity.evalaution_of_xmethods_sparsity(a_trained_model=a_trained_GNN_Model,\n","                                                                                         test_data=test_dataset)\n","            sparsity_score = sparsity_xmethod_example.my_sparsity(your_dataset=test_dataset, saliencies_for_multiple_classes=explanations,\n","                                                                  importance_threshold=self.sparsity_threshold)\n","            print(a_trained_GNN_Model.__class__.__name__, \" Model by \",explainer_name, \" Sparsity_Score: \", sparsity_score)\n","\n","\n","            #  Stability\n","            py_path = '/content/drive/MyDrive/Explainability Methods/Evaluation of Explainability Methods/Script/'\n","            sys.path.insert(0,py_path)\n","            import evaluation_of_xmethods_stability as eval_xai_stability\n","            eval_xai_stability = reload(eval_xai_stability)\n","\n","            stability = eval_xai_stability.evalaution_of_xmethods_stability(a_trained_model=a_trained_GNN_Model, test_data=test_dataset)\n","            perturbed_test_dataset = stability.perturb_node_features_of_dataset(test_dataset, self.stability_perturbation_mean, self.stability_perturbation_std)\n","\n","            explanations_for_perturbed_data = {}\n","            for i in range(num_classes):\n","                explanations_for_perturbed_data[i] = []\n","            for graph in perturbed_test_dataset:\n","                for class_index in range(num_classes):\n","                    EXP = GNNExplainer.GNNExplainer(a_trained_GNN_Model, explainer_epoch, explainer_learning_rate)\n","                    node_mask, edge_mask = EXP(graph, class_index)\n","                    explanations_for_perturbed_data[class_index].append(node_mask)\n","\n","            stability_score = stability.my_stability(normal_saliencies_for_multiple_classes=explanations,\n","                                                     perturbed_saliencies_for_multiple_classes=explanations_for_perturbed_data,\n","                                                     top_k_features=2, importance_threshold=self.stability_threshold, style='Node')\n","            print(a_trained_GNN_Model.__class__.__name__, \" Model by \",explainer_name, \" Stability_Score: \", stability_score)\n","\n","\n","\n","\n","\n","            ######################################################################################################################\n","\n","        elif explainer_name == \"PGExplainer\":\n","\n","            import sys\n","            py_path = '/content/drive/MyDrive/Explainability Methods/PGExplainer on Graph Classification/Script/'\n","            sys.path.insert(0,py_path)\n","            import pgexplainer_on_graph_classification as PGExplainer_Module\n","            PGExplainer_Module = reload(PGExplainer_Module)\n","\n","            explanations_list = {}\n","            explanations_tensor = {}\n","            for i in range(num_classes):\n","                explanations_list[i] = []\n","                explanations_tensor[i] = []\n","            pgex_t1 = perf_counter()\n","            for class_index in tqdm(range(num_classes)):\n","\n","                ExTrain_or_ExTest = 'train'\n","                EXP = PGExplainer_Module.PGExplainer(GNN_Model=a_trained_GNN_Model, explainer_save_index=explainer_epoch,\n","                                                     Exp_Epoch=explainer_epoch, Exp_lr=explainer_learning_rate,\n","                                                     node_feat_dim=test_dataset[0].x[0].size()[0])\n","\n","                EXP(ExTrain_or_ExTest=ExTrain_or_ExTest, Exp_Load_index=explainer_epoch, your_dataset=test_dataset,\n","                    target_class=class_index)\n","\n","                ExTrain_or_ExTest = 'test'\n","                EXP = PGExplainer_Module.PGExplainer(GNN_Model=a_trained_GNN_Model, explainer_save_index=explainer_epoch,\n","                                                     Exp_Epoch=explainer_epoch, Exp_lr=explainer_learning_rate,\n","                                                     node_feat_dim=test_dataset[0].x[0].size()[0])\n","                for graph in test_dataset:\n","                    edge_mask = EXP(ExTrain_or_ExTest=ExTrain_or_ExTest, Exp_Load_index=explainer_epoch, your_dataset=[graph],\n","                                     target_class=class_index)\n","                    explanations_list[class_index].append(edge_mask[0].tolist())\n","                    explanations_tensor[class_index].append(edge_mask[0])\n","            pgex_timing = perf_counter()-pgex_t1\n","            average_explanation_time = (pgex_timing)/(explainer_epoch*len(test_dataset))\n","            print(a_trained_GNN_Model.__class__.__name__, \" Model by \",  explainer_name, \" average_explanation_time: \", average_explanation_time)\n","\n","            #  Fidelity Plus\n","            py_path = '/content/drive/MyDrive/Explainability Methods/Evaluation of Explainability Methods/Script/'\n","            sys.path.insert(0,py_path)\n","            import evaluation_of_xmethods_fidelity_plus as eval_xai_fid_plus\n","            eval_xai_fid_plus = reload(eval_xai_fid_plus)\n","\n","            fid_plus_xmethod_example = eval_xai_fid_plus.evalaution_of_xmethods_fidelity_plus(a_trained_model=a_trained_GNN_Model,\n","                                                                                              test_data=test_dataset)\n","            fid_plus_score = fid_plus_xmethod_example.my_fidelity(saliencies_for_multiple_classes=explanations_tensor,\n","                                                                  importance_threshold=self.fid_plus_threshold, style=\"Edge\")\n","            print(a_trained_GNN_Model.__class__.__name__, \" Model by \",explainer_name, \" Fid+: \", fid_plus_score)\n","\n","\n","            #  Fidelity Minus\n","            py_path = '/content/drive/MyDrive/Explainability Methods/Evaluation of Explainability Methods/Script/'\n","            sys.path.insert(0,py_path)\n","            import evaluation_of_xmethods_fidelity_minus as eval_xai_fid_minus\n","            eval_xai_fid_minus = reload(eval_xai_fid_minus)\n","\n","            fid_minus_xmethod_example = eval_xai_fid_minus.evalaution_of_xmethods_fidelity_minus(a_trained_model=a_trained_GNN_Model,\n","                                                                                                 test_data=test_dataset)\n","            fid_minus_score = fid_minus_xmethod_example.my_fidelity(saliencies_for_multiple_classes=explanations_tensor,\n","                                                                    importance_threshold=self.fid_minus_threshold, style=\"Edge\")\n","            print(a_trained_GNN_Model.__class__.__name__, \" Model by \",explainer_name, \" Fid-: \", fid_minus_score)\n","\n","\n","            #  Contrastivity\n","            py_path = '/content/drive/MyDrive/Explainability Methods/Evaluation of Explainability Methods/Script/'\n","            sys.path.insert(0,py_path)\n","            import evaluation_of_xmethods_contrastivity as eval_xai_contrastivity\n","            eval_xai_contrastivity = reload(eval_xai_contrastivity)\n","\n","            contrastivity_xmethod_example = eval_xai_contrastivity.evalaution_of_xmethods_contrastivity(a_trained_model=a_trained_GNN_Model,\n","                                                                                                        test_data=test_dataset)\n","            contrastivity_score = contrastivity_xmethod_example.my_contrastivity(your_dataset=test_dataset,\n","                                                                                 saliencies_for_multiple_classes=explanations_list,\n","                                                                                 importance_threshold=self.contrastivity_threshold,\n","                                                                                 contrast_coeff=1e+11)\n","            print(a_trained_GNN_Model.__class__.__name__, \" Model by \",explainer_name, \" Contrastivity_Score: \", contrastivity_score)\n","\n","\n","            #  Sparsity\n","            py_path = '/content/drive/MyDrive/Explainability Methods/Evaluation of Explainability Methods/Script/'\n","            sys.path.insert(0,py_path)\n","            import evaluation_of_xmethods_sparsity as eval_xai_sparsity\n","            eval_xai_sparsity = reload(eval_xai_sparsity)\n","\n","            sparsity_xmethod_example = eval_xai_sparsity.evalaution_of_xmethods_sparsity(a_trained_model=a_trained_GNN_Model,\n","                                                                                         test_data=test_dataset)\n","            sparsity_score = sparsity_xmethod_example.my_sparsity(your_dataset=test_dataset,\n","                                                                  saliencies_for_multiple_classes=explanations_list,\n","                                                                  importance_threshold=self.sparsity_threshold)\n","            print(a_trained_GNN_Model.__class__.__name__, \" Model by \",explainer_name, \" Sparsity_Score: \", sparsity_score)\n","\n","\n","            #  Stability\n","            py_path = '/content/drive/MyDrive/Explainability Methods/Evaluation of Explainability Methods/Script/'\n","            sys.path.insert(0,py_path)\n","            import evaluation_of_xmethods_stability as eval_xai_stability\n","            eval_xai_stability = reload(eval_xai_stability)\n","\n","            stability = eval_xai_stability.evalaution_of_xmethods_stability(a_trained_model=a_trained_GNN_Model, test_data=test_dataset)\n","            perturbed_test_dataset = stability.perturb_node_features_of_dataset(test_dataset, self.stability_perturbation_mean, self.stability_perturbation_std)\n","\n","            explanations_for_perturbed_data_tensor = {}\n","            explanations_for_perturbed_data_list = {}\n","            for i in range(num_classes):\n","                explanations_for_perturbed_data_tensor[i] = []\n","                explanations_for_perturbed_data_list[i] = []\n","\n","            for class_index in range(num_classes):\n","\n","                ExTrain_or_ExTest = 'train'\n","                EXP = PGExplainer_Module.PGExplainer(GNN_Model=a_trained_GNN_Model, explainer_save_index=explainer_epoch,\n","                                                     Exp_Epoch=explainer_epoch, Exp_lr=explainer_learning_rate,\n","                                                     node_feat_dim=test_dataset[0].x[0].size()[0])\n","\n","                EXP(ExTrain_or_ExTest=ExTrain_or_ExTest, Exp_Load_index=explainer_epoch, your_dataset=test_dataset,\n","                    target_class=class_index)\n","\n","                ExTrain_or_ExTest = 'test'\n","                EXP = PGExplainer_Module.PGExplainer(GNN_Model=a_trained_GNN_Model, explainer_save_index=explainer_epoch,\n","                                                     Exp_Epoch=explainer_epoch, Exp_lr=explainer_learning_rate,\n","                                                     node_feat_dim=test_dataset[0].x[0].size()[0])\n","                for graph in perturbed_test_dataset:\n","                    edge_mask = EXP(ExTrain_or_ExTest=ExTrain_or_ExTest, Exp_Load_index=explainer_epoch, your_dataset=[graph],\n","                                     target_class=class_index)\n","                    explanations_for_perturbed_data_tensor[class_index].append(edge_mask[0])\n","                    explanations_for_perturbed_data_list[class_index].append(edge_mask[0].tolist())\n","\n","            stability_score = stability.my_stability(normal_saliencies_for_multiple_classes=explanations_list,\n","                                                     perturbed_saliencies_for_multiple_classes=explanations_for_perturbed_data_list,\n","                                                     top_k_features=2, importance_threshold=self.stability_threshold, style='Edge')\n","            print(a_trained_GNN_Model.__class__.__name__, \" Model by \",explainer_name, \" Stability_Score: \", stability_score)\n","\n","\n","\n","            ######################################################################################################################\n","\n","        elif explainer_name == \"GraphMask\":\n","\n","            import sys\n","            py_path = '/content/drive/MyDrive/Explainability Methods/GraphMask on Graph Classification/Script/'\n","            sys.path.insert(0,py_path)\n","            import graphmask_on_graph_classification as GraphMask_Module\n","            GraphMask_Module = reload(GraphMask_Module)\n","\n","            explanations_list = {}\n","            explanations_tensor = {}\n","            for i in range(num_classes):\n","                explanations_list[i] = []\n","                explanations_tensor[i] = []\n","            graphmask_t1 = perf_counter()\n","            for class_index in tqdm(range(num_classes)):\n","\n","                ExTrain_or_ExTest = 'train'\n","                EXP = GraphMask_Module.GraphMask(GNN_Model=a_trained_GNN_Model, explainer_save_index=explainer_epoch,\n","                                                 Exp_Epoch=explainer_epoch, Exp_lr=0.001, explainer_hid_dim=test_dataset[0].x[0].size()[0],\n","                                                 node_feat_dim=test_dataset[0].x[0].size()[0])\n","\n","                EXP(ExTrain_or_ExTest=ExTrain_or_ExTest, Exp_Load_index=explainer_epoch, your_dataset=test_dataset,\n","                    target_class=class_index)\n","\n","                ExTrain_or_ExTest = 'test'\n","                EXP = GraphMask_Module.GraphMask(GNN_Model=a_trained_GNN_Model, explainer_save_index=explainer_epoch,\n","                                                 Exp_Epoch=explainer_epoch, Exp_lr=0.001, explainer_hid_dim=test_dataset[0].x[0].size()[0],\n","                                                 node_feat_dim=test_dataset[0].x[0].size()[0])\n","                for graph in test_dataset:\n","                    edge_mask = EXP(ExTrain_or_ExTest=ExTrain_or_ExTest, Exp_Load_index=explainer_epoch, your_dataset=[graph],\n","                                     target_class=class_index)\n","                    explanations_list[class_index].append(edge_mask[0].tolist())\n","                    explanations_tensor[class_index].append(edge_mask[0])\n","            graphmask_timing = perf_counter()-graphmask_t1\n","            average_explanation_time = (graphmask_timing)/(explainer_epoch*len(test_dataset))\n","            print(a_trained_GNN_Model.__class__.__name__, \" Model by \",  explainer_name, \" average_explanation_time: \", average_explanation_time)\n","\n","\n","            #  Fidelity Plus\n","            py_path = '/content/drive/MyDrive/Explainability Methods/Evaluation of Explainability Methods/Script/'\n","            sys.path.insert(0,py_path)\n","            import evaluation_of_xmethods_fidelity_plus as eval_xai_fid_plus\n","            eval_xai_fid_plus = reload(eval_xai_fid_plus)\n","\n","            fid_plus_xmethod_example = eval_xai_fid_plus.evalaution_of_xmethods_fidelity_plus(a_trained_model=a_trained_GNN_Model,\n","                                                                                              test_data=test_dataset)\n","            fid_plus_score = fid_plus_xmethod_example.my_fidelity(saliencies_for_multiple_classes=explanations_tensor,\n","                                                                  importance_threshold=self.fid_plus_threshold, style=\"Edge\")\n","            print(a_trained_GNN_Model.__class__.__name__, \" Model by \",explainer_name, \" Fid+: \", fid_plus_score)\n","\n","\n","            #  Fidelity Minus\n","            py_path = '/content/drive/MyDrive/Explainability Methods/Evaluation of Explainability Methods/Script/'\n","            sys.path.insert(0,py_path)\n","            import evaluation_of_xmethods_fidelity_minus as eval_xai_fid_minus\n","            eval_xai_fid_minus = reload(eval_xai_fid_minus)\n","\n","            fid_minus_xmethod_example = eval_xai_fid_minus.evalaution_of_xmethods_fidelity_minus(a_trained_model=a_trained_GNN_Model,\n","                                                                                                 test_data=test_dataset)\n","            fid_minus_score = fid_minus_xmethod_example.my_fidelity(saliencies_for_multiple_classes=explanations_tensor,\n","                                                                    importance_threshold=self.fid_minus_threshold, style=\"Edge\")\n","            print(a_trained_GNN_Model.__class__.__name__, \" Model by \",explainer_name, \" Fid-: \", fid_minus_score)\n","\n","\n","            #  Contrastivity\n","            py_path = '/content/drive/MyDrive/Explainability Methods/Evaluation of Explainability Methods/Script/'\n","            sys.path.insert(0,py_path)\n","            import evaluation_of_xmethods_contrastivity as eval_xai_contrastivity\n","            eval_xai_contrastivity = reload(eval_xai_contrastivity)\n","\n","            contrastivity_xmethod_example = eval_xai_contrastivity.evalaution_of_xmethods_contrastivity(a_trained_model=a_trained_GNN_Model,\n","                                                                                                        test_data=test_dataset)\n","            contrastivity_score = contrastivity_xmethod_example.my_contrastivity(your_dataset=test_dataset,\n","                                                                                 saliencies_for_multiple_classes=explanations_list,\n","                                                                                 importance_threshold=self.contrastivity_threshold,\n","                                                                                 contrast_coeff=1e+11)\n","            print(a_trained_GNN_Model.__class__.__name__, \" Model by \",explainer_name, \" Contrastivity_Score: \", contrastivity_score)\n","\n","\n","            #  Sparsity\n","            py_path = '/content/drive/MyDrive/Explainability Methods/Evaluation of Explainability Methods/Script/'\n","            sys.path.insert(0,py_path)\n","            import evaluation_of_xmethods_sparsity as eval_xai_sparsity\n","            eval_xai_sparsity = reload(eval_xai_sparsity)\n","\n","            sparsity_xmethod_example = eval_xai_sparsity.evalaution_of_xmethods_sparsity(a_trained_model=a_trained_GNN_Model,\n","                                                                                         test_data=test_dataset)\n","            sparsity_score = sparsity_xmethod_example.my_sparsity(your_dataset=test_dataset,\n","                                                                  saliencies_for_multiple_classes=explanations_list,\n","                                                                  importance_threshold=self.sparsity_threshold)\n","            print(a_trained_GNN_Model.__class__.__name__, \" Model by \",explainer_name, \" Sparsity_Score: \", sparsity_score)\n","\n","\n","\n","            #  Stability\n","            py_path = '/content/drive/MyDrive/Explainability Methods/Evaluation of Explainability Methods/Script/'\n","            sys.path.insert(0,py_path)\n","            import evaluation_of_xmethods_stability as eval_xai_stability\n","            eval_xai_stability = reload(eval_xai_stability)\n","\n","            stability = eval_xai_stability.evalaution_of_xmethods_stability(a_trained_model=a_trained_GNN_Model, test_data=test_dataset)\n","            perturbed_test_dataset = stability.perturb_node_features_of_dataset(test_dataset, self.stability_perturbation_mean, self.stability_perturbation_std)\n","\n","            explanations_for_perturbed_data_tensor = {}\n","            explanations_for_perturbed_data_list = {}\n","            for i in range(num_classes):\n","                explanations_for_perturbed_data_tensor[i] = []\n","                explanations_for_perturbed_data_list[i] = []\n","\n","            for class_index in range(num_classes):\n","\n","                ExTrain_or_ExTest = 'train'\n","                EXP = GraphMask_Module.GraphMask(GNN_Model=a_trained_GNN_Model, explainer_save_index=explainer_epoch,\n","                                                 Exp_Epoch=explainer_epoch, Exp_lr=0.001, explainer_hid_dim=test_dataset[0].x[0].size()[0],\n","                                                 node_feat_dim=test_dataset[0].x[0].size()[0])\n","\n","                EXP(ExTrain_or_ExTest=ExTrain_or_ExTest, Exp_Load_index=explainer_epoch, your_dataset=test_dataset,\n","                    target_class=class_index)\n","\n","                ExTrain_or_ExTest = 'test'\n","                EXP = GraphMask_Module.GraphMask(GNN_Model=a_trained_GNN_Model, explainer_save_index=explainer_epoch,\n","                                                 Exp_Epoch=explainer_epoch, Exp_lr=0.001, explainer_hid_dim=test_dataset[0].x[0].size()[0],\n","                                                 node_feat_dim=test_dataset[0].x[0].size()[0])\n","                for graph in perturbed_test_dataset:\n","                    edge_mask = EXP(ExTrain_or_ExTest=ExTrain_or_ExTest, Exp_Load_index=explainer_epoch, your_dataset=[graph],\n","                                     target_class=class_index)\n","                    explanations_for_perturbed_data_tensor[class_index].append(edge_mask[0])\n","                    explanations_for_perturbed_data_list[class_index].append(edge_mask[0].tolist())\n","\n","            stability_score = stability.my_stability(normal_saliencies_for_multiple_classes=explanations_list,\n","                                                     perturbed_saliencies_for_multiple_classes=explanations_for_perturbed_data_list,\n","                                                     top_k_features=2, importance_threshold=self.stability_threshold, style='Edge')\n","            print(a_trained_GNN_Model.__class__.__name__, \" Model by \",explainer_name, \" Stability_Score: \", stability_score)\n","\n","\n","\n","\n","\n","            ######################################################################################################################\n","\n","        elif explainer_name == \"SubGraphX\":\n","            import sys\n","            py_path = '/content/drive/MyDrive/Explainability Methods/SubGraphX on Graph Classification/Script/'\n","            sys.path.insert(0,py_path)\n","            import subgraphx_on_graph_classification_final_format as SubGraphX_Module\n","            SubGraphX_Module = reload(SubGraphX_Module)\n","\n","            Train_or_Test='test'\n","            if Train_or_Test == 'train':\n","\n","                SubGraphX_t1 = perf_counter()\n","                SubGX = SubGraphX_Module.SubGraphX(GNN_Model=GNN_Model, num_classes=num_classes, num_hops=2, explain_graph=True,\n","                                                   rollout_count=20, min_children_threshold=5, ubc1_c_coef= 10.0,\n","                                                   expand_count_threshold=5, high2low=True, sample_num=100,\n","                                                   save_permission_explanations= True, dataset_name=self.dataset_name)\n","                for i, graph in enumerate(test_dataset):\n","                    explanation_results, related_preds, time_list = SubGX(graph=graph, graph_index=i)\n","\n","                SubGraphX_timing = perf_counter()-SubGraphX_t1\n","                average_explanation_time = (SubGraphX_timing)/(len(test_dataset))\n","                print(a_trained_GNN_Model.__class__.__name__, \" Model by \",  explainer_name, \" average_explanation_time: \", average_explanation_time)\n","\n","            elif Train_or_Test == 'test':\n","                subgx_out_the_fly = SubGraphX_Module.SubGraphX_off_the_fly(test_dataset=test_dataset, Task_name='Graph Classification',\n","                                                                           num_classes=num_classes, dataset_name=self.dataset_name,\n","                                                                           Model_Name=GNN_Model.__class__.__name__)\n","                whole_data = subgx_out_the_fly(test_dataset)\n","\n","                explanations = {}\n","                for i in range(num_classes):\n","                    explanations[i] = []\n","                for cls in range(num_classes):\n","                    for i in range(len(test_dataset)):\n","                        explanations[cls].append(torch.sum(whole_data[i][cls]['important_nodes'].x, dim=1).tolist())\n","\n","\n","\n","\n","            #  Fidelity Plus\n","            py_path = '/content/drive/MyDrive/Explainability Methods/Evaluation of Explainability Methods/Script/'\n","            sys.path.insert(0,py_path)\n","            import evaluation_of_xmethods_fidelity_plus as eval_xai_fid_plus\n","            eval_xai_fid_plus = reload(eval_xai_fid_plus)\n","\n","            fid_plus_xmethod_example = eval_xai_fid_plus.evalaution_of_xmethods_fidelity_plus(a_trained_model=a_trained_GNN_Model,\n","                                                                                              test_data=test_dataset)\n","            fid_plus_score = fid_plus_xmethod_example.my_fidelity(saliencies_for_multiple_classes=explanations,\n","                                                                  importance_threshold=self.fid_plus_threshold, style=\"Node\")\n","            print(a_trained_GNN_Model.__class__.__name__, \" Model by \",explainer_name, \" Fid+: \", fid_plus_score)\n","\n","\n","            #  Fidelity Minus\n","            py_path = '/content/drive/MyDrive/Explainability Methods/Evaluation of Explainability Methods/Script/'\n","            sys.path.insert(0,py_path)\n","            import evaluation_of_xmethods_fidelity_minus as eval_xai_fid_minus\n","            eval_xai_fid_minus = reload(eval_xai_fid_minus)\n","\n","            fid_minus_xmethod_example = eval_xai_fid_minus.evalaution_of_xmethods_fidelity_minus(a_trained_model=a_trained_GNN_Model,\n","                                                                                                 test_data=test_dataset)\n","            fid_minus_score = fid_minus_xmethod_example.my_fidelity(saliencies_for_multiple_classes=explanations,\n","                                                                    importance_threshold=self.fid_minus_threshold, style=\"Node\")\n","            print(a_trained_GNN_Model.__class__.__name__, \" Model by \",explainer_name, \" Fid-: \", fid_minus_score)\n","\n","\n","            #  Contrastivity\n","            py_path = '/content/drive/MyDrive/Explainability Methods/Evaluation of Explainability Methods/Script/'\n","            sys.path.insert(0,py_path)\n","            import evaluation_of_xmethods_contrastivity as eval_xai_contrastivity\n","            eval_xai_contrastivity = reload(eval_xai_contrastivity)\n","\n","            contrastivity_xmethod_example = eval_xai_contrastivity.evalaution_of_xmethods_contrastivity(a_trained_model=a_trained_GNN_Model,\n","                                                                                                        test_data=test_dataset)\n","            contrastivity_score = contrastivity_xmethod_example.my_contrastivity(your_dataset=test_dataset,\n","                                                                                 saliencies_for_multiple_classes=explanations,\n","                                                                                 importance_threshold=self.contrastivity_threshold,\n","                                                                                 contrast_coeff=1e+11)\n","            print(a_trained_GNN_Model.__class__.__name__, \" Model by \",explainer_name, \" Contrastivity_Score: \", contrastivity_score)\n","\n","\n","            #  Sparsity\n","            py_path = '/content/drive/MyDrive/Explainability Methods/Evaluation of Explainability Methods/Script/'\n","            sys.path.insert(0,py_path)\n","            import evaluation_of_xmethods_sparsity as eval_xai_sparsity\n","            eval_xai_sparsity = reload(eval_xai_sparsity)\n","\n","            sparsity_xmethod_example = eval_xai_sparsity.evalaution_of_xmethods_sparsity(a_trained_model=a_trained_GNN_Model,\n","                                                                                         test_data=test_dataset)\n","            sparsity_score = sparsity_xmethod_example.my_sparsity(your_dataset=test_dataset,\n","                                                                  saliencies_for_multiple_classes=explanations,\n","                                                                  importance_threshold=self.sparsity_threshold)\n","            print(a_trained_GNN_Model.__class__.__name__, \" Model by \",explainer_name, \" Sparsity_Score: \", sparsity_score)\n","\n","\n","\n","\n","            #  Stability\n","            py_path = '/content/drive/MyDrive/Explainability Methods/Evaluation of Explainability Methods/Script/'\n","            sys.path.insert(0,py_path)\n","            import evaluation_of_xmethods_stability as eval_xai_stability\n","            eval_xai_stability = reload(eval_xai_stability)\n","\n","            stability = eval_xai_stability.evalaution_of_xmethods_stability(a_trained_model=a_trained_GNN_Model, test_data=test_dataset)\n","            perturbed_test_dataset = stability.perturb_node_features_of_dataset(test_dataset, self.stability_perturbation_mean,\n","                                                                                self.stability_perturbation_std)\n","\n","            explanations_for_perturbed_data = {}\n","            for i in range(num_classes):\n","                explanations_for_perturbed_data[i] = []\n","\n","            Train_or_Test_perturbed='train'\n","            if Train_or_Test_perturbed == 'train':\n","                SubGX = SubGraphX_Module.SubGraphX(GNN_Model=GNN_Model, num_classes=num_classes, num_hops=2, explain_graph=True,\n","                                                rollout_count=20, min_children_threshold=5, ubc1_c_coef= 10.0,\n","                                                expand_count_threshold=5, high2low=True, sample_num=100,\n","                                                save_permission_explanations= True, dataset_name=\"Perturbed_\"+str(self.dataset_name))\n","                for i, graph in enumerate(perturbed_test_dataset):\n","                    explanation_results, related_preds, time_list = SubGX(graph=graph, graph_index=i)\n","            elif Train_or_Test_perturbed == 'test':\n","                subgx_out_the_fly = SubGraphX_Module.SubGraphX_off_the_fly(test_dataset=perturbed_test_dataset, Task_name='Graph Classification',\n","                                                                           num_classes=num_classes, dataset_name=\"Perturbed_\"+str(self.dataset_name),\n","                                                                           Model_Name=GNN_Model.__class__.__name__)\n","                whole_data = subgx_out_the_fly(perturbed_test_dataset)\n","\n","                for cls in range(num_classes):\n","                    for i in range(len(perturbed_test_dataset)):\n","                        explanations_for_perturbed_data[cls].append(torch.sum(whole_data[i][cls]['important_nodes'].x, dim=1).tolist())\n","\n","            # print(\"explanations: \", explanations)\n","            # print(\"explanations_for_perturbed_data: \", explanations_for_perturbed_data)\n","            # for cls in explanations.keys():\n","            #     l1=[]\n","            #     for graph in explanations[cls]:\n","            #         l1.append(len(graph))\n","            #     print(l1)\n","            # for cls in explanations_for_perturbed_data.keys():\n","            #     l1=[]\n","            #     for graph in explanations_for_perturbed_data[cls]:\n","            #         l1.append(len(graph))\n","            #     print(l1)\n","            stability_score = stability.my_stability(normal_saliencies_for_multiple_classes=explanations,\n","                                                     perturbed_saliencies_for_multiple_classes=explanations_for_perturbed_data,\n","                                                     top_k_features=2, importance_threshold=self.stability_threshold, style='Node')\n","            print(a_trained_GNN_Model.__class__.__name__, \" Model by \",explainer_name, \" Stability_Score: \", stability_score)\n","\n","\n","            ######################################################################################################################\n","\n","        elif explainer_name == \"CF2\":\n","            import sys\n","            py_path = '/content/drive/MyDrive/Explainability Methods/CF2 on Graph Classification/Script/'\n","            sys.path.insert(0,py_path)\n","            import cf2_on_graph_classification_final_format as CF2_Module\n","            CF2_Module = reload(CF2_Module)\n","            explanations = {}\n","            explanations_list = {}\n","            for i in range(num_classes):\n","                explanations[i] = []\n","                explanations_list[i] = []\n","            CF2_t1 = perf_counter()\n","            for i in range(num_classes):\n","                cf2_explanation = CF2_Module.CF2_Explaination(GNN_Model=a_trained_GNN_Model, your_dataset=test_dataset,\n","                                                              explainer_epochs=explainer_epoch, fix_exp=None, input_dim=test_dataset[0].x[0].size()[0],\n","                                                              hid_dim=test_dataset[0].x[0].size()[0], output_dim=num_classes)\n","                exp_dict = cf2_explanation.explain_nodes_gnn_stats(category=i)\n","                for key, value in exp_dict.items():\n","                    explanations[i].append(value)\n","                    explanations_list[i].append(value.tolist())\n","            CF2_timing = perf_counter()-CF2_t1\n","            average_explanation_time = (CF2_timing)/(explainer_epoch*len(test_dataset))\n","            print(a_trained_GNN_Model.__class__.__name__, \" Model by \",  explainer_name, \" average_explanation_time: \", average_explanation_time)\n","\n","\n","            #  Fidelity Plus\n","            py_path = '/content/drive/MyDrive/Explainability Methods/Evaluation of Explainability Methods/Script/'\n","            sys.path.insert(0,py_path)\n","            import evaluation_of_xmethods_fidelity_plus as eval_xai_fid_plus\n","            eval_xai_fid_plus = reload(eval_xai_fid_plus)\n","\n","            fid_plus_xmethod_example = eval_xai_fid_plus.evalaution_of_xmethods_fidelity_plus(a_trained_model=a_trained_GNN_Model,\n","                                                                                              test_data=test_dataset)\n","            fid_plus_score = fid_plus_xmethod_example.my_fidelity(saliencies_for_multiple_classes=explanations,\n","                                                                  importance_threshold=self.fid_plus_threshold, style=\"Edge\")\n","            print(a_trained_GNN_Model.__class__.__name__, \" Model by \",explainer_name, \" Fid+: \", fid_plus_score)\n","\n","\n","            #  Fidelity Minus\n","            py_path = '/content/drive/MyDrive/Explainability Methods/Evaluation of Explainability Methods/Script/'\n","            sys.path.insert(0,py_path)\n","            import evaluation_of_xmethods_fidelity_minus as eval_xai_fid_minus\n","            eval_xai_fid_minus = reload(eval_xai_fid_minus)\n","\n","            fid_minus_xmethod_example = eval_xai_fid_minus.evalaution_of_xmethods_fidelity_minus(a_trained_model=a_trained_GNN_Model,\n","                                                                                                 test_data=test_dataset)\n","            fid_minus_score = fid_minus_xmethod_example.my_fidelity(saliencies_for_multiple_classes=explanations,\n","                                                                    importance_threshold=self.fid_minus_threshold, style=\"Edge\")\n","            print(a_trained_GNN_Model.__class__.__name__, \" Model by \",explainer_name, \" Fid-: \", fid_minus_score)\n","\n","\n","            #  Contrastivity\n","            py_path = '/content/drive/MyDrive/Explainability Methods/Evaluation of Explainability Methods/Script/'\n","            sys.path.insert(0,py_path)\n","            import evaluation_of_xmethods_contrastivity as eval_xai_contrastivity\n","            eval_xai_contrastivity = reload(eval_xai_contrastivity)\n","\n","            contrastivity_xmethod_example = eval_xai_contrastivity.evalaution_of_xmethods_contrastivity(a_trained_model=a_trained_GNN_Model,\n","                                                                                                        test_data=test_dataset)\n","            contrastivity_score = contrastivity_xmethod_example.my_contrastivity(your_dataset=test_dataset,\n","                                                                                 saliencies_for_multiple_classes=explanations_list,\n","                                                                                 importance_threshold=self.contrastivity_threshold,\n","                                                                                 contrast_coeff=1e+11)\n","            print(a_trained_GNN_Model.__class__.__name__, \" Model by \",explainer_name, \" Contrastivity_Score: \", contrastivity_score)\n","\n","\n","            #  Sparsity\n","            py_path = '/content/drive/MyDrive/Explainability Methods/Evaluation of Explainability Methods/Script/'\n","            sys.path.insert(0,py_path)\n","            import evaluation_of_xmethods_sparsity as eval_xai_sparsity\n","            eval_xai_sparsity = reload(eval_xai_sparsity)\n","\n","            sparsity_xmethod_example = eval_xai_sparsity.evalaution_of_xmethods_sparsity(a_trained_model=a_trained_GNN_Model,\n","                                                                                         test_data=test_dataset)\n","            sparsity_score = sparsity_xmethod_example.my_sparsity(your_dataset=test_dataset,\n","                                                                  saliencies_for_multiple_classes=explanations_list,\n","                                                                  importance_threshold=self.sparsity_threshold)\n","            print(a_trained_GNN_Model.__class__.__name__, \" Model by \",explainer_name, \" Sparsity_Score: \", sparsity_score)\n","\n","\n","\n","            #  Stability\n","            py_path = '/content/drive/MyDrive/Explainability Methods/Evaluation of Explainability Methods/Script/'\n","            sys.path.insert(0,py_path)\n","            import evaluation_of_xmethods_stability as eval_xai_stability\n","            eval_xai_stability = reload(eval_xai_stability)\n","\n","            stability = eval_xai_stability.evalaution_of_xmethods_stability(a_trained_model=a_trained_GNN_Model,\n","                                                                            test_data=test_dataset)\n","            perturbed_test_dataset = stability.perturb_node_features_of_dataset(test_dataset, self.stability_perturbation_mean,\n","                                                                                self.stability_perturbation_std)\n","\n","            explanations_for_perturbed_data_tensor = {}\n","            explanations_for_perturbed_data_list = {}\n","            for i in range(num_classes):\n","                explanations_for_perturbed_data_tensor[i] = []\n","                explanations_for_perturbed_data_list[i] = []\n","\n","            for i in range(num_classes):\n","                cf2_explanation = CF2_Module.CF2_Explaination(GNN_Model=a_trained_GNN_Model, your_dataset=perturbed_test_dataset,\n","                                                              explainer_epochs=explainer_epoch, fix_exp=None, input_dim=test_dataset[0].x[0].size()[0],\n","                                                              hid_dim=test_dataset[0].x[0].size()[0], output_dim=num_classes)\n","                exp_dict = cf2_explanation.explain_nodes_gnn_stats(category=i)\n","                for key, value in exp_dict.items():\n","                    explanations_for_perturbed_data_tensor[i].append(value)\n","                    explanations_for_perturbed_data_list[i].append(value.tolist())\n","\n","            stability_score = stability.my_stability(normal_saliencies_for_multiple_classes=explanations_list,\n","                                                     perturbed_saliencies_for_multiple_classes=explanations_for_perturbed_data_list,\n","                                                     top_k_features=2, importance_threshold=self.stability_threshold, style='Edge')\n","            print(a_trained_GNN_Model.__class__.__name__, \" Model by \",explainer_name, \" Stability_Score: \", stability_score)\n","\n","\n","\n","\n","\n","\n","\n","\n","            ######################################################################################################################\n","\n","        elif explainer_name == \"PGMExplainer\":\n","            import sys\n","            py_path = '/content/drive/MyDrive/Explainability Methods/PGMExplainer on Graph Classification/Script/'\n","            sys.path.insert(0,py_path)\n","            import pgmexplainer_on_graph_classification_final_format as PGMExplainer_Module\n","            PGMExplainer_Module = reload(PGMExplainer_Module)\n","\n","            explanations = {}\n","            for i in range(num_classes):\n","                explanations[i] = []\n","            PGMExplainer_t1 = perf_counter()\n","            for i in tqdm(range(num_classes)):\n","                for j in range(len(test_dataset)):\n","\n","                    pgmx = PGMExplainer_Module.PGM_Graph_Explainer(GNN_Model=a_trained_GNN_Model, graph=test_dataset[j],\n","                                                                   perturb_feature_list=[None], perturb_mode = \"mean\",\n","                                                                   perturb_indicator = \"abs\")\n","                    pgm_node, p_values, candidate_nodes, dependent_nodes = pgmx.explain(num_samples=len(test_dataset[j].x),\n","                                                                                        noise_offset_percentage=50, top_node=5,\n","                                                                                        p_value_threshold=0.05, class_index=i)\n","                    test_graph = deepcopy(test_dataset[j])\n","                    for k in range(len(test_graph.x)):\n","                        if k not in pgm_node:\n","                            test_graph.x[k] = torch.zeros_like(test_graph.x[k])\n","                    graph_list = []\n","                    for node in test_graph.x:\n","                        graph_list.append(max(node).tolist())\n","                    explanations[i].append(graph_list)\n","            PGMExplainer_timing = perf_counter()-PGMExplainer_t1\n","            average_explanation_time = (PGMExplainer_timing)/(explainer_epoch*len(test_dataset))\n","            print(a_trained_GNN_Model.__class__.__name__, \" Model by \",  explainer_name, \" average_explanation_time: \", average_explanation_time)\n","\n","            #  Fidelity Plus\n","            py_path = '/content/drive/MyDrive/Explainability Methods/Evaluation of Explainability Methods/Script/'\n","            sys.path.insert(0,py_path)\n","            import evaluation_of_xmethods_fidelity_plus as eval_xai_fid_plus\n","            eval_xai_fid_plus = reload(eval_xai_fid_plus)\n","\n","            fid_plus_xmethod_example = eval_xai_fid_plus.evalaution_of_xmethods_fidelity_plus(a_trained_model=a_trained_GNN_Model,\n","                                                                                              test_data=test_dataset)\n","            fid_plus_score = fid_plus_xmethod_example.my_fidelity(saliencies_for_multiple_classes=explanations,\n","                                                                  importance_threshold=self.fid_plus_threshold, style=\"Node\")\n","            print(a_trained_GNN_Model.__class__.__name__, \" Model by \",explainer_name, \" Fid+: \", fid_plus_score)\n","\n","\n","            #  Fidelity Minus\n","            py_path = '/content/drive/MyDrive/Explainability Methods/Evaluation of Explainability Methods/Script/'\n","            sys.path.insert(0,py_path)\n","            import evaluation_of_xmethods_fidelity_minus as eval_xai_fid_minus\n","            eval_xai_fid_minus = reload(eval_xai_fid_minus)\n","\n","            fid_minus_xmethod_example = eval_xai_fid_minus.evalaution_of_xmethods_fidelity_minus(a_trained_model=a_trained_GNN_Model,\n","                                                                                                 test_data=test_dataset)\n","            fid_minus_score = fid_minus_xmethod_example.my_fidelity(saliencies_for_multiple_classes=explanations,\n","                                                                    importance_threshold=self.fid_minus_threshold, style=\"Node\")\n","            print(a_trained_GNN_Model.__class__.__name__, \" Model by \",explainer_name, \" Fid-: \", fid_minus_score)\n","\n","\n","\n","\n","            #  Contrastivity\n","            py_path = '/content/drive/MyDrive/Explainability Methods/Evaluation of Explainability Methods/Script/'\n","            sys.path.insert(0,py_path)\n","            import evaluation_of_xmethods_contrastivity as eval_xai_contrastivity\n","            eval_xai_contrastivity = reload(eval_xai_contrastivity)\n","\n","            contrastivity_xmethod_example = eval_xai_contrastivity.evalaution_of_xmethods_contrastivity(a_trained_model=a_trained_GNN_Model,\n","                                                                                                        test_data=test_dataset)\n","            contrastivity_score = contrastivity_xmethod_example.my_contrastivity(your_dataset=test_dataset,\n","                                                                                 saliencies_for_multiple_classes=explanations,\n","                                                                                 importance_threshold=self.contrastivity_threshold,\n","                                                                                 contrast_coeff=1e+11)\n","            print(a_trained_GNN_Model.__class__.__name__, \" Model by \",explainer_name, \" Contrastivity_Score: \", contrastivity_score)\n","\n","\n","            #  Sparsity\n","            py_path = '/content/drive/MyDrive/Explainability Methods/Evaluation of Explainability Methods/Script/'\n","            sys.path.insert(0,py_path)\n","            import evaluation_of_xmethods_sparsity as eval_xai_sparsity\n","            eval_xai_sparsity = reload(eval_xai_sparsity)\n","\n","            sparsity_xmethod_example = eval_xai_sparsity.evalaution_of_xmethods_sparsity(a_trained_model=a_trained_GNN_Model,\n","                                                                                         test_data=test_dataset)\n","            sparsity_score = sparsity_xmethod_example.my_sparsity(your_dataset=test_dataset,\n","                                                                  saliencies_for_multiple_classes=explanations,\n","                                                                  importance_threshold=self.sparsity_threshold)\n","            print(a_trained_GNN_Model.__class__.__name__, \" Model by \",explainer_name, \" Sparsity_Score: \", sparsity_score)\n","\n","\n","            #  Stability\n","            py_path = '/content/drive/MyDrive/Explainability Methods/Evaluation of Explainability Methods/Script/'\n","            sys.path.insert(0,py_path)\n","            import evaluation_of_xmethods_stability as eval_xai_stability\n","            eval_xai_stability = reload(eval_xai_stability)\n","\n","            stability = eval_xai_stability.evalaution_of_xmethods_stability(a_trained_model=a_trained_GNN_Model,\n","                                                                            test_data=test_dataset)\n","            perturbed_test_dataset = stability.perturb_node_features_of_dataset(test_dataset, self.stability_perturbation_mean,\n","                                                                                self.stability_perturbation_std)\n","\n","            explanations_for_perturbed_data = {}\n","            for i in range(num_classes):\n","                explanations_for_perturbed_data[i] = []\n","\n","            for i in range(num_classes):\n","                for j in range(len(perturbed_test_dataset)):\n","\n","                    pgmx = PGMExplainer_Module.PGM_Graph_Explainer(GNN_Model=a_trained_GNN_Model, graph=perturbed_test_dataset[j],\n","                                                                   perturb_feature_list=[None], perturb_mode = \"mean\",\n","                                                                   perturb_indicator = \"abs\")\n","                    pgm_node, p_values, candidate_nodes, dependent_nodes = pgmx.explain(num_samples=len(perturbed_test_dataset[j].x),\n","                                                                                        noise_offset_percentage=50, top_node=5,\n","                                                                                        p_value_threshold=0.05, class_index=i)\n","                    test_graph = deepcopy(perturbed_test_dataset[j])\n","                    for k in range(len(test_graph.x)):\n","                        if k not in pgm_node:\n","                            test_graph.x[k] = torch.zeros_like(test_graph.x[k])\n","                    graph_list = []\n","                    for node in test_graph.x:\n","                        graph_list.append(max(node).tolist())\n","                    explanations_for_perturbed_data[i].append(graph_list)\n","            stability_score = stability.my_stability(normal_saliencies_for_multiple_classes=explanations,\n","                                            perturbed_saliencies_for_multiple_classes=explanations_for_perturbed_data,\n","                                            top_k_features=2, importance_threshold=self.sparsity_threshold, style='Node')\n","            print(a_trained_GNN_Model.__class__.__name__, \" Model by \",explainer_name, \" Stability_Score: \", stability_score)\n","\n","\n","\n","\n","\n","\n","\n","            ######################################################################################################################\n","\n","        elif explainer_name == \"XGNN\":\n","            import sys\n","            py_path = '/content/drive/MyDrive/Explainability Methods/XGNN on Graph Classification/Script/'\n","            sys.path.insert(0,py_path)\n","            import xgnn_on_graph_classification_final_format as XGNN_Module\n","            XGNN_Module = reload(XGNN_Module)\n","            mutag_max_number_of_nodes=38\n","            explanations = {}\n","            for i in range(num_classes):\n","                explanations[i] = []\n","            xgnn_t1 = perf_counter()\n","            for cls_index in range(num_classes):\n","                xgnn_training = XGNN_Module.XGNN_training(GNN_Model=a_trained_GNN_Model, max_geneneration_iterations=10,\n","                                                          num_node_features=test_dataset[0].x[0].size()[0],\n","                                                          candidate_set_length=test_dataset[0].x[0].size()[0],\n","                                                          max_number_of_nodes=mutag_max_number_of_nodes,\n","                                                          random_start=True, rollout_count=10, class_of_explanation=cls_index,\n","                                                          hyp_for_rollout=1, hyp_for_rules=2, dropout_rate=0.5, explainer_lr=0.01,\n","                                                          b1=0.9, b2=0.999, weight_decay=5e-4)\n","                trained_graph = xgnn_training(explainer_epochs=explainer_epoch)\n","                explanation_graph = Data(x=trained_graph['feat'][:trained_graph['num_nodes']], edge_index=trained_graph['adj'], y=cls_index)\n","\n","                py_path = '/content/drive/MyDrive/Explainability Methods/TopDown Approach on Graph Classification/Script/'\n","                sys.path.insert(0,py_path)\n","                import topdown_approach_for_global_methods as topdown_isomorphism_scoring\n","                topdown_isomorphism_scoring = reload(topdown_isomorphism_scoring)\n","\n","                for i in range(len(test_dataset)):\n","                    common_edges_finder = topdown_isomorphism_scoring.global_explanation_and_samples_intersection(explanation=explanation_graph, input_graph=test_dataset[i])\n","                    intersection_edges = common_edges_finder()\n","                    explanations[cls_index].append(torch.from_numpy(np.array(list(intersection_edges.values()))))\n","            xgnn_timing = perf_counter()-xgnn_t1\n","            average_explanation_time = (xgnn_timing)/(explainer_epoch*len(test_dataset))\n","            print(a_trained_GNN_Model.__class__.__name__, \" Model by \",  explainer_name, \" average_explanation_time: \", average_explanation_time)\n","\n","\n","            #  Fidelity Plus\n","            py_path = '/content/drive/MyDrive/Explainability Methods/Evaluation of Explainability Methods/Script/'\n","            sys.path.insert(0,py_path)\n","            import evaluation_of_xmethods_fidelity_plus as eval_xai_fid_plus\n","            eval_xai_fid_plus = reload(eval_xai_fid_plus)\n","\n","            fid_plus_xmethod_example = eval_xai_fid_plus.evalaution_of_xmethods_fidelity_plus(a_trained_model=a_trained_GNN_Model,\n","                                                                                              test_data=test_dataset)\n","            fid_plus_score = fid_plus_xmethod_example.my_fidelity(saliencies_for_multiple_classes=explanations,\n","                                                                  importance_threshold=self.fid_plus_threshold, style=\"Edge\")\n","            print(a_trained_GNN_Model.__class__.__name__, \" Model by \",explainer_name, \" Fid+: \", fid_plus_score)\n","\n","\n","\n","            #  Fidelity Minus\n","            py_path = '/content/drive/MyDrive/Explainability Methods/Evaluation of Explainability Methods/Script/'\n","            sys.path.insert(0,py_path)\n","            import evaluation_of_xmethods_fidelity_minus as eval_xai_fid_minus\n","            eval_xai_fid_minus = reload(eval_xai_fid_minus)\n","\n","            fid_minus_xmethod_example = eval_xai_fid_minus.evalaution_of_xmethods_fidelity_minus(a_trained_model=a_trained_GNN_Model,\n","                                                                                                 test_data=test_dataset)\n","            fid_minus_score = fid_minus_xmethod_example.my_fidelity(saliencies_for_multiple_classes=explanations,\n","                                                                    importance_threshold=self.fid_minus_threshold, style=\"Edge\")\n","            print(a_trained_GNN_Model.__class__.__name__, \" Model by \",explainer_name, \" Fid-: \", fid_minus_score)\n","\n","\n","\n","            #  Contrastivity\n","            py_path = '/content/drive/MyDrive/Explainability Methods/Evaluation of Explainability Methods/Script/'\n","            sys.path.insert(0,py_path)\n","            import evaluation_of_xmethods_contrastivity as eval_xai_contrastivity\n","            eval_xai_contrastivity = reload(eval_xai_contrastivity)\n","\n","            contrastivity_xmethod_example = eval_xai_contrastivity.evalaution_of_xmethods_contrastivity(a_trained_model=a_trained_GNN_Model,\n","                                                                                                        test_data=test_dataset)\n","            contrastivity_score = contrastivity_xmethod_example.my_contrastivity(your_dataset=test_dataset,\n","                                                                                 saliencies_for_multiple_classes=explanations,\n","                                                                                 importance_threshold=self.contrastivity_threshold,\n","                                                                                 contrast_coeff=1e+11)\n","            print(a_trained_GNN_Model.__class__.__name__, \" Model by \",explainer_name, \" Contrastivity_Score: \", contrastivity_score)\n","\n","\n","\n","\n","            #  Sparsity\n","            py_path = '/content/drive/MyDrive/Explainability Methods/Evaluation of Explainability Methods/Script/'\n","            sys.path.insert(0,py_path)\n","            import evaluation_of_xmethods_sparsity as eval_xai_sparsity\n","            eval_xai_sparsity = reload(eval_xai_sparsity)\n","\n","            sparsity_xmethod_example = eval_xai_sparsity.evalaution_of_xmethods_sparsity(a_trained_model=a_trained_GNN_Model,\n","                                                                                         test_data=test_dataset)\n","            sparsity_score = sparsity_xmethod_example.my_sparsity(your_dataset=test_dataset,\n","                                                                  saliencies_for_multiple_classes=explanations,\n","                                                                  importance_threshold=self.sparsity_threshold)\n","            print(a_trained_GNN_Model.__class__.__name__, \" Model by \",explainer_name, \" Sparsity_Score: \", sparsity_score)\n","\n","\n","\n","\n","\n","            #  Stability\n","            py_path = '/content/drive/MyDrive/Explainability Methods/Evaluation of Explainability Methods/Script/'\n","            sys.path.insert(0,py_path)\n","            import evaluation_of_xmethods_stability as eval_xai_stability\n","            eval_xai_stability = reload(eval_xai_stability)\n","\n","            stability = eval_xai_stability.evalaution_of_xmethods_stability(a_trained_model=a_trained_GNN_Model,\n","                                                                            test_data=test_dataset)\n","            perturbed_test_dataset = stability.perturb_node_features_of_dataset(test_dataset, self.stability_perturbation_mean,\n","                                                                                self.stability_perturbation_std)\n","\n","            explanations_for_perturbed_data = {}\n","            for i in range(num_classes):\n","                explanations_for_perturbed_data[i] = []\n","\n","            for cls_index in range(num_classes):\n","                xgnn_training = XGNN_Module.XGNN_training(GNN_Model=a_trained_GNN_Model, max_geneneration_iterations=10,\n","                                                          num_node_features=test_dataset[0].x[0].size()[0],\n","                                                          candidate_set_length=test_dataset[0].x[0].size()[0],\n","                                                          max_number_of_nodes=mutag_max_number_of_nodes,\n","                                                          random_start=True, rollout_count=10, class_of_explanation=cls_index,\n","                                                          hyp_for_rollout=1, hyp_for_rules=2, dropout_rate=0.5, explainer_lr=0.01,\n","                                                          b1=0.9, b2=0.999, weight_decay=5e-4)\n","                perturbed_trained_graph = xgnn_training(explainer_epochs=explainer_epoch)\n","                per_explanation_graph = Data(x=perturbed_trained_graph['feat'][:perturbed_trained_graph['num_nodes']], edge_index=perturbed_trained_graph['adj'], y=cls_index)\n","                py_path = '/content/drive/MyDrive/Explainability Methods/Evaluation of Model-level and Global Methods on Graph Classification/Script/'\n","                sys.path.insert(0,py_path)\n","                import topdown_approach_for_global_methods as topdown_isomorphism_scoring\n","                topdown_isomorphism_scoring = reload(topdown_isomorphism_scoring)\n","\n","                for i in range(len(perturbed_test_dataset)):\n","                    common_edges_finder = topdown_isomorphism_scoring.global_explanation_and_samples_intersection(explanation=per_explanation_graph, input_graph=perturbed_test_dataset[i])\n","                    perturbed_intersection_edges = common_edges_finder()\n","                    explanations_for_perturbed_data[cls_index].append(torch.from_numpy(np.array(list(perturbed_intersection_edges.values()))))\n","\n","            stability_score = stability.my_stability(normal_saliencies_for_multiple_classes=explanations,\n","                                                     perturbed_saliencies_for_multiple_classes=explanations_for_perturbed_data,\n","                                                     top_k_features=2, importance_threshold=self.stability_threshold, style='Edge')\n","            print(a_trained_GNN_Model.__class__.__name__, \" Model by \",explainer_name, \" Stability_Score: \", stability_score)\n","\n","\n","\n","            # import pickle\n","            # file_path = '/content/drive/My Drive/Explainability Methods/'+str(Explainability_name)+' on ' + str(Task_name) + '/Experimental Results/Edge_Limit/' + File_Name + 'Limited_Edge_Explanation_Class_' + str(class_of_explanation)\n","\n","            # with open(file_path, 'wb') as file:\n","            #     pickle.dump(generated_graph, file)\n","            ######################################################################################################################\n","\n","        elif explainer_name == \"GNNInterpreter\":\n","            import sys\n","            py_path = '/content/drive/MyDrive/Explainability Methods/GNNInterpreter on Graph Classification/Script/'\n","            sys.path.insert(0,py_path)\n","            import gnninterpreter_on_graph_classification_final_format as GNNInterpreter_Module\n","\n","\n","\n","\n","            explanations_size = {0:9, 1:7, 2:8}\n","            explanations = {}\n","            for i in range(num_classes):\n","                explanations[i] = []\n","\n","            generations = {}\n","            for i in range(num_classes):\n","                generations[i] = []\n","\n","            mean_of_node_embeddings = {}\n","            for i in range(num_classes):\n","                mean_of_node_embeddings[i] = torch.mean(torch.cat([graph.x for graph in test_dataset if graph.y == i], dim=0), axis=0)\n","\n","            gnninterpreter_t1 = perf_counter()\n","            for cls_index in range(num_classes):\n","                losses_aggregated = GNNInterpreter_Module.losses_aggregation(\n","                    [\n","                        dict(key=\"continuous_generated_embeddings\", criterion=GNNInterpreter_Module.Embedding_Loss_by_Cosine_Similarity(target_embedding=mean_of_node_embeddings[cls_index]), weight=10),\n","                        dict(key=\"discrete_generated_embeddings\", criterion=GNNInterpreter_Module.Embedding_Loss_by_Cosine_Similarity(target_embedding=mean_of_node_embeddings[cls_index]), weight=10),\n","                        dict(key=\"logits_continuous\", criterion=GNNInterpreter_Module.Explanation_Class_Score(class_idx=cls_index, mode='maximize'), weight=1),\n","                        dict(key=\"logits_continuous\", criterion=GNNInterpreter_Module.MeanPenalty(), weight=0),\n","                        dict(key=\"logits_discrete\", criterion=GNNInterpreter_Module.Explanation_Class_Score(class_idx=cls_index, mode='maximize'), weight=1),\n","                        dict(key=\"logits_discrete\", criterion=GNNInterpreter_Module.MeanPenalty(), weight=0),\n","                        dict(key=\"edge_parameters\", criterion=GNNInterpreter_Module.NormPenalty(order=1), weight=1),\n","                        dict(key=\"edge_parameters\", criterion=GNNInterpreter_Module.NormPenalty(order=2), weight=1),\n","                        dict(key=\"node_feature_parameters\", criterion=GNNInterpreter_Module.NormPenalty(order=1), weight=0),\n","                        dict(key=\"node_feature_parameters\", criterion=GNNInterpreter_Module.NormPenalty(order=2), weight=0),\n","                        # dict(key=\"edge_feature_parameters\", criterion=NormPenalty(order=1), weight=0),\n","                        # dict(key=\"edge_feature_parameters\", criterion=NormPenalty(order=2), weight=0),\n","                        dict(key=\"edge_parameters_pairs_of_nodes\", criterion=GNNInterpreter_Module.KLDivergencePenalty(binary=True), weight=0)\n","                        ]\n","                    )\n","                generator = GNNInterpreter_Module.Graph_Generator(max_nodes=explanations_size[cls_index], num_node_classes=test_dataset[0].x[0].size()[0],\n","                                                                  num_edge_classes=None, nodes=None, edges=None, Graph=None,\n","                                                                  learning_node_feat=True, learning_edge_feat=False, temperature=0.15)\n","\n","                generations[cls_index] = GNNInterpreter_Module.Generation_Manager_wrt_Classes(generator=generator,\n","                                                                                              discriminator=a_trained_GNN_Model,\n","                                                                                              aggregate_losses=losses_aggregated,\n","                                                                                              optimizer=(o := torch.optim.SGD(generator.parameters(), lr=1)),\n","                                                                                              dataset=test_dataset,\n","                                                                                              budget_penalty=GNNInterpreter_Module.BudgetPenalty_for_second_regularization(budget=10, order=2, beta=1),\n","                                                                                              targeted_probabilities={cls_index: (0.9, 1)},\n","                                                                                              batch_size_for_same_sized_graphs=1)\n","\n","                continuous_generated_graph, discrete_generated_graph = generations[cls_index].train(explainer_epoch)\n","                # explanations[cls_index] = discrete_generated_graph\n","                #print(\"discrete_generated_graph: \", discrete_generated_graph)\n","\n","                py_path = '/content/drive/MyDrive/Explainability Methods/TopDown Approach on Graph Classification/Script/'\n","                sys.path.insert(0,py_path)\n","                import topdown_approach_for_global_methods as topdown_isomorphism_scoring\n","                topdown_isomorphism_scoring = reload(topdown_isomorphism_scoring)\n","\n","                for i in range(len(test_dataset)):\n","                    common_edges_finder = topdown_isomorphism_scoring.global_explanation_and_samples_intersection(explanation=discrete_generated_graph, input_graph=test_dataset[i])\n","                    intersection_edges = common_edges_finder()\n","                    explanations[cls_index].append(torch.from_numpy(np.array(list(intersection_edges.values()))))\n","\n","            gnninterpreter_timing = perf_counter()-gnninterpreter_t1\n","            average_explanation_time = (gnninterpreter_timing)/(explainer_epoch*len(test_dataset))\n","            print(a_trained_GNN_Model.__class__.__name__, \" Model by \",  explainer_name, \" average_explanation_time: \", average_explanation_time)\n","\n","\n","            #  Fidelity Plus\n","            py_path = '/content/drive/MyDrive/Explainability Methods/Evaluation of Explainability Methods/Script/'\n","            sys.path.insert(0,py_path)\n","            import evaluation_of_xmethods_fidelity_plus as eval_xai_fid_plus\n","            eval_xai_fid_plus = reload(eval_xai_fid_plus)\n","\n","            fid_plus_xmethod_example = eval_xai_fid_plus.evalaution_of_xmethods_fidelity_plus(a_trained_model=a_trained_GNN_Model,\n","                                                                                              test_data=test_dataset)\n","            fid_plus_score = fid_plus_xmethod_example.my_fidelity(saliencies_for_multiple_classes=explanations,\n","                                                                  importance_threshold=self.fid_plus_threshold, style=\"Edge\")\n","            print(a_trained_GNN_Model.__class__.__name__, \" Model by \",explainer_name, \" Fid+: \", fid_plus_score)\n","\n","\n","\n","            #  Fidelity Minus\n","            py_path = '/content/drive/MyDrive/Explainability Methods/Evaluation of Explainability Methods/Script/'\n","            sys.path.insert(0,py_path)\n","            import evaluation_of_xmethods_fidelity_minus as eval_xai_fid_minus\n","            eval_xai_fid_minus = reload(eval_xai_fid_minus)\n","\n","            fid_minus_xmethod_example = eval_xai_fid_minus.evalaution_of_xmethods_fidelity_minus(a_trained_model=a_trained_GNN_Model,\n","                                                                                                 test_data=test_dataset)\n","            fid_minus_score = fid_minus_xmethod_example.my_fidelity(saliencies_for_multiple_classes=explanations,\n","                                                                    importance_threshold=self.fid_minus_threshold, style=\"Edge\")\n","            print(a_trained_GNN_Model.__class__.__name__, \" Model by \",explainer_name, \" Fid-: \", fid_minus_score)\n","\n","\n","\n","            #  Contrastivity\n","            py_path = '/content/drive/MyDrive/Explainability Methods/Evaluation of Explainability Methods/Script/'\n","            sys.path.insert(0,py_path)\n","            import evaluation_of_xmethods_contrastivity as eval_xai_contrastivity\n","            eval_xai_contrastivity = reload(eval_xai_contrastivity)\n","\n","            contrastivity_xmethod_example = eval_xai_contrastivity.evalaution_of_xmethods_contrastivity(a_trained_model=a_trained_GNN_Model,\n","                                                                                                        test_data=test_dataset)\n","            contrastivity_score = contrastivity_xmethod_example.my_contrastivity(your_dataset=test_dataset,\n","                                                                                 saliencies_for_multiple_classes=explanations,\n","                                                                                 importance_threshold=self.contrastivity_threshold,\n","                                                                                 contrast_coeff=1e+11)\n","            print(a_trained_GNN_Model.__class__.__name__, \" Model by \",explainer_name, \" Contrastivity_Score: \", contrastivity_score)\n","\n","\n","\n","\n","            #  Sparsity\n","            py_path = '/content/drive/MyDrive/Explainability Methods/Evaluation of Explainability Methods/Script/'\n","            sys.path.insert(0,py_path)\n","            import evaluation_of_xmethods_sparsity as eval_xai_sparsity\n","            eval_xai_sparsity = reload(eval_xai_sparsity)\n","\n","            sparsity_xmethod_example = eval_xai_sparsity.evalaution_of_xmethods_sparsity(a_trained_model=a_trained_GNN_Model,\n","                                                                                         test_data=test_dataset)\n","            sparsity_score = sparsity_xmethod_example.my_sparsity(your_dataset=test_dataset,\n","                                                                  saliencies_for_multiple_classes=explanations,\n","                                                                  importance_threshold=self.sparsity_threshold)\n","            print(a_trained_GNN_Model.__class__.__name__, \" Model by \",explainer_name, \" Sparsity_Score: \", sparsity_score)\n","\n","\n","\n","\n","            #  Stability\n","            py_path = '/content/drive/MyDrive/Explainability Methods/Evaluation of Explainability Methods/Script/'\n","            sys.path.insert(0,py_path)\n","            import evaluation_of_xmethods_stability as eval_xai_stability\n","            eval_xai_stability = reload(eval_xai_stability)\n","\n","            generations_by_perturbed_data = {}\n","            for i in range(num_classes):\n","                generations_by_perturbed_data[i] = []\n","\n","            stability = eval_xai_stability.evalaution_of_xmethods_stability(a_trained_model=a_trained_GNN_Model,\n","                                                                            test_data=test_dataset)\n","            perturbed_test_dataset = stability.perturb_node_features_of_dataset(test_dataset, self.stability_perturbation_mean,\n","                                                                                self.stability_perturbation_std)\n","\n","            explanations_for_perturbed_data = {}\n","            for i in range(num_classes):\n","                explanations_for_perturbed_data[i] = []\n","\n","            for cls_index in range(num_classes):\n","                losses_aggregated = GNNInterpreter_Module.losses_aggregation(\n","                    [\n","                        dict(key=\"continuous_generated_embeddings\", criterion=GNNInterpreter_Module.Embedding_Loss_by_Cosine_Similarity(target_embedding=mean_of_node_embeddings[cls_index]), weight=10),\n","                        dict(key=\"discrete_generated_embeddings\", criterion=GNNInterpreter_Module.Embedding_Loss_by_Cosine_Similarity(target_embedding=mean_of_node_embeddings[cls_index]), weight=10),\n","                        dict(key=\"logits_continuous\", criterion=GNNInterpreter_Module.Explanation_Class_Score(class_idx=cls_index, mode='maximize'), weight=1),\n","                        dict(key=\"logits_continuous\", criterion=GNNInterpreter_Module.MeanPenalty(), weight=0),\n","                        dict(key=\"logits_discrete\", criterion=GNNInterpreter_Module.Explanation_Class_Score(class_idx=cls_index, mode='maximize'), weight=1),\n","                        dict(key=\"logits_discrete\", criterion=GNNInterpreter_Module.MeanPenalty(), weight=0),\n","                        dict(key=\"edge_parameters\", criterion=GNNInterpreter_Module.NormPenalty(order=1), weight=1),\n","                        dict(key=\"edge_parameters\", criterion=GNNInterpreter_Module.NormPenalty(order=2), weight=1),\n","                        dict(key=\"node_feature_parameters\", criterion=GNNInterpreter_Module.NormPenalty(order=1), weight=0),\n","                        dict(key=\"node_feature_parameters\", criterion=GNNInterpreter_Module.NormPenalty(order=2), weight=0),\n","                        # dict(key=\"edge_feature_parameters\", criterion=NormPenalty(order=1), weight=0),\n","                        # dict(key=\"edge_feature_parameters\", criterion=NormPenalty(order=2), weight=0),\n","                        dict(key=\"edge_parameters_pairs_of_nodes\", criterion=GNNInterpreter_Module.KLDivergencePenalty(binary=True), weight=0)\n","                        ]\n","                    )\n","                generator = GNNInterpreter_Module.Graph_Generator(max_nodes=explanations_size[cls_index],\n","                                                                  num_node_classes=test_dataset[0].x[0].size()[0],\n","                                                                  num_edge_classes=4, nodes=None, edges=None, Graph=None,\n","                                                                  learning_node_feat=True, learning_edge_feat=False, temperature=0.15)\n","\n","                generations_by_perturbed_data[cls_index] = GNNInterpreter_Module.Generation_Manager_wrt_Classes(generator=generator,\n","                                                                                              discriminator=a_trained_GNN_Model,\n","                                                                                              aggregate_losses=losses_aggregated,\n","                                                                                              optimizer=(o := torch.optim.SGD(generator.parameters(), lr=1)),\n","                                                                                              dataset=perturbed_test_dataset,\n","                                                                                              budget_penalty=GNNInterpreter_Module.BudgetPenalty_for_second_regularization(budget=10, order=2, beta=1),\n","                                                                                              targeted_probabilities={cls_index: (0.9, 1)},\n","                                                                                              batch_size_for_same_sized_graphs=1)\n","\n","                continuous_generated_graph, discrete_generated_graph = generations_by_perturbed_data[cls_index].train(explainer_epoch)\n","\n","                py_path = '/content/drive/MyDrive/Explainability Methods/Evaluation of Model-level and Global Methods on Graph Classification/Script/'\n","                sys.path.insert(0,py_path)\n","                import topdown_approach_for_global_methods as topdown_isomorphism_scoring\n","                topdown_isomorphism_scoring = reload(topdown_isomorphism_scoring)\n","\n","                for i in range(len(perturbed_test_dataset)):\n","                    common_edges_finder = topdown_isomorphism_scoring.global_explanation_and_samples_intersection(explanation=discrete_generated_graph, input_graph=perturbed_test_dataset[i])\n","                    intersection_edges = common_edges_finder()\n","                    explanations_for_perturbed_data[cls_index].append(torch.from_numpy(np.array(list(intersection_edges.values()))))\n","\n","            stability_score = stability.my_stability(normal_saliencies_for_multiple_classes=explanations,\n","                                                     perturbed_saliencies_for_multiple_classes=explanations_for_perturbed_data,\n","                                                     top_k_features=2, importance_threshold=self.stability_threshold, style='Edge')\n","            print(a_trained_GNN_Model.__class__.__name__, \" Model by \",explainer_name, \" Stability_Score: \", stability_score)\n","\n","\n","            ######################################################################################################################\n","\n","# GCN_plus_GAP - DGCNN - DIFFPOOL - GIN\n","# GNNExplainer - PGExplainer - GraphMask - SubGraphX - CF2 - PGMExplainer - XGNN - GNNInterpreter\n","# Datasets name: MUTAG, Fake\n","just_call_me = one_model_one_explainer(dataset_name=\"NCI1\")\n","just_call_me(a_trained_GNN_Model=GNN_Model, explainer_name=\"SubGraphX\", test_dataset=nci1_test_dataset, num_classes=2,\n","             explainer_epoch=200, explainer_learning_rate=0.001)"],"metadata":{"id":"V6gdXm6Wwg4g","executionInfo":{"status":"error","timestamp":1716218364143,"user_tz":-120,"elapsed":17596,"user":{"displayName":"Ehsan Mobaraki","userId":"06314958514195310048"}},"colab":{"base_uri":"https://localhost:8080/","height":435},"outputId":"946eebf4-d695-45e7-9bdc-2b2f4120d443"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Name of the Dataset:  NCI1\n","Number of Classes:  2\n","Test Dataset Size:  822\n","Explainer:  SubGraphX\n","Explainer Training Epochs:  200\n","Explainer Learning Rate:  0.001\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-52-9b840239e7e2>\u001b[0m in \u001b[0;36m<cell line: 1163>\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1161\u001b[0m \u001b[0;31m# Datasets name: MUTAG, Fake\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1162\u001b[0m \u001b[0mjust_call_me\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mone_model_one_explainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"NCI1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1163\u001b[0;31m just_call_me(a_trained_GNN_Model=GNN_Model, explainer_name=\"SubGraphX\", test_dataset=nci1_test_dataset, num_classes=2,\n\u001b[0m\u001b[1;32m   1164\u001b[0m              explainer_epoch=200, explainer_learning_rate=0.001)\n","\u001b[0;32m<ipython-input-52-9b840239e7e2>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, a_trained_GNN_Model, explainer_name, test_dataset, num_classes, explainer_epoch, explainer_learning_rate)\u001b[0m\n\u001b[1;32m    429\u001b[0m                                                                            \u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m                                                                            Model_Name=GNN_Model.__class__.__name__)\n\u001b[0;32m--> 431\u001b[0;31m                 \u001b[0mwhole_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubgx_out_the_fly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    432\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m                 \u001b[0mexplanations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/MyDrive/Explainability Methods/SubGraphX on Graph Classification/Script/subgraphx_on_graph_classification_final_format.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, test_dataset)\u001b[0m\n\u001b[1;32m    647\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 649\u001b[0;31m         \u001b[0mwhole_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreconfig_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExplainability_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTask_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    650\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwhole_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    651\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/MyDrive/Explainability Methods/SubGraphX on Graph Classification/Script/subgraphx_on_graph_classification_final_format.py\u001b[0m in \u001b[0;36mreconfig_data\u001b[0;34m(self, your_dataset, Explainability_name, Task_name)\u001b[0m\n\u001b[1;32m    628\u001b[0m             \u001b[0mwhole_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 630\u001b[0;31m                 \u001b[0mmasked_data_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaskout_data_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasked_pred_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaskout_pred_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mExplainability_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTask_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    631\u001b[0m                 \u001b[0mwhole_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m                 \u001b[0mwhole_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"important_nodes\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmasked_data_class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/MyDrive/Explainability Methods/SubGraphX on Graph Classification/Script/subgraphx_on_graph_classification_final_format.py\u001b[0m in \u001b[0;36mload_data\u001b[0;34m(self, Explainability_name, Task_name, loading_graph_index, category)\u001b[0m\n\u001b[1;32m    638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExplainability_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTask_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloading_graph_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 640\u001b[0;31m         \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/My Drive/Explainability Methods/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExplainability_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" on \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTask_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/Model/temp/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"SubGraphX_Explainer_\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModel_Name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"_graph_\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"important_for_class_\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcategory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"_\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"_\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloading_graph_index\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    641\u001b[0m         \u001b[0mmask_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mask_data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m         \u001b[0mmaskout_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'maskout_data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m    997\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    998\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 999\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1000\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1001\u001b[0m             \u001b[0;31m# If we want to actually tail call to torch.jit.load, we need to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_is_zipfile\u001b[0;34m(f)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;31m# Read the first few bytes and match against the ZIP file signature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0mlocal_header_magic_number\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mb'PK\\x03\\x04'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m     \u001b[0mread_bytes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocal_header_magic_number\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m     \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mread_bytes\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlocal_header_magic_number\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","source":["### **Fid+**"],"metadata":{"id":"AD5rtmGThXDT"}},{"cell_type":"code","source":["class evalaution_of_xmethods_fidelity_plus(object):\n","    def __init__(self, a_trained_model, test_data):\n","        super(evalaution_of_xmethods_fidelity_plus, self).__init__()\n","        self.a_trained_model = a_trained_model\n","        self.test_data = test_data\n","\n","    def is_salient(self, score, importance_threshold):\n","        if importance_threshold == score == 0:\n","            return True\n","        if importance_threshold == score == 1:\n","            return False\n","        if importance_threshold < score:\n","            return True\n","        else:\n","            return False\n","\n","    def Compute_ROC_AUC(self, your_model, your_dataset):\n","        preds = []\n","        reals = []\n","\n","        your_model.eval()\n","        for batched_data in your_dataset:\n","            if your_model.__class__.__name__ == \"GCN_plus_GAP\":\n","                Output_of_Hidden_Layers, pooling_layer_output, ffn_output, prediction = your_model(batched_data)\n","            elif your_model.__class__.__name__ == \"DGCNN_Model\":\n","                final_GNN_layer_output, sortpooled_embedings, output_conv1d_1, maxpooled_output_conv1d_1, output_conv1d_2, to_dense, output_h1, dropout_output_h1, output_h2, prediction = your_model(batched_data, None)\n","            elif your_model.__class__.__name__ == \"DIFFPOOL_Model\":\n","                concatination_list_of_poolings, prediction_output_not_softed, prediction = your_model(batched_data, None)\n","            elif your_model.__class__.__name__ == \"GIN_Model\":\n","                mlps_output_embeds, mlp_outputs_globalSUMpooled, lin1_output, lin1_output_dropouted, lin2_output, prediction = your_model(batched_data, None)\n","            else:\n","                print(\"Model Name is not valid.\")\n","\n","            # preds.extend(torch.argmax(prediction, dim=1).tolist())\n","            preds.extend(prediction.tolist())\n","            reals.extend(batched_data.y.tolist())\n","\n","        is_binary = len(np.unique(reals)) == 2\n","        if is_binary:\n","            preds = (np.array(preds)[:, 1] > 0.5).astype(int)\n","\n","        auc_roc = metrics.roc_auc_score(np.array(reals), np.array(preds), multi_class=\"ovr\", average=\"micro\")\n","\n","        return auc_roc\n","\n","    def drop_important_node_features(self, your_dataset, importance_threshold, attribution_scores):\n","        occluded_GNNgraph_list = []\n","        for i in range(len(attribution_scores)):\n","            sample_graph = deepcopy(your_dataset[i])\n","            for j in range(len(attribution_scores[i])):\n","                for k in range(len(attribution_scores[i][j])):\n","                    if self.is_salient(attribution_scores[i][j][k], importance_threshold):\n","                        sample_graph.x[j][k] = 0\n","            occluded_GNNgraph_list.append(sample_graph)\n","        return occluded_GNNgraph_list\n","\n","    def drop_important_nodes(self, your_dataset, importance_threshold, attribution_scores):\n","        occluded_GNNgraph_list = []\n","        for i in range(len(attribution_scores)):\n","            sample_graph = deepcopy(your_dataset[i])\n","            for j in range(len(attribution_scores[i])):\n","                if self.is_salient(attribution_scores[i][j], importance_threshold):\n","                    sample_graph.x[j][:] = 0\n","            occluded_GNNgraph_list.append(sample_graph)\n","        return occluded_GNNgraph_list\n","\n","    def Fidelity_node_features(self, your_dataset, generated_saliency_maps, importance_threshold):\n","\n","        auc_roc_before_droping_important_nodes = self.Compute_ROC_AUC(self.a_trained_model, your_dataset)\n","        new_graph_dataset = self.drop_important_node_features(your_dataset, importance_threshold, generated_saliency_maps)\n","        auc_roc_after_droping_important_nodes = self.Compute_ROC_AUC(self.a_trained_model, new_graph_dataset)\n","\n","        return auc_roc_before_droping_important_nodes - auc_roc_after_droping_important_nodes\n","\n","    def Fidelity_node(self, your_dataset, generated_saliency_maps, importance_threshold):\n","\n","        auc_roc_before_droping_important_nodes = self.Compute_ROC_AUC(self.a_trained_model, your_dataset)\n","        new_graph_dataset = self.drop_important_nodes(your_dataset, importance_threshold, generated_saliency_maps)\n","        auc_roc_after_droping_important_nodes = self.Compute_ROC_AUC(self.a_trained_model, new_graph_dataset)\n","\n","        return auc_roc_before_droping_important_nodes - auc_roc_after_droping_important_nodes\n","\n","    def drop_important_edges(self, your_dataset, importance_threshold, Edge_Masks_Dropped):\n","        attribution_scores = []\n","        Edge_Masks_original = []\n","        Edge_Masks_Dropped_copy = []\n","        for tensor in Edge_Masks_Dropped:\n","            Edge_Masks_original.append(deepcopy(tensor.detach()))\n","            Edge_Masks_Dropped_copy.append(deepcopy(tensor.detach()))\n","\n","        for edge_mask in Edge_Masks_Dropped_copy:\n","\n","            importance_indices = edge_mask > importance_threshold * (max(edge_mask)-min(edge_mask))\n","            edge_mask[importance_indices] = 0\n","\n","        return Edge_Masks_original, Edge_Masks_Dropped_copy\n","\n","    def Fidelity_edge(self, your_dataset, generated_saliency_maps, importance_threshold):\n","\n","        auc_roc_before_droping_important_nodes = self.Compute_ROC_AUC_edge(self.a_trained_model, your_dataset, False)\n","        Edge_Masks_original, Edge_Masks_Dropped = self.drop_important_edges(your_dataset, importance_threshold, generated_saliency_maps)\n","        auc_roc_after_droping_important_nodes = self.Compute_ROC_AUC_edge(self.a_trained_model, your_dataset, Edge_Masks_Dropped)\n","\n","        return auc_roc_before_droping_important_nodes - auc_roc_after_droping_important_nodes\n","\n","    def normalize_saliency_node_features_based(self, sal_maps):\n","        Graphs_new_gradients = []\n","        for graph_grads in sal_maps:\n","            new_gradients = []\n","            for node_grads in graph_grads:\n","                node_gradients = []\n","                for dim in node_grads:\n","                    node_gradients.append((dim-min(node_grads))/(max(node_grads)-min(node_grads)))\n","                new_gradients.append(node_gradients)\n","            Graphs_new_gradients.append(new_gradients)\n","        return Graphs_new_gradients\n","\n","    def normalize_saliency_node_based(self, sal_maps):\n","        Graphs_new_gradients = []\n","        for graph_grads in sal_maps:\n","            new_gradients = []\n","            for node_grads in graph_grads:\n","                val = (node_grads-min(graph_grads))/(max(graph_grads)-min(graph_grads)) if (max(graph_grads)-min(graph_grads)) != 0 else 0\n","                new_gradients.append(val)\n","            Graphs_new_gradients.append(new_gradients)\n","        return Graphs_new_gradients\n","\n","    def clear_masks(self, model):\n","\n","        for module in model.modules():\n","            if isinstance(module, MessagePassing):\n","                module.explain = False\n","                module._edge_mask = None\n","                module._loop_mask = None\n","                module._apply_sigmoid = True\n","        return module\n","\n","    def apply_masks(self, model, mask, edge_index, apply_sigmoid):\n","        loop_mask = edge_index[0] != edge_index[1]\n","\n","        for module in model.modules():\n","            if isinstance(module, MessagePassing):\n","\n","                if (not isinstance(mask, Parameter)\n","                        and '_edge_mask' in module._parameters):\n","                    mask = Parameter(mask)\n","\n","                module.explain = True\n","                module._edge_mask = mask\n","                module._loop_mask = loop_mask\n","                module._apply_sigmoid = apply_sigmoid\n","\n","    def Compute_ROC_AUC_edge(self, your_model, your_dataset, masked):\n","        preds = []\n","        reals = []\n","\n","        if masked == False:\n","            your_model.eval()\n","            for batched_data in your_dataset:\n","                reals.extend(batched_data.y.tolist())\n","                if your_model.__class__.__name__ == \"GCN_plus_GAP\":\n","                    Output_of_Hidden_Layers, pooling_layer_output, ffn_output, soft = your_model(batched_data)\n","                    preds.extend(soft.tolist())\n","                elif your_model.__class__.__name__ == \"DGCNN_Model\":\n","                    final_GNN_layer_output, sortpooled_embedings, output_conv1d_1, maxpooled_output_conv1d_1, output_conv1d_2, to_dense, output_h1, dropout_output_h1, output_h2, soft = your_model(batched_data, None)\n","                    preds.extend(soft.tolist())\n","                elif your_model.__class__.__name__ == \"DIFFPOOL_Model\":\n","                    concatination_list_of_poolings, prediction_output_not_softed, soft = your_model(batched_data, None)\n","                    preds.extend(soft.tolist())\n","                elif your_model.__class__.__name__ == \"GIN_Model\":\n","                    mlps_output_embeds, mlp_outputs_globalSUMpooled, lin1_output, lin1_output_dropouted, lin2_output, soft = your_model(batched_data, None)\n","                    preds.extend(soft.tolist())\n","        else:\n","            your_model.eval()\n","            for batched_data, edge_mask in zip(your_dataset, masked):\n","                reals.extend(batched_data.y.tolist())\n","                if your_model.__class__.__name__ == \"GCN_plus_GAP\":\n","                    self.apply_masks(your_model, edge_mask, batched_data.edge_index, apply_sigmoid=True)\n","                    Output_of_Hidden_Layers, pooling_layer_output, ffn_output, soft = your_model(batched_data)\n","                    preds.extend(soft.tolist())\n","                    self.clear_masks(your_model)\n","                elif your_model.__class__.__name__ == \"DGCNN_Model\":\n","                    final_GNN_layer_output, sortpooled_embedings, output_conv1d_1, maxpooled_output_conv1d_1, output_conv1d_2, to_dense, output_h1, dropout_output_h1, output_h2, soft = your_model(batched_data, edge_mask.tolist())\n","                    preds.extend(soft.tolist())\n","                elif your_model.__class__.__name__ == \"DIFFPOOL_Model\":\n","                    concatination_list_of_poolings, prediction_output_not_softed, soft = your_model(batched_data, edge_mask.tolist())\n","                    preds.extend(soft.tolist())\n","                elif your_model.__class__.__name__ == \"GIN_Model\":\n","                    mlps_output_embeds, mlp_outputs_globalSUMpooled, lin1_output, lin1_output_dropouted, lin2_output, soft = your_model(batched_data, edge_mask.tolist())\n","                    preds.extend(soft.tolist())\n","\n","\n","        # preds = torch.cat(preds)\n","        # preds, max_idxs = torch.max(preds[:], dim=1)\n","        # print(\"preds: \", preds)\n","        is_binary = len(np.unique(reals)) == 2\n","        if is_binary:\n","            preds = (np.array(preds)[:, 1] > 0.5).astype(int)\n","        roc_auc = metrics.roc_auc_score(np.array(reals), np.array(preds), multi_class=\"ovr\", average=\"micro\")\n","        return roc_auc\n","\n","    def normalize_saliency_edge_based(self, saliency_maps):\n","        Graphs_new_gradients = []\n","\n","        for graph_grads in saliency_maps:\n","            for edge_grads in graph_grads:\n","                edge_grads = (edge_grads-torch.min(graph_grads))/(torch.max(graph_grads)-torch.min(graph_grads)) if (torch.max(graph_grads)-torch.min(graph_grads)) != 0 else 0\n","            Graphs_new_gradients.append(graph_grads)\n","\n","        return Graphs_new_gradients\n","\n","    def my_fidelity(self, saliencies_for_multiple_classes, importance_threshold, style):\n","        if style == \"Node Feature\":\n","            try:\n","                fid_scores = []\n","                for key, value in saliencies_for_multiple_classes.items():\n","                    saliency_map = self.normalize_saliency_node_features_based(value)\n","                    fid_score = self.Fidelity_node_features(self.test_data, saliency_map, importance_threshold)\n","                    fid_scores.append(fid_score)\n","\n","                return mean(fid_scores)\n","            except:\n","                print(\"attributions are not in appropriate shape\")\n","\n","        elif style == \"Node\":\n","            try:\n","                fid_scores = []\n","                for key, value in saliencies_for_multiple_classes.items():\n","                    saliency_map = self.normalize_saliency_node_based(value)\n","                    fid_score = self.Fidelity_node(self.test_data, saliency_map, importance_threshold)\n","                    fid_scores.append(fid_score)\n","                return mean(fid_scores)\n","            except:\n","                print(\"attributions are not in appropriate shape\")\n","\n","        elif style ==\"Edge\":\n","            # try:\n","                fid_scores = []\n","                for key, value in saliencies_for_multiple_classes.items():\n","                    saliency_map = self.normalize_saliency_edge_based(value)\n","                    fid_score = self.Fidelity_edge(self.test_data, saliency_map, importance_threshold)\n","                    fid_scores.append(fid_score)\n","                return mean(fid_scores)\n","            # except:\n","            #     print(\"attributions are not in appropriate shape\")\n","\n","        else:\n","            print(\"node based not covered yet\")"],"metadata":{"id":"hlT-y0_I8fM5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **Fid-**"],"metadata":{"id":"bC9Dju7lK0Za"}},{"cell_type":"code","source":["class evalaution_of_xmethods_fidelity_minus(object):\n","    def __init__(self, a_trained_model, test_data):\n","        super(evalaution_of_xmethods_fidelity_minus, self).__init__()\n","        self.a_trained_model = a_trained_model\n","        self.test_data = test_data\n","\n","    def is_salient(self, score, importance_threshold):\n","        if importance_threshold == score == 0:\n","            return True\n","        if importance_threshold == score == 1:\n","            return False\n","        if importance_threshold < score:\n","            return True\n","        else:\n","            return False\n","\n","    def Compute_ROC_AUC(self, your_model, your_dataset):\n","        preds = []\n","        reals = []\n","\n","        your_model.eval()\n","        for batched_data in your_dataset:\n","            if your_model.__class__.__name__ == \"GCN_plus_GAP\":\n","                Output_of_Hidden_Layers, pooling_layer_output, ffn_output, prediction = your_model(batched_data)\n","            elif your_model.__class__.__name__ == \"DGCNN_Model\":\n","                final_GNN_layer_output, sortpooled_embedings, output_conv1d_1, maxpooled_output_conv1d_1, output_conv1d_2, to_dense, output_h1, dropout_output_h1, output_h2, prediction = your_model(batched_data, None)\n","            elif your_model.__class__.__name__ == \"DIFFPOOL_Model\":\n","                concatination_list_of_poolings, prediction_output_not_softed, prediction = your_model(batched_data, None)\n","            elif your_model.__class__.__name__ == \"GIN_Model\":\n","                mlps_output_embeds, mlp_outputs_globalSUMpooled, lin1_output, lin1_output_dropouted, lin2_output, prediction = your_model(batched_data, None)\n","            else:\n","                print(\"Model Name is not valid.\")\n","\n","            preds.extend(prediction.tolist())\n","            reals.extend(batched_data.y.tolist())\n","\n","        # for i in range(len(your_dataset)):\n","        #     reals.append(your_dataset[i].y.tolist())\n","\n","        is_binary = len(np.unique(reals)) == 2\n","        if is_binary:\n","            preds = (np.array(preds)[:, 1] > 0.5).astype(int)\n","\n","        auc_roc = metrics.roc_auc_score(np.array(reals), np.array(preds), multi_class=\"ovr\", average=\"micro\")\n","\n","        # preds = torch.cat(preds)\n","        # preds, max_idxs = torch.max(preds[:], dim=1)\n","\n","        # auc_roc = metrics.roc_auc_score(reals, preds, average='macro')\n","        return auc_roc\n","\n","    def drop_unimportant_node_features(self, your_dataset, importance_threshold, attribution_scores):\n","        occluded_GNNgraph_list = []\n","        for i in range(len(attribution_scores)):\n","            sample_graph = deepcopy(your_dataset[i])\n","            for j in range(len(attribution_scores[i])):\n","                for k in range(len(attribution_scores[i][j])):\n","                    if self.is_salient(attribution_scores[i][j][k], importance_threshold) == False:  ################## change is here\n","                        sample_graph.x[j][k] = 0\n","            occluded_GNNgraph_list.append(sample_graph)\n","        return occluded_GNNgraph_list\n","\n","    def drop_unimportant_nodes(self, your_dataset, importance_threshold, attribution_scores):\n","        occluded_GNNgraph_list = []\n","        for i in range(len(attribution_scores)):\n","            sample_graph = deepcopy(your_dataset[i])\n","            for j in range(len(attribution_scores[i])):\n","                if self.is_salient(attribution_scores[i][j], importance_threshold) == False:\n","                    sample_graph.x[j][:] = 0\n","            occluded_GNNgraph_list.append(sample_graph)\n","        return occluded_GNNgraph_list\n","\n","    def Fidelity_node_features(self, your_dataset, generated_saliency_maps, importance_threshold):\n","\n","        auc_roc_before_droping_unimportant_nodes = self.Compute_ROC_AUC(self.a_trained_model, your_dataset)\n","        new_graph_dataset = self.drop_unimportant_node_features(your_dataset, importance_threshold, generated_saliency_maps)\n","        auc_roc_after_droping_unimportant_nodes = self.Compute_ROC_AUC(self.a_trained_model, new_graph_dataset)\n","\n","        return auc_roc_before_droping_unimportant_nodes - auc_roc_after_droping_unimportant_nodes\n","\n","    def Fidelity_node(self, your_dataset, generated_saliency_maps, importance_threshold):\n","\n","        auc_roc_before_droping_unimportant_nodes = self.Compute_ROC_AUC(self.a_trained_model, your_dataset)\n","        new_graph_dataset = self.drop_unimportant_nodes(your_dataset, importance_threshold, generated_saliency_maps)\n","        auc_roc_after_droping_unimportant_nodes = self.Compute_ROC_AUC(self.a_trained_model, new_graph_dataset)\n","\n","        return auc_roc_before_droping_unimportant_nodes - auc_roc_after_droping_unimportant_nodes\n","\n","    def drop_unimportant_edges(self, your_dataset, importance_threshold, Edge_Masks_Dropped):\n","\n","        attribution_scores = []\n","        Edge_Masks_original = []\n","        Edge_Masks_Dropped_copy = []\n","        for tensor in Edge_Masks_Dropped:\n","            Edge_Masks_original.append(deepcopy(tensor.detach()))\n","            Edge_Masks_Dropped_copy.append(deepcopy(tensor.detach()))\n","\n","        for edge_mask in Edge_Masks_Dropped_copy:\n","\n","            unimportance_indices = edge_mask <= importance_threshold * (max(edge_mask)-min(edge_mask)) # Changed here > to <=\n","            edge_mask[unimportance_indices] = 0\n","\n","        return Edge_Masks_original, Edge_Masks_Dropped_copy\n","\n","    def Fidelity_edge(self, your_dataset, generated_saliency_maps, importance_threshold):\n","\n","        auc_roc_before_droping_unimportant_nodes = self.Compute_ROC_AUC_edge(self.a_trained_model, your_dataset, False)\n","        Edge_Masks_original, Edge_Masks_Dropped = self.drop_unimportant_edges(your_dataset, importance_threshold, generated_saliency_maps)\n","        auc_roc_after_droping_unimportant_nodes = self.Compute_ROC_AUC_edge(self.a_trained_model, your_dataset, Edge_Masks_Dropped)\n","\n","        return auc_roc_before_droping_unimportant_nodes - auc_roc_after_droping_unimportant_nodes\n","\n","    def normalize_saliency_node_features_based(self, sal_maps):\n","        Graphs_new_gradients = []\n","        for graph_grads in sal_maps:\n","            new_gradients = []\n","            for node_grads in graph_grads:\n","                node_gradients = []\n","                for dim in node_grads:\n","                    node_gradients.append((dim-min(node_grads))/(max(node_grads)-min(node_grads)))\n","                new_gradients.append(node_gradients)\n","            Graphs_new_gradients.append(new_gradients)\n","\n","        return Graphs_new_gradients\n","\n","    def normalize_saliency_node_based(self, sal_maps):\n","        Graphs_new_gradients = []\n","        for graph_grads in sal_maps:\n","            new_gradients = []\n","            for node_grads in graph_grads:\n","                val = (node_grads-min(graph_grads))/(max(graph_grads)-min(graph_grads)) if (max(graph_grads)-min(graph_grads)) != 0 else 0\n","                new_gradients.append(val)\n","            Graphs_new_gradients.append(new_gradients)\n","\n","        return Graphs_new_gradients\n","\n","    def clear_masks(self, model):\n","\n","        for module in model.modules():\n","            if isinstance(module, MessagePassing):\n","                module.explain = False\n","                module._edge_mask = None\n","                module._loop_mask = None\n","                module._apply_sigmoid = True\n","        return module\n","\n","    def apply_masks(self, model, mask, edge_index, apply_sigmoid):\n","        loop_mask = edge_index[0] != edge_index[1]\n","\n","        for module in model.modules():\n","            if isinstance(module, MessagePassing):\n","\n","                if (not isinstance(mask, Parameter)\n","                        and '_edge_mask' in module._parameters):\n","                    mask = Parameter(mask)\n","\n","                module.explain = True\n","                module._edge_mask = mask\n","                module._loop_mask = loop_mask\n","                module._apply_sigmoid = apply_sigmoid\n","\n","    def Compute_ROC_AUC_edge(self, your_model, your_dataset, masked):\n","        preds = []\n","        reals = []\n","\n","        if masked == False:\n","            your_model.eval()\n","            for batched_data in your_dataset:\n","                reals.extend(batched_data.y.tolist())\n","                if your_model.__class__.__name__ == \"GCN_plus_GAP\":\n","                    Output_of_Hidden_Layers, pooling_layer_output, ffn_output, soft = your_model(batched_data)\n","                    preds.extend(soft.tolist())\n","                elif your_model.__class__.__name__ == \"DGCNN_Model\":\n","                    final_GNN_layer_output, sortpooled_embedings, output_conv1d_1, maxpooled_output_conv1d_1, output_conv1d_2, to_dense, output_h1, dropout_output_h1, output_h2, soft = your_model(batched_data, None)\n","                    preds.extend(soft.tolist())\n","                elif your_model.__class__.__name__ == \"DIFFPOOL_Model\":\n","                    concatination_list_of_poolings, prediction_output_not_softed, soft = your_model(batched_data, None)\n","                    preds.extend(soft.tolist())\n","                elif your_model.__class__.__name__ == \"GIN_Model\":\n","                    mlps_output_embeds, mlp_outputs_globalSUMpooled, lin1_output, lin1_output_dropouted, lin2_output, soft = your_model(batched_data, None)\n","                    preds.extend(soft.tolist())\n","        else:\n","            your_model.eval()\n","            for batched_data, edge_mask in zip(your_dataset, masked):\n","                reals.extend(batched_data.y.tolist())\n","                if your_model.__class__.__name__ == \"GCN_plus_GAP\":\n","                    self.apply_masks(your_model, edge_mask, batched_data.edge_index, apply_sigmoid=True)\n","                    Output_of_Hidden_Layers, pooling_layer_output, ffn_output, soft = your_model(batched_data)\n","                    preds.extend(soft.tolist())\n","                    self.clear_masks(your_model)\n","                elif your_model.__class__.__name__ == \"DGCNN_Model\":\n","                    final_GNN_layer_output, sortpooled_embedings, output_conv1d_1, maxpooled_output_conv1d_1, output_conv1d_2, to_dense, output_h1, dropout_output_h1, output_h2, soft = your_model(batched_data, edge_mask.tolist())\n","                    preds.extend(soft.tolist())\n","                elif your_model.__class__.__name__ == \"DIFFPOOL_Model\":\n","                    concatination_list_of_poolings, prediction_output_not_softed, soft = your_model(batched_data, edge_mask.tolist())\n","                    preds.extend(soft.tolist())\n","                elif your_model.__class__.__name__ == \"GIN_Model\":\n","                    mlps_output_embeds, mlp_outputs_globalSUMpooled, lin1_output, lin1_output_dropouted, lin2_output, soft = your_model(batched_data, edge_mask.tolist())\n","                    preds.extend(soft.tolist())\n","\n","        # for i, batched_graph in enumerate(your_dataset):\n","        #     reals.append(batched_graph.y.cpu().detach().tolist())\n","\n","        # preds = torch.cat(preds)\n","        # preds, max_idxs = torch.max(preds[:], dim=1)\n","\n","        # roc_auc = metrics.roc_auc_score(reals, preds, average='macro')\n","        is_binary = len(np.unique(reals)) == 2\n","        if is_binary:\n","            preds = (np.array(preds)[:, 1] > 0.5).astype(int)\n","        roc_auc = metrics.roc_auc_score(np.array(reals), np.array(preds), multi_class=\"ovr\", average=\"micro\")\n","        return roc_auc\n","\n","    def normalize_saliency_edge_based(self, saliency_maps):\n","        Graphs_new_gradients = []\n","\n","        for graph_grads in saliency_maps:\n","            for edge_grads in graph_grads:\n","                edge_grads = (edge_grads-torch.min(graph_grads))/(torch.max(graph_grads)-torch.min(graph_grads)) if (torch.max(graph_grads)-torch.min(graph_grads)) != 0 else 0\n","            Graphs_new_gradients.append(graph_grads)\n","\n","        return Graphs_new_gradients\n","\n","    def my_fidelity(self, saliencies_for_multiple_classes, importance_threshold, style):\n","        if style == \"Node Feature\":\n","            try:\n","                fid_scores = []\n","                for key, value in saliencies_for_multiple_classes.items():\n","                    saliency_map = self.normalize_saliency_node_features_based(value)\n","                    fid_score = self.Fidelity_node_features(self.test_data, saliency_map, importance_threshold)\n","                    fid_scores.append(fid_score)\n","\n","                return mean(fid_scores)\n","            except:\n","                print(\"attributions are not in appropriate shape\")\n","\n","        elif style == \"Node\":\n","            try:\n","                fid_scores = []\n","                for key, value in saliencies_for_multiple_classes.items():\n","                    saliency_map = self.normalize_saliency_node_based(value)\n","                    fid_score = self.Fidelity_node(self.test_data, saliency_map, importance_threshold)\n","                    fid_scores.append(fid_score)\n","                return mean(fid_scores)\n","            except:\n","                print(\"attributions are not in appropriate shape\")\n","\n","        elif style ==\"Edge\":\n","            try:\n","                fid_scores = []\n","                for key, value in saliencies_for_multiple_classes.items():\n","                    saliency_map = self.normalize_saliency_edge_based(value)\n","                    fid_score = self.Fidelity_edge(self.test_data, saliency_map, importance_threshold)\n","                    fid_scores.append(fid_score)\n","                return mean(fid_scores)\n","            except:\n","                print(\"attributions are not in appropriate shape\")\n","\n","        else:\n","            print(\"node based not covered yet\")"],"metadata":{"id":"w9WBGnPtKycZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **Stability**"],"metadata":{"id":"Q94lhvEwZB-b"}},{"cell_type":"code","source":["class evalaution_of_xmethods_stability(object):\n","    def __init__(self, a_trained_model, test_data):\n","        \"\"\"\n","            Perturb Node Features for Entire Dataset\n","            Explain perturbed dataset\n","            Compute their distance\n","        \"\"\"\n","        super(evalaution_of_xmethods_stability, self).__init__()\n","\n","        self.a_trained_model = a_trained_model\n","        self.test_data = test_data\n","\n","\n","    def normalize_saliency_node_features_based(self, sal_maps):\n","        Graphs_new_gradients = []\n","        for graph_grads in sal_maps:\n","            new_gradients = []\n","            for node_grads in graph_grads:\n","                node_gradients = []\n","                for dim in node_grads:\n","                    node_gradients.append((dim-min(node_grads))/(max(node_grads)-min(node_grads)) if (max(node_grads)-min(node_grads)) != 0 else 0)\n","                new_gradients.append(node_gradients)\n","            Graphs_new_gradients.append(new_gradients)\n","\n","        return Graphs_new_gradients\n","\n","    def normalize_saliency_node_based(self, sal_maps):\n","        Graphs_new_gradients = []\n","        for graph_grads in sal_maps:\n","            new_gradients = []\n","            for node_grads in graph_grads:\n","                val = (node_grads-min(graph_grads))/(max(graph_grads)-min(graph_grads)) if (max(graph_grads)-min(graph_grads)) != 0 else 0\n","                new_gradients.append(val)\n","            Graphs_new_gradients.append(new_gradients)\n","\n","        return Graphs_new_gradients\n","\n","    def normalize_stability_distance(self, distance_list):\n","        normalized_distance_list = []\n","        for exp_diff in distance_list:\n","            val = (exp_diff-min(distance_list))/(max(distance_list)-min(distance_list)) if (max(distance_list)-min(distance_list)) != 0 else 0\n","            normalized_distance_list.append(val)\n","        return normalized_distance_list\n","\n","    def perturb_node_features_of_dataset(self, dataset, mean, std):\n","        dataset_perturbed = deepcopy(dataset)\n","        for graph in dataset_perturbed:\n","            continuous_noise = torch.ones(dataset[0].x.size()[1]).normal_(mean, std)\n","            graph.x += continuous_noise\n","        return dataset_perturbed\n","\n","    def dist_explanation(self, normal_saliency, perturbed_saliency, top_k_features):\n","        distance_list = []\n","        for normal_graph, perturbed_graph in zip(normal_saliency, perturbed_saliency):\n","            perturbed_graph = np.array(perturbed_graph)\n","            perturbed_graph = torch.from_numpy(perturbed_graph)\n","            normal_graph = np.array(normal_graph)\n","            normal_graph = torch.from_numpy(normal_graph)\n","\n","            normal_graph_mask = torch.where(normal_graph >= normal_graph.topk(top_k_features)[0][-1].item(),\n","                                            torch.ones_like(normal_graph), torch.zeros_like(normal_graph))\n","            perturbed_graph_mask = torch.where(perturbed_graph >= perturbed_graph.topk(top_k_features)[0][-1].item(),\n","                                                torch.ones_like(perturbed_graph), torch.zeros_like(perturbed_graph))\n","\n","            distance = F.pairwise_distance(normal_graph_mask.unsqueeze(dim=0), perturbed_graph_mask.unsqueeze(dim=0), p=1) / normal_graph_mask.size()[0]\n","            distance_list.append(distance.tolist()[0])\n","        return distance_list\n","\n","    def one_graph_all_together(self, normal_saliency, perturbed_saliency):\n","        normal_saliency_list = []\n","        perturbed_saliency_list = []\n","        for normal_graph, perturbed_graph in zip(normal_saliency, perturbed_saliency):\n","            normal_saliency_list.append([node_feat for node in normal_graph for node_feat in node])\n","            perturbed_saliency_list.append([node_feat for node in perturbed_graph for node_feat in node])\n","        return perturbed_saliency_list, perturbed_saliency_list\n","\n","    def distance_thresholding_node_features(self, distance_list, importance_threshold):\n","        stability_list = []\n","        for graph_score in distance_list:\n","            if importance_threshold == graph_score == 0:\n","                stability_list.append(1)\n","            if importance_threshold == graph_score == 1:\n","                stability_list.append(0)\n","            if importance_threshold < graph_score:\n","                stability_list.append(1 - graph_score)\n","            else:\n","                stability_list.append(0)\n","        return stability_list\n","\n","    def normalize_saliency_edge_based(self, saliency_maps):\n","        Graphs_new_gradients = []\n","\n","        for graph_grads in saliency_maps:\n","            #new_gradients = []\n","            for edge_grads in graph_grads:\n","                edge_grads = (edge_grads-torch.min(graph_grads))/(torch.max(graph_grads)-torch.min(graph_grads)) if (torch.max(graph_grads)-torch.min(graph_grads)) != 0 else 0\n","                #new_gradients.append(val)\n","            Graphs_new_gradients.append(graph_grads.tolist())\n","\n","        return Graphs_new_gradients\n","\n","    def my_stability(self, normal_saliencies_for_multiple_classes, perturbed_saliencies_for_multiple_classes,\n","                     top_k_features, importance_threshold, style):\n","        if style == \"Node Feature\":\n","            try:\n","                normal_saliency_classes_listed = {}\n","                perturbed_saliency_classes_listed = {}\n","                distances_class_based = {}\n","                for key, values in normal_saliencies_for_multiple_classes.items():\n","                    normal_saliencies_for_multiple_classes[key] = self.normalize_saliency_node_features_based(normal_saliencies_for_multiple_classes[key])\n","                    perturbed_saliencies_for_multiple_classes[key] = self.normalize_saliency_node_features_based(perturbed_saliencies_for_multiple_classes[key])\n","\n","                    normal_saliency_classes_listed[key], perturbed_saliency_classes_listed[key] = self.one_graph_all_together(normal_saliency_class0, perturbed_saliency_class0)\n","                    distances_class_based[key] = self.dist_explanation(normal_saliency_classes_listed[key], perturbed_saliency_classes_listed[key], top_k_features)\n","                    distances_class_based[key] = self.normalize_stability_distance(distances_class_based[key])\n","                    distances_class_based[key] = self.distance_thresholding_node_features(distances_class_based[key], importance_threshold)\n","\n","                distance_list = []\n","                for key, value in distances_class_based.items():\n","                    distance_list.append(mean(value))\n","\n","                return mean(distance_list)\n","            except:\n","                print(\"attributions are not in appropriate shape\")\n","\n","        elif style == \"Node\":\n","            #try:\n","                normal_saliency_classes_listed = {}\n","                perturbed_saliency_classes_listed = {}\n","                distances_class_based = {}\n","                for key, values in normal_saliencies_for_multiple_classes.items():\n","                    normal_saliencies_for_multiple_classes[key] = self.normalize_saliency_node_based(normal_saliencies_for_multiple_classes[key])\n","                    perturbed_saliencies_for_multiple_classes[key] = self.normalize_saliency_node_based(perturbed_saliencies_for_multiple_classes[key])\n","\n","                    distances_class_based[key] = self.dist_explanation(normal_saliencies_for_multiple_classes[key], perturbed_saliencies_for_multiple_classes[key], top_k_features)\n","                    distances_class_based[key] = self.normalize_stability_distance(distances_class_based[key])\n","                    distances_class_based[key] = self.distance_thresholding_node_features(distances_class_based[key], importance_threshold)\n","\n","                distance_list = []\n","                for key, value in distances_class_based.items():\n","                    distance_list.append(mean(value))\n","\n","                return mean(distance_list)\n","            #except:\n","            #    print(\"attributions are not in appropriate shape\")\n","\n","        elif style ==\"Edge\":\n","            try:\n","                normal_saliency_classes_listed = {}\n","                perturbed_saliency_classes_listed = {}\n","                distances_class_based = {}\n","                for key, values in normal_saliencies_for_multiple_classes.items():\n","                    normal_saliencies_for_multiple_classes[key] = self.normalize_saliency_edge_based(normal_saliencies_for_multiple_classes[key])\n","                    perturbed_saliencies_for_multiple_classes[key] = self.normalize_saliency_edge_based(perturbed_saliencies_for_multiple_classes[key])\n","\n","                    distances_class_based[key] = self.dist_explanation(normal_saliencies_for_multiple_classes[key], perturbed_saliencies_for_multiple_classes[key], top_k_features)\n","                    distances_class_based[key] = self.normalize_stability_distance(distances_class_based[key])\n","                    distances_class_based[key] = self.distance_thresholding_node_features(distances_class_based[key], importance_threshold)\n","\n","                distance_list = []\n","                for key, value in distances_class_based.items():\n","                    distance_list.append(mean(value))\n","\n","                return mean(distance_list)\n","            except:\n","                print(\"attributions are not in appropriate shape\")\n","        else:\n","            print(\"Only node_feat, node, and edge are covered.\")\n","\n","#stability = evalaution_of_xmethods_stability(GNN_Model, test_dataset)\n","\n","#perturbed_dataset = stability.perturb_node_features_of_dataset(test_dataset, 0.1, 0.1)\n","\n","#stability.my_stability(normal_saliency=importance_levels_for_feature_of_nodes_one_node_based,\n","#                       perturbed_saliency=importance_levels_for_feature_of_nodes_one_node_based_perturbed,\n","#                       top_k_features=10, importance_threshold=0.0000000001, style=\"Node\")"],"metadata":{"id":"YGY9ZIP-ZFwB"},"execution_count":null,"outputs":[]}]}