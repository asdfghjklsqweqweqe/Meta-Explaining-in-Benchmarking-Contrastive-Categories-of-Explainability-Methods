# -*- coding: utf-8 -*-
"""one_model_one_explainer_topdown.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1c532rW6EZXW2j23xvvlHU3rm6yuToBjm
"""

# Install required packages.
import os

#!pip install torch==1.7.0
import torch
os.environ['TORCH'] = torch.__version__
print(torch.__version__)


# !pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html
# !pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html
# !pip install -q git+https://github.com/pyg-team/pytorch_geometric.git
# !pip install git+https://github.com/rusty1s/pytorch_geometric.git

import argparse
import os
import sys
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
import numpy as np
from math import sqrt
from statistics import mean
import torch_geometric
from torch_geometric.datasets import TUDataset
import torch
import torch.nn as nn
from torch.nn.parameter import Parameter
from torch_geometric.nn import GCNConv
import torch.nn.functional as F
from torch.nn import Linear, ReLU, Sequential
from sklearn import metrics
from scipy.spatial.distance import hamming
import statistics
import pandas
import csv
from time import perf_counter
from torch_geometric.nn import GCNConv, global_mean_pool
from torch_geometric.loader import DataLoader
import torch_geometric.nn as gnn
from torch.autograd import graph
from typing import Any, Dict, Optional, Union
from IPython.core.display import deepcopy
from torch_geometric.nn import MessagePassing
import copy
from importlib import reload
import pickle
from sklearn.preprocessing import label_binarize
from tqdm.auto import tqdm
from torch_geometric.data import Data, Batch, Dataset




class load_my_dataset:
    def __init__(self, dataset_name, BATCH_SIZE):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.dataset_name = dataset_name
        self.BATCH_SIZE = BATCH_SIZE
    def __call__(self):
        if self.dataset_name == "Graph-SST5":
            from dig.xgraph.dataset import SentiGraphDataset
            py_path = '/data/cs.aau.dk/ey33jw/'
            os.chdir(py_path)
            current_directory = os.getcwd()
            entire_dataset = SentiGraphDataset(root='./Datasets_for_Explainability_Methods/', name='Graph-SST5')
            py_path = '/data/cs.aau.dk/ey33jw/Explainability_Methods/'
            os.chdir(py_path)
            # current_directory = os.getcwd()
        elif self.dataset_name == "IsCyclic":
            with open("/data/cs.aau.dk/ey33jw/Datasets_for_Explainability_Methods/IsCyclic/iscyclic_graphs.pkl",
                      'rb') as f:
                entire_dataset = pickle.load(f)
        else:
            entire_dataset = TUDataset(root='data/TUDataset', name=self.dataset_name)

        labels = []
        for graph in entire_dataset:
            labels.append(int(graph.y))
        y_true = np.array(labels)
        unique_classes = np.unique(y_true)
        num_classes = len(unique_classes)
        # print("num_classes: ", num_classes)

        df = pandas.read_csv("/data/cs.aau.dk/ey33jw/Datasets_for_Explainability_Methods/" +
                             "Train and Test Indexes on Graph Classification/Experimental Results/train_test_indexes_" +
                             str(self.dataset_name) + ".csv")

        read_training_list_indexes__ = df['Train Indexes']
        read_test_list_indexes__ = df['Test Indexes']
        read_test_list_indexes__ = read_test_list_indexes__.dropna()
        read_test_list_indexes = []
        read_training_list_indexes = []
        for element in read_test_list_indexes__:
            read_test_list_indexes.append(int(element))
        for element in read_training_list_indexes__:
            read_training_list_indexes.append(int(element))

        # print(read_training_list_indexes)
        # print(read_test_list_indexes)

        train_dataset = []
        test_dataset = []
        for index in read_training_list_indexes:
            train_dataset.append(entire_dataset[index])
        for index in read_test_list_indexes:
            test_dataset.append(entire_dataset[index])

        #########################################################################################                    BUG
        if self.dataset_name == "ENZYMES":
            del test_dataset[13]
        if self.dataset_name == "NCI1":
            del test_dataset[751]
        # if self.dataset_name == "Graph-SST5":
        #     del test_dataset[411]

        # print(f'Number of training graphs: {len(train_dataset)}')
        # print(f'Number of test graphs: {len(test_dataset)}')
        #
        # print(train_dataset[0])
        train_dataloader = DataLoader(train_dataset, batch_size=self.BATCH_SIZE, shuffle=False)
        test_dataloader = DataLoader(test_dataset, batch_size=self.BATCH_SIZE, shuffle=False)
        # print(train_dataloader.batch_size)
        # batch = next(iter(train_dataloader))
        # print(batch.y)
        # print(len(train_dataloader))

        return train_dataset, test_dataset, train_dataloader, test_dataloader, num_classes, entire_dataset

class load_my_GNN_Model:
    def __init__(self, gnn_model_index, dataset_name, num_classes, entire_dataset, dropout_rate, weight_initializer,
                 bias, act_fun, classifier_weight_decay, classifier_lr, gnn_model_loading_epoch):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.dropout_rate = dropout_rate
        self.dataset_name = dataset_name
        self.entire_dataset = entire_dataset
        self.num_classes = num_classes
        self.weight_initializer = weight_initializer
        self.bias = bias
        self.act_fun = act_fun
        self.classifier_weight_decay = classifier_weight_decay
        self.classifier_lr = classifier_lr
        self.GNN_Models_Name = dict([(1, "GCN_plus_GAP_Model"), (2, "DGCNN_Model"), (3, "DIFFPOOL_Model"),
                                     (4, "GIN_Model")])
        self.GNN_Model_Name = self.GNN_Models_Name[gnn_model_index]
        self.gnn_model_loading_epoch = gnn_model_loading_epoch
        self.Datsets_Name_Dict = {1: "MUTAG", 2: "NCI1", 3: "Graph-SST5", 4: "ENZYMES", 5: "PROTEINS", 6: "IsCyclic"}
        self.Weight_Initializer = {1: "XAVIER", 2: "Kaiming", 3: "Uniform"}
        self.Loss_Function = {1: "CrossEntropy", 2: "MAE", 3: "MSE"}
        self.default_path = '/data/cs.aau.dk/ey33jw/Explainability_Methods/'
        sys.path.insert(0, self.default_path+'/Models/Script/Layers')

    def __call__(self):

        ###################################################################################################          GCN

        if self.GNN_Model_Name == "GCN_plus_GAP_Model":
            import GCN_Layer as gcn_layer
            import GlobalAveragePooling as globalaveragepooling
            import IdenticalPooling as identicalpooling

            sys.path.insert(0, self.default_path+'/Models/Script')
            import GCN_plus_GAP as gcn_plus_gap_model
            node_feat_size = self.entire_dataset[0].x.size()[-1]
            gcn_input_dim = {'MUTAG': node_feat_size, 'NCI1': node_feat_size, 'ENZYMES': 16, 'Graph-SST5': 64,
                             "PROTEINS": 16, 'IsCyclic': node_feat_size}
            self.GNN_Model = gcn_plus_gap_model.GCN_plus_GAP_Model(model_level="graph", num_classes=self.num_classes,
                                                                   GNN_layers=[node_feat_size,
                                                                               gcn_input_dim[self.dataset_name]],
                                                                   Weight_Initializer=self.weight_initializer,
                                                                   Bias=self.bias, act_fun=self.act_fun,
                                                                   dropout_rate=self.dropout_rate).to(self.device)

        ###################################################################################################        DGCNN

        if self.GNN_Model_Name == "DGCNN_Model":
            import DGCNN_Layer as dgcnn_layer
            import DGCNN_GNN_Layers as dgcnn_gnn_layers
            import DGCNN_SortPooling_Layer as sortpooling_layer
            import DGCNN_MLP as dgcnn_mlp

            sys.path.insert(0, self.default_path+'/Models/Script')
            import DGCNN as dgcnn_model

            k_dgcnn = {'MUTAG': 17, 'NCI1': 32, 'ENZYMES': 32, 'Graph-SST5': 19, 'PROTEINS': 26, 'IsCyclic': 20}
            node_feat_size = self.entire_dataset[0].x.size()[-1]
            dgcnn_input_dim = {'MUTAG': [32, 32, 32, 32], 'NCI1': [32, 32, 32, 32], 'ENZYMES': [32, 32, 32, 32],
                               'Graph-SST5': [64, 64, 64, 64], 'PROTEINS': [32, 32, 32, 32],
                               'IsCyclic': [32, 32, 32, 32]}

            self.GNN_Model = dgcnn_model.DGCNN_Model(GNN_layers=dgcnn_input_dim[self.dataset_name], Bias=self.bias,
                                                     num_classes=self.num_classes, mlp_act_fun="ReLu",
                                                     dgcnn_act_fun="tanh", Weight_Initializer=self.weight_initializer,
                                                     mlp_dropout_rate=self.dropout_rate,
                                                     dgcnn_k=k_dgcnn[self.dataset_name],
                                                     node_feat_size=node_feat_size, hid_channels=[16, 32],
                                                     conv1d_kernels=[2, 5], ffn_layer_size=128,
                                                     strides=[2, 1]).to(self.device)

        ###################################################################################################     DIFFPOOL

        if self.GNN_Model_Name == "DIFFPOOL_Model":
            import Batched_GraphSage_Layer as batched_graphsage_layer
            import Batched_DIFFPOOL_Assignment as batched_diffpool_assignment
            import Batched_DIFFPOOL_Embedding as batched_diffpool_embedding
            import Batched_DIFFPOOL_Layer as batched_diffpool_layer
            sys.path.insert(0, self.default_path+'/Models/Script')
            import DIFFPOOL as diffpool_model
            node_feat_size = self.entire_dataset[0].x.size()[-1]
            self.GNN_Model = diffpool_model.DIFFPOOL_Model(embedding_input_dim=node_feat_size,
                                                           embedding_num_block_layers=1, embedding_hid_dim=64,
                                                           new_feature_size=64, assignment_input_dim=node_feat_size,
                                                           assignment_num_block_layers=1, assignment_hid_dim=64,
                                                           max_number_of_nodes=256, prediction_hid_layers=[50],
                                                           concat_neighborhood=False, num_classes=self.num_classes,
                                                           Weight_Initializer=self.weight_initializer, Bias=self.bias,
                                                           act_fun=self.act_fun, dropout_rate=self.dropout_rate,
                                                           normalize_graphsage=False, aggregation="mean",
                                                           concat_diffpools_outputs=True,num_pooling=1,
                                                           pooling="mean").to(self.device)

        ###################################################################################################          GIN

        if self.GNN_Model_Name == "GIN_Model":
            import GIN_MLP_Layers as gin_mlp_layers
            sys.path.insert(0, self.default_path+'/Models/Script')
            import GIN as gin_model
            node_feat_size = self.entire_dataset[0].x.size()[-1]

            gin_input_dim = {'MUTAG': node_feat_size, 'NCI1': node_feat_size, 'ENZYMES': 16, 'Graph-SST5': 64,
                             'PROTEINS': 16, 'IsCyclic': node_feat_size}
            self.GNN_Model = gin_model.GIN_Model(num_mlp_layers=4, Bias=self.bias, num_slp_layers=2,
                                                 mlp_act_fun=self.act_fun, mlp_input_dim=node_feat_size,
                                                 mlp_hid_dim=gin_input_dim[self.dataset_name], joint_embeddings=False,
                                                 mlp_output_dim=self.num_classes, dropout_rate=self.dropout_rate,
                                                 Weight_Initializer=self.weight_initializer).to(self.device)

        GNN_Model_state_dict = torch.load("/data/cs.aau.dk/ey33jw/Train_GNNs_by_Optimal_HyperParameters/" +
                                          self.GNN_Model_Name + "/" + self.GNN_Model_Name + "_" + self.dataset_name +
                                          "_" + str(self.gnn_model_loading_epoch)+".pt",
                                          map_location=self.device)
        # print("self.bias: ", self.bias)
        if self.bias == False:
            keys_to_remove = [key for key in GNN_Model_state_dict['model_state_dict'].keys() if 'bias' in key]
            for key in keys_to_remove:
                del GNN_Model_state_dict['model_state_dict'][key]
        # if 'eps' in self.GNN_Model.state_dict().keys():
        #     print("True for model")
        # if 'eps' in GNN_Model_state_dict['model_state_dict'].keys():
        #     print("True for loading")
        # for key in GNN_Model_state_dict['model_state_dict'].keys():
        #     print(key)
        # print("self.GNN_Model: ", self.GNN_Model.state_dict().keys())
        # print(GNN_Model_state_dict['model_state_dict'])
        self.GNN_Model.load_state_dict(GNN_Model_state_dict['model_state_dict'])
        self.GNN_Model_Optimizer = torch.optim.Adam(self.GNN_Model.parameters(), lr=self.classifier_lr,
                                                    weight_decay=self.classifier_weight_decay)

        return self.GNN_Model_Optimizer, self.GNN_Model



import sys
py_path = 'Models/Script/'
sys.path.insert(0, py_path)


class one_model_one_explainer:
    def __init__(self, dataset_name):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.dataset_name = dataset_name
        self.fid_plus_threshold = 0.01
        self.fid_minus_threshold = 0.01
        self.contrastivity_threshold = 0.5
        self.sparsity_threshold = 0.5
        self.stability_threshold = 0.5
        self.stability_perturbation_mean = 0.1
        self.stability_perturbation_std = 0.1


    def __call__(self, a_trained_GNN_Model, explainer_name, test_dataset, num_classes, explainer_epoch,
                 explainer_learning_rate, test_dataset_batched):

        a_trained_GNN_Model = a_trained_GNN_Model.to(self.device)
        print("****           TopDown Approach           ****")
        print("Model Name: ", a_trained_GNN_Model.__class__.__name__)
        print("Name of the Dataset: ", self.dataset_name)
        print("Number of Classes: ", num_classes)
        print("Test Dataset Size: ", len(test_dataset))
        print("Explainer: ", explainer_name)
        print("Explainer Training Epochs: ", explainer_epoch)
        print("Explainer Learning Rate: ", explainer_learning_rate)

        ################################################################################################################

        if explainer_name == "GNNExplainer":
            import sys
            py_path = '/data/cs.aau.dk/ey33jw/Explainability_Methods/GNNExplainer on Graph Classification/Script/'
            sys.path.insert(0, py_path)
            import gnnexplainer_on_graph_classification as GNNExplainer

            explanations = {}
            for i in range(num_classes):
                explanations[i] = []
            t1 = perf_counter()
            for i, graph in tqdm(enumerate(test_dataset)):
                graph = graph.to(self.device)
                for class_index in range(num_classes):
                    EXP = GNNExplainer.GNNExplainer(a_trained_GNN_Model, explainer_epoch, explainer_learning_rate)
                    node_mask, edge_mask = EXP(graph, class_index)
                    explanations[class_index].append(node_mask)

            ggnexplainer_timing = perf_counter() - t1
            average_explanation_time = (ggnexplainer_timing) / (explainer_epoch * len(test_dataset))
            print(a_trained_GNN_Model.__class__.__name__, " Model by ",  explainer_name,
                  " average_explanation_time: ", average_explanation_time)

            #  Fidelity Plus
            py_path = '/data/cs.aau.dk/ey33jw/Explainability_Methods/Evaluation of Explainability Methods/Script/'
            sys.path.insert(0, py_path)
            import evaluation_of_xmethods_fidelity_plus as eval_xai_fid_plus
            eval_xai_fid_plus = reload(eval_xai_fid_plus)

            fid_plus_xmethod_example = eval_xai_fid_plus.evalaution_of_xmethods_fidelity_plus(
                a_trained_model=a_trained_GNN_Model, test_data=test_dataset, num_classes=num_classes)
            fid_plus_score = fid_plus_xmethod_example.my_fidelity(saliencies_for_multiple_classes=explanations,
                                                                  importance_threshold=self.fid_plus_threshold,
                                                                  style="Node")
            print(a_trained_GNN_Model.__class__.__name__, " Model by ", explainer_name, " Fid+: ", fid_plus_score)

            #  Fidelity Minus
            py_path = '/data/cs.aau.dk/ey33jw/Explainability_Methods/Evaluation of Explainability Methods/Script/'
            sys.path.insert(0, py_path)
            import evaluation_of_xmethods_fidelity_minus as eval_xai_fid_minus
            eval_xai_fid_minus = reload(eval_xai_fid_minus)

            fid_minus_xmethod_example = eval_xai_fid_minus.evalaution_of_xmethods_fidelity_minus(
                a_trained_model=a_trained_GNN_Model, test_data=test_dataset)
            fid_minus_score = fid_minus_xmethod_example.my_fidelity(saliencies_for_multiple_classes=explanations,
                                                                    importance_threshold=self.fid_minus_threshold,
                                                                    style="Node")
            print(a_trained_GNN_Model.__class__.__name__, " Model by ", explainer_name, " Fid-: ", fid_minus_score)

            #  Contrastivity
            py_path = '/data/cs.aau.dk/ey33jw/Explainability_Methods/Evaluation of Explainability Methods/Script/'
            sys.path.insert(0, py_path)
            import evaluation_of_xmethods_contrastivity as eval_xai_contrastivity
            eval_xai_contrastivity = reload(eval_xai_contrastivity)

            contrastivity_xmethod_example = eval_xai_contrastivity.evalaution_of_xmethods_contrastivity(
                a_trained_model=a_trained_GNN_Model, test_data=test_dataset)
            contrastivity_score = contrastivity_xmethod_example.my_contrastivity(
                your_dataset=test_dataset, saliencies_for_multiple_classes=explanations,
                importance_threshold=self.contrastivity_threshold, contrast_coeff=1e+11)
            print(a_trained_GNN_Model.__class__.__name__, " Model by ",explainer_name,
                  " Contrastivity_Score: ", contrastivity_score)

            #  Sparsity
            py_path = '/data/cs.aau.dk/ey33jw/Explainability_Methods/Evaluation of Explainability Methods/Script/'
            sys.path.insert(0,py_path)
            import evaluation_of_xmethods_sparsity as eval_xai_sparsity
            eval_xai_sparsity = reload(eval_xai_sparsity)

            sparsity_xmethod_example = eval_xai_sparsity.evalaution_of_xmethods_sparsity(
                a_trained_model=a_trained_GNN_Model, test_data=test_dataset)
            sparsity_score = sparsity_xmethod_example.my_sparsity(
                your_dataset=test_dataset, saliencies_for_multiple_classes=explanations,
                importance_threshold=self.sparsity_threshold)
            print(a_trained_GNN_Model.__class__.__name__, " Model by ", explainer_name,
                  " Sparsity_Score: ", sparsity_score)

            #  Stability
            py_path = '/data/cs.aau.dk/ey33jw/Explainability_Methods/Evaluation of Explainability Methods/Script/'
            sys.path.insert(0, py_path)
            import evaluation_of_xmethods_stability as eval_xai_stability
            eval_xai_stability = reload(eval_xai_stability)

            stability = eval_xai_stability.evalaution_of_xmethods_stability(a_trained_model=a_trained_GNN_Model,
                                                                            test_data=test_dataset)
            perturbed_test_dataset = stability.perturb_node_features_of_dataset(test_dataset,
                                                                                self.stability_perturbation_mean,
                                                                                self.stability_perturbation_std)

            explanations_for_perturbed_data = {}
            for i in range(num_classes):
                explanations_for_perturbed_data[i] = []
            for graph in perturbed_test_dataset:
                graph = graph.to(self.device)
                for class_index in range(num_classes):
                    EXP = GNNExplainer.GNNExplainer(a_trained_GNN_Model, explainer_epoch, explainer_learning_rate)
                    node_mask, edge_mask = EXP(graph, class_index)
                    explanations_for_perturbed_data[class_index].append(node_mask)

            stability_score = stability.my_stability(
                normal_saliencies_for_multiple_classes=explanations,
                perturbed_saliencies_for_multiple_classes=explanations_for_perturbed_data, top_k_features=2,
                importance_threshold=self.stability_threshold, style='Node')
            print(a_trained_GNN_Model.__class__.__name__, " Model by ",
                  explainer_name, " Stability_Score: ", stability_score)

            ############################################################################################################

        elif explainer_name == "PGExplainer":

            import sys
            py_path = '/data/cs.aau.dk/ey33jw/Explainability_Methods/PGExplainer on Graph Classification/Script/'
            sys.path.insert(0, py_path)
            import pgexplainer_on_graph_classification as PGExplainer_Module
            PGExplainer_Module = reload(PGExplainer_Module)

            explanations_list = {}
            explanations_tensor = {}
            for i in range(num_classes):
                explanations_list[i] = []
                explanations_tensor[i] = []
            pgex_t1 = perf_counter()

            pgex_dim = {"GCN_plus_GAP_Model": {"MUTAG": test_dataset[0].x.size(-1), "NCI1": test_dataset[0].x.size(-1),
                                               "ENZYMES": 16, "Graph-SST5": 64, "PROTEINS": 16,
                                               "IsCyclic": test_dataset[0].x.size(-1)},
                        "DGCNN_Model": {"MUTAG": 32, "NCI1": test_dataset[0].x.size(-1), "ENZYMES": 32,
                                        "Graph-SST5": 64, "PROTEINS": 32, "IsCyclic": 32},
                        "DIFFPOOL_Model": {"MUTAG": test_dataset[0].x.size(-1), "NCI1": 37,
                                           "ENZYMES": test_dataset[0].x.size(-1),
                                           "Graph-SST5": test_dataset[0].x.size(-1), "PROTEINS": 64,
                                           "IsCyclic": test_dataset[0].x.size(-1)},
                        "GIN_Model": {"MUTAG": test_dataset[0].x.size(-1), "NCI1": test_dataset[0].x.size(-1),
                                      "ENZYMES": 16, "Graph-SST5": 64, "PROTEINS": 16,
                                      "IsCyclic": test_dataset[0].x.size(-1)}}
            for class_index in tqdm(range(num_classes)):

                ExTrain_or_ExTest = 'train'
                EXP = PGExplainer_Module.PGExplainer(
                    GNN_Model=a_trained_GNN_Model, explainer_save_index=explainer_epoch, Exp_Epoch=explainer_epoch,
                    Exp_lr=explainer_learning_rate,
                    node_feat_dim=pgex_dim[a_trained_GNN_Model.__class__.__name__][self.dataset_name])

                EXP(ExTrain_or_ExTest=ExTrain_or_ExTest, Exp_Load_index=explainer_epoch,
                    your_dataset=test_dataset_batched, target_class=class_index)

                ExTrain_or_ExTest = 'test'
                EXP = PGExplainer_Module.PGExplainer(
                    GNN_Model=a_trained_GNN_Model, explainer_save_index=explainer_epoch, Exp_Epoch=explainer_epoch,
                    Exp_lr=explainer_learning_rate,
                    node_feat_dim=pgex_dim[a_trained_GNN_Model.__class__.__name__][self.dataset_name])
                for graph in test_dataset:
                    edge_mask = EXP(ExTrain_or_ExTest=ExTrain_or_ExTest, Exp_Load_index=explainer_epoch,
                                    your_dataset=[graph], target_class=class_index)
                    explanations_list[class_index].append(edge_mask[0].tolist())
                    explanations_tensor[class_index].append(edge_mask[0])
            pgex_timing = perf_counter()-pgex_t1
            average_explanation_time = pgex_timing / (explainer_epoch * len(test_dataset))
            print(a_trained_GNN_Model.__class__.__name__, " Model by ",  explainer_name,
                  " average_explanation_time: ", average_explanation_time)

            #  Fidelity Plus
            py_path = '/data/cs.aau.dk/ey33jw/Explainability_Methods/Evaluation of Explainability Methods/Script/'
            sys.path.insert(0, py_path)
            import evaluation_of_xmethods_fidelity_plus as eval_xai_fid_plus
            eval_xai_fid_plus = reload(eval_xai_fid_plus)

            fid_plus_xmethod_example = eval_xai_fid_plus.evalaution_of_xmethods_fidelity_plus(
                a_trained_model=a_trained_GNN_Model, test_data=test_dataset, num_classes=num_classes)
            fid_plus_score = fid_plus_xmethod_example.my_fidelity(saliencies_for_multiple_classes=explanations_tensor,
                                                                  importance_threshold=self.fid_plus_threshold,
                                                                  style="Edge")
            print(a_trained_GNN_Model.__class__.__name__, " Model by ", explainer_name, " Fid+: ", fid_plus_score)

            #  Fidelity Minus
            py_path = '/data/cs.aau.dk/ey33jw/Explainability_Methods/Evaluation of Explainability Methods/Script/'
            sys.path.insert(0, py_path)
            import evaluation_of_xmethods_fidelity_minus as eval_xai_fid_minus
            eval_xai_fid_minus = reload(eval_xai_fid_minus)

            fid_minus_xmethod_example = eval_xai_fid_minus.evalaution_of_xmethods_fidelity_minus(
                a_trained_model=a_trained_GNN_Model, test_data=test_dataset)
            fid_minus_score = fid_minus_xmethod_example.my_fidelity(
                saliencies_for_multiple_classes=explanations_tensor, importance_threshold=self.fid_minus_threshold,
                style="Edge")
            print(a_trained_GNN_Model.__class__.__name__, " Model by ", explainer_name, " Fid-: ", fid_minus_score)

            #  Contrastivity
            py_path = '/data/cs.aau.dk/ey33jw/Explainability_Methods/Evaluation of Explainability Methods/Script/'
            sys.path.insert(0, py_path)
            import evaluation_of_xmethods_contrastivity as eval_xai_contrastivity
            eval_xai_contrastivity = reload(eval_xai_contrastivity)

            contrastivity_xmethod_example = eval_xai_contrastivity.evalaution_of_xmethods_contrastivity(
                a_trained_model=a_trained_GNN_Model, test_data=test_dataset)
            contrastivity_score = contrastivity_xmethod_example.my_contrastivity(
                your_dataset=test_dataset, saliencies_for_multiple_classes=explanations_list,
                importance_threshold=self.contrastivity_threshold, contrast_coeff=1e+11)
            print(a_trained_GNN_Model.__class__.__name__, " Model by ", explainer_name, " Contrastivity_Score: ",
                  contrastivity_score)

            #  Sparsity
            py_path = '/data/cs.aau.dk/ey33jw/Explainability_Methods/Evaluation of Explainability Methods/Script/'
            sys.path.insert(0, py_path)
            import evaluation_of_xmethods_sparsity as eval_xai_sparsity
            eval_xai_sparsity = reload(eval_xai_sparsity)

            sparsity_xmethod_example = eval_xai_sparsity.evalaution_of_xmethods_sparsity(
                a_trained_model=a_trained_GNN_Model, test_data=test_dataset)
            sparsity_score = sparsity_xmethod_example.my_sparsity(your_dataset=test_dataset,
                                                                  saliencies_for_multiple_classes=explanations_list,
                                                                  importance_threshold=self.sparsity_threshold)
            print(a_trained_GNN_Model.__class__.__name__, " Model by ", explainer_name, " Sparsity_Score: ",
                  sparsity_score)

            #  Stability
            py_path = '/data/cs.aau.dk/ey33jw/Explainability_Methods/Evaluation of Explainability Methods/Script/'
            sys.path.insert(0, py_path)
            import evaluation_of_xmethods_stability as eval_xai_stability
            eval_xai_stability = reload(eval_xai_stability)

            stability = eval_xai_stability.evalaution_of_xmethods_stability(a_trained_model=a_trained_GNN_Model,
                                                                            test_data=test_dataset)
            perturbed_test_dataset = stability.perturb_node_features_of_dataset(test_dataset,
                                                                                self.stability_perturbation_mean,
                                                                                self.stability_perturbation_std)

            explanations_for_perturbed_data_tensor = {}
            explanations_for_perturbed_data_list = {}
            for i in range(num_classes):
                explanations_for_perturbed_data_tensor[i] = []
                explanations_for_perturbed_data_list[i] = []

            for class_index in range(num_classes):

                ExTrain_or_ExTest = 'train'
                EXP = PGExplainer_Module.PGExplainer(
                    GNN_Model=a_trained_GNN_Model, explainer_save_index=explainer_epoch, Exp_Epoch=explainer_epoch,
                    Exp_lr=explainer_learning_rate,
                    node_feat_dim=pgex_dim[a_trained_GNN_Model.__class__.__name__][self.dataset_name])

                EXP(ExTrain_or_ExTest=ExTrain_or_ExTest, Exp_Load_index=explainer_epoch,
                    your_dataset=test_dataset_batched, target_class=class_index)

                ExTrain_or_ExTest = 'test'
                EXP = PGExplainer_Module.PGExplainer(
                    GNN_Model=a_trained_GNN_Model, explainer_save_index=explainer_epoch,
                    Exp_Epoch=explainer_epoch, Exp_lr=explainer_learning_rate,
                    node_feat_dim=pgex_dim[a_trained_GNN_Model.__class__.__name__][self.dataset_name])
                for graph in perturbed_test_dataset:
                    edge_mask = EXP(ExTrain_or_ExTest=ExTrain_or_ExTest, Exp_Load_index=explainer_epoch,
                                    your_dataset=[graph], target_class=class_index)
                    explanations_for_perturbed_data_tensor[class_index].append(edge_mask[0])
                    explanations_for_perturbed_data_list[class_index].append(edge_mask[0].tolist())

            stability_score = stability.my_stability(
                normal_saliencies_for_multiple_classes=explanations_list,
                perturbed_saliencies_for_multiple_classes=explanations_for_perturbed_data_list, top_k_features=2,
                importance_threshold=self.stability_threshold, style='Edge')
            print(a_trained_GNN_Model.__class__.__name__, " Model by ", explainer_name, " Stability_Score: ",
                  stability_score)

            ############################################################################################################

        elif explainer_name == "GraphMask":

            import sys
            py_path = 'GraphMask on Graph Classification/Script/'
            sys.path.insert(0,py_path)
            import graphmask_on_graph_classification as GraphMask_Module
            GraphMask_Module = reload(GraphMask_Module)

            explanations_list = {}
            explanations_tensor = {}
            for i in range(num_classes):
                explanations_list[i] = []
                explanations_tensor[i] = []
            graphmask_t1 = perf_counter()
            graphmask_explainers_input_dims = {"GCN_plus_GAP_Model": {"MUTAG": test_dataset[0].x.size(-1),
                                                                      "NCI1": test_dataset[0].x.size(-1), "ENZYMES": 16,
                                                                      "Graph-SST5": 64, "PROTEINS": 16,
                                                                      "IsCyclic": test_dataset[0].x.size(-1)},
                                               "DGCNN_Model": {"MUTAG": 32, "NCI1": 32, "ENZYMES": 32,
                                                               "Graph-SST5": 64, "PROTEINS": 32, "IsCyclic": 32},
                                               "DIFFPOOL_Model": {"MUTAG": test_dataset[0].x.size(-1),
                                                                  "NCI1": test_dataset[0].x.size(-1),
                                                                  "ENZYMES": test_dataset[0].x.size(-1),
                                                                  "Graph-SST5": test_dataset[0].x.size(-1),
                                                                  "PROTEINS": test_dataset[0].x.size(-1),
                                                                  "IsCyclic": test_dataset[0].x.size(-1)},
                                               "GIN_Model": {"MUTAG": test_dataset[0].x.size(-1),
                                                             "NCI1": test_dataset[0].x.size(-1), "ENZYMES": 16,
                                                             "Graph-SST5": 64, "PROTEINS": 16,
                                                             "IsCyclic": test_dataset[0].x.size(-1)}}

            graphmask_explainers_hid_dims = {
                "GCN_plus_GAP_Model": {"MUTAG": test_dataset[0].x.size(-1), "NCI1": test_dataset[0].x.size(-1),
                                       "ENZYMES": test_dataset[0].x.size(-1), "Graph-SST5": 64, "PROTEINS": 16,
                                       "IsCyclic": test_dataset[0].x.size(-1)},
                "DGCNN_Model": {"MUTAG": test_dataset[0].x.size(-1), "NCI1": test_dataset[0].x.size(-1),
                                "ENZYMES": test_dataset[0].x.size(-1), "Graph-SST5": 64, "PROTEINS": 32,
                                "IsCyclic": test_dataset[0].x.size(-1)},
                "DIFFPOOL_Model": {"MUTAG": test_dataset[0].x.size(-1), "NCI1": test_dataset[0].x.size(-1),
                                   "ENZYMES": test_dataset[0].x.size(-1), "Graph-SST5": 64,
                                   "PROTEINS": test_dataset[0].x.size(-1), "IsCyclic": test_dataset[0].x.size(-1)},
                "GIN_Model": {"MUTAG": test_dataset[0].x.size(-1), "NCI1": test_dataset[0].x.size(-1),
                              "ENZYMES": test_dataset[0].x.size(-1), "Graph-SST5": 64, "PROTEINS": 16,
                              "IsCyclic": test_dataset[0].x.size(-1)}}

            for class_index in tqdm(range(num_classes)):

                ExTrain_or_ExTest = 'train'
                EXP = GraphMask_Module.GraphMask(
                    GNN_Model=a_trained_GNN_Model, explainer_save_index=explainer_epoch, Exp_Epoch=explainer_epoch,
                    Exp_lr=0.001,
                    explainer_hid_dim=graphmask_explainers_hid_dims[a_trained_GNN_Model.__class__.__name__][
                        self.dataset_name],
                    explainer_input_dim=graphmask_explainers_input_dims[a_trained_GNN_Model.__class__.__name__][
                        self.dataset_name],
                    dataset_name=self.dataset_name)

                EXP(ExTrain_or_ExTest=ExTrain_or_ExTest, Exp_Load_index=explainer_epoch,
                    your_dataset=test_dataset_batched, target_class=class_index)

                ExTrain_or_ExTest = 'test'
                EXP = GraphMask_Module.GraphMask(
                    GNN_Model=a_trained_GNN_Model, explainer_save_index=explainer_epoch,
                    Exp_Epoch=explainer_epoch, Exp_lr=0.001,
                    explainer_hid_dim=graphmask_explainers_hid_dims[a_trained_GNN_Model.__class__.__name__][
                        self.dataset_name],
                    explainer_input_dim=graphmask_explainers_input_dims[a_trained_GNN_Model.__class__.__name__][
                        self.dataset_name],
                    dataset_name=self.dataset_name)
                for graph in test_dataset:
                    edge_mask = EXP(ExTrain_or_ExTest=ExTrain_or_ExTest, Exp_Load_index=explainer_epoch,
                                    your_dataset=[graph], target_class=class_index)
                    explanations_list[class_index].append(edge_mask[0].tolist())
                    explanations_tensor[class_index].append(edge_mask[0])
            graphmask_timing = perf_counter()-graphmask_t1
            average_explanation_time = graphmask_timing / (explainer_epoch * len(test_dataset))
            print(a_trained_GNN_Model.__class__.__name__, " Model by ",  explainer_name,
                  " average_explanation_time: ", average_explanation_time)

            #  Fidelity Plus
            py_path = 'Evaluation of Explainability Methods/Script/'
            sys.path.insert(0,py_path)
            import evaluation_of_xmethods_fidelity_plus as eval_xai_fid_plus
            eval_xai_fid_plus = reload(eval_xai_fid_plus)

            fid_plus_xmethod_example = eval_xai_fid_plus.evalaution_of_xmethods_fidelity_plus(
                a_trained_model=a_trained_GNN_Model,
                test_data=test_dataset, num_classes=num_classes)
            fid_plus_score = fid_plus_xmethod_example.my_fidelity(saliencies_for_multiple_classes=explanations_tensor,
                                                                  importance_threshold=self.fid_plus_threshold,
                                                                  style="Edge")
            print(a_trained_GNN_Model.__class__.__name__, " Model by ", explainer_name, " Fid+: ", fid_plus_score)

            #  Fidelity Minus
            py_path = 'Evaluation of Explainability Methods/Script/'
            sys.path.insert(0,py_path)
            import evaluation_of_xmethods_fidelity_minus as eval_xai_fid_minus
            eval_xai_fid_minus = reload(eval_xai_fid_minus)

            fid_minus_xmethod_example = eval_xai_fid_minus.evalaution_of_xmethods_fidelity_minus(
                a_trained_model=a_trained_GNN_Model,
                test_data=test_dataset)
            fid_minus_score = fid_minus_xmethod_example.my_fidelity(saliencies_for_multiple_classes=explanations_tensor,
                                                                    importance_threshold=self.fid_minus_threshold,
                                                                    style="Edge")
            print(a_trained_GNN_Model.__class__.__name__, " Model by ", explainer_name, " Fid-: ", fid_minus_score)

            #  Contrastivity
            py_path = 'Evaluation of Explainability Methods/Script/'
            sys.path.insert(0,py_path)
            import evaluation_of_xmethods_contrastivity as eval_xai_contrastivity
            eval_xai_contrastivity = reload(eval_xai_contrastivity)

            contrastivity_xmethod_example = eval_xai_contrastivity.evalaution_of_xmethods_contrastivity(
                a_trained_model=a_trained_GNN_Model, test_data=test_dataset)
            contrastivity_score = contrastivity_xmethod_example.my_contrastivity(
                your_dataset=test_dataset, saliencies_for_multiple_classes=explanations_list,
                importance_threshold=self.contrastivity_threshold, contrast_coeff=1e+11)
            print(a_trained_GNN_Model.__class__.__name__, " Model by ", explainer_name,
                  " Contrastivity_Score: ", contrastivity_score)

            #  Sparsity
            py_path = 'Evaluation of Explainability Methods/Script/'
            sys.path.insert(0,py_path)
            import evaluation_of_xmethods_sparsity as eval_xai_sparsity
            eval_xai_sparsity = reload(eval_xai_sparsity)

            sparsity_xmethod_example = eval_xai_sparsity.evalaution_of_xmethods_sparsity(
                a_trained_model=a_trained_GNN_Model, test_data=test_dataset)
            sparsity_score = sparsity_xmethod_example.my_sparsity(your_dataset=test_dataset,
                                                                  saliencies_for_multiple_classes=explanations_list,
                                                                  importance_threshold=self.sparsity_threshold)
            print(a_trained_GNN_Model.__class__.__name__, " Model by ",
                  explainer_name, " Sparsity_Score: ", sparsity_score)

            #  Stability
            py_path = 'Evaluation of Explainability Methods/Script/'
            sys.path.insert(0,py_path)
            import evaluation_of_xmethods_stability as eval_xai_stability
            eval_xai_stability = reload(eval_xai_stability)

            stability = eval_xai_stability.evalaution_of_xmethods_stability(
                a_trained_model=a_trained_GNN_Model, test_data=test_dataset)
            perturbed_test_dataset = stability.perturb_node_features_of_dataset(
                test_dataset, self.stability_perturbation_mean, self.stability_perturbation_std)

            explanations_for_perturbed_data_tensor = {}
            explanations_for_perturbed_data_list = {}
            for i in range(num_classes):
                explanations_for_perturbed_data_tensor[i] = []
                explanations_for_perturbed_data_list[i] = []

            for class_index in range(num_classes):

                ExTrain_or_ExTest = 'train'
                EXP = GraphMask_Module.GraphMask(GNN_Model=a_trained_GNN_Model, explainer_save_index=explainer_epoch,
                                                 Exp_Epoch=explainer_epoch, Exp_lr=0.001,
                                                 explainer_input_dim=graphmask_explainers_input_dims[
                                                     a_trained_GNN_Model.__class__.__name__][self.dataset_name],
                                                 explainer_hid_dim=
                                                 graphmask_explainers_hid_dims[a_trained_GNN_Model.__class__.__name__][
                                                     self.dataset_name],
                                                 dataset_name=self.dataset_name)

                EXP(ExTrain_or_ExTest=ExTrain_or_ExTest, Exp_Load_index=explainer_epoch,
                    your_dataset=test_dataset_batched, target_class=class_index)

                ExTrain_or_ExTest = 'test'
                EXP = GraphMask_Module.GraphMask(GNN_Model=a_trained_GNN_Model, explainer_save_index=explainer_epoch,
                                                 Exp_Epoch=explainer_epoch, Exp_lr=0.001,
                                                 explainer_input_dim=graphmask_explainers_input_dims[
                                                     a_trained_GNN_Model.__class__.__name__][self.dataset_name],
                                                 explainer_hid_dim=
                                                 graphmask_explainers_hid_dims[a_trained_GNN_Model.__class__.__name__][
                                                     self.dataset_name],
                                                 dataset_name=self.dataset_name)
                for graph in perturbed_test_dataset:
                    edge_mask = EXP(ExTrain_or_ExTest=ExTrain_or_ExTest, Exp_Load_index=explainer_epoch,
                                    your_dataset=[graph], target_class=class_index)
                    explanations_for_perturbed_data_tensor[class_index].append(edge_mask[0])
                    explanations_for_perturbed_data_list[class_index].append(edge_mask[0].tolist())

            stability_score = stability.my_stability(
                normal_saliencies_for_multiple_classes=explanations_list,
                perturbed_saliencies_for_multiple_classes=explanations_for_perturbed_data_list, top_k_features=2,
                importance_threshold=self.stability_threshold, style='Edge')
            print(a_trained_GNN_Model.__class__.__name__, " Model by ",
                  explainer_name, " Stability_Score: ", stability_score)

            count_nodes_in_explanations=True
            if count_nodes_in_explanations:
                avg_sizes_for_class = {}
                for cls in range(num_classes):
                    node_index_list = []
                    threshold = 0.5
                    for i, explanation in enumerate(explanations_tensor[cls]):

                        indices = torch.nonzero(explanation > threshold).squeeze()
                        if indices.dim() == 0:
                            indices = indices.unsqueeze(0)
                        indices_list = indices.tolist()
                        sources = test_dataset[i].edge_index[0][indices_list].tolist()
                        targets = test_dataset[i].edge_index[1][indices_list].tolist()

                        whole = sources + targets
                        whole_no_repeat = list(set(whole))
                        node_index_list.append(whole_no_repeat)
                    size_list = []
                    for list1 in node_index_list:
                        size_list.append(len(list1))
                    # print(statistics.mean(size_list))
                    avg_sizes_for_class[cls] = statistics.mean(size_list)
                    print("avg_sizes_for_class: ", avg_sizes_for_class)

            ############################################################################################################

        elif explainer_name == "SubGraphX":
            import sys
            py_path = 'SubGraphX on Graph Classification/Script/'
            sys.path.insert(0,py_path)
            import subgraphx_on_graph_classification_final_format as SubGraphX_Module
            SubGraphX_Module = reload(SubGraphX_Module)

            Train_or_Test='train'
            if Train_or_Test == 'train':
                explanations = {}
                for i in range(num_classes):
                    explanations[i] = []
                SubGraphX_t1 = perf_counter()
                SubGX = SubGraphX_Module.SubGraphX(GNN_Model=a_trained_GNN_Model, num_classes=num_classes, num_hops=2,
                                                   explain_graph=True, rollout_count=20, min_children_threshold=5,
                                                   ubc1_c_coef=10.0, expand_count_threshold=5, high2low=True,
                                                   sample_num=100, save_permission_explanations=True,
                                                   dataset_name=self.dataset_name)
                for i, graph in tqdm(enumerate(test_dataset)):
                    explanation_results, graph_index, graph = SubGX(graph=graph, graph_index=i)
                    for cls in range(num_classes):
                        explanations[cls].extend(explanation_results[cls])

                SubGraphX_timing = perf_counter()-SubGraphX_t1
                average_explanation_time = SubGraphX_timing / (len(test_dataset))
                print(a_trained_GNN_Model.__class__.__name__, " Model by ",  explainer_name,
                      " average_explanation_time: ", average_explanation_time)

            elif Train_or_Test == 'test':
                subgx_out_the_fly = SubGraphX_Module.SubGraphX_off_the_fly(
                    test_dataset=test_dataset, Task_name='Graph Classification', num_classes=num_classes,
                    dataset_name=self.dataset_name, Model_Name=a_trained_GNN_Model.__class__.__name__)
                whole_data = subgx_out_the_fly(test_dataset)

                explanations = {}
                for i in range(num_classes):
                    explanations[i] = []
                for cls in range(num_classes):
                    for i in range(len(test_dataset)):
                        # explanations[cls].append(torch.sum(whole_data[i][cls]['important_nodes'].x, dim=1).tolist())
                        explanations[cls].append(whole_data[i][cls]['explanation_results'])

            #  Fidelity Plus
            py_path = 'Evaluation of Explainability Methods/Script/'
            sys.path.insert(0,py_path)
            import evaluation_of_xmethods_fidelity_plus as eval_xai_fid_plus
            eval_xai_fid_plus = reload(eval_xai_fid_plus)

            fid_plus_xmethod_example = eval_xai_fid_plus.evalaution_of_xmethods_fidelity_plus(
                a_trained_model=a_trained_GNN_Model, test_data=test_dataset, num_classes=num_classes)
            fid_plus_score = fid_plus_xmethod_example.my_fidelity(saliencies_for_multiple_classes=explanations,
                                                                  importance_threshold=self.fid_plus_threshold,
                                                                  style="Node")
            print(a_trained_GNN_Model.__class__.__name__, " Model by ", explainer_name, " Fid+: ", fid_plus_score)

            #  Fidelity Minus
            py_path = 'Evaluation of Explainability Methods/Script/'
            sys.path.insert(0,py_path)
            import evaluation_of_xmethods_fidelity_minus as eval_xai_fid_minus
            eval_xai_fid_minus = reload(eval_xai_fid_minus)

            fid_minus_xmethod_example = eval_xai_fid_minus.evalaution_of_xmethods_fidelity_minus(
                a_trained_model=a_trained_GNN_Model, test_data=test_dataset)
            fid_minus_score = fid_minus_xmethod_example.my_fidelity(
                saliencies_for_multiple_classes=explanations, importance_threshold=self.fid_minus_threshold,
                style="Node")
            print(a_trained_GNN_Model.__class__.__name__, " Model by ", explainer_name, " Fid-: ", fid_minus_score)

            #  Contrastivity
            py_path = 'Evaluation of Explainability Methods/Script/'
            sys.path.insert(0,py_path)
            import evaluation_of_xmethods_contrastivity as eval_xai_contrastivity
            eval_xai_contrastivity = reload(eval_xai_contrastivity)

            contrastivity_xmethod_example = eval_xai_contrastivity.evalaution_of_xmethods_contrastivity(
                a_trained_model=a_trained_GNN_Model, test_data=test_dataset)
            contrastivity_score = contrastivity_xmethod_example.my_contrastivity(
                your_dataset=test_dataset, saliencies_for_multiple_classes=explanations,
                importance_threshold=self.contrastivity_threshold, contrast_coeff=1e+11)
            print(a_trained_GNN_Model.__class__.__name__, " Model by ", explainer_name,
                  " Contrastivity_Score: ", contrastivity_score)

            #  Sparsity
            py_path = 'Evaluation of Explainability Methods/Script/'
            sys.path.insert(0,py_path)
            import evaluation_of_xmethods_sparsity as eval_xai_sparsity
            eval_xai_sparsity = reload(eval_xai_sparsity)

            sparsity_xmethod_example = eval_xai_sparsity.evalaution_of_xmethods_sparsity(
                a_trained_model=a_trained_GNN_Model, test_data=test_dataset)
            sparsity_score = sparsity_xmethod_example.my_sparsity(
                your_dataset=test_dataset, saliencies_for_multiple_classes=explanations,
                importance_threshold=self.sparsity_threshold)
            print(a_trained_GNN_Model.__class__.__name__, " Model by ",explainer_name,
                  " Sparsity_Score: ", sparsity_score)

            #  Stability
            py_path = 'Evaluation of Explainability Methods/Script/'
            sys.path.insert(0,py_path)
            import evaluation_of_xmethods_stability as eval_xai_stability
            eval_xai_stability = reload(eval_xai_stability)

            stability = eval_xai_stability.evalaution_of_xmethods_stability(a_trained_model=a_trained_GNN_Model,
                                                                            test_data=test_dataset)
            perturbed_test_dataset = stability.perturb_node_features_of_dataset(test_dataset,
                                                                                self.stability_perturbation_mean,
                                                                                self.stability_perturbation_std)

            explanations_for_perturbed_data = {}
            for i in range(num_classes):
                explanations_for_perturbed_data[i] = []

            Train_or_Test_perturbed=Train_or_Test
            if Train_or_Test_perturbed == 'train':
                SubGX = SubGraphX_Module.SubGraphX(GNN_Model=a_trained_GNN_Model, num_classes=num_classes, num_hops=2,
                                                   explain_graph=True, rollout_count=20, min_children_threshold=5,
                                                   ubc1_c_coef= 10.0, expand_count_threshold=5, high2low=True,
                                                   sample_num=100, save_permission_explanations=True,
                                                   dataset_name="Perturbed_"+str(self.dataset_name))
                for i, graph in enumerate(perturbed_test_dataset):
                    explanation_results, graph_index, graph = SubGX(graph=graph, graph_index=i)
                    for cls in range(num_classes):
                        explanations_for_perturbed_data[cls].extend(explanation_results[cls])
            elif Train_or_Test_perturbed == 'test':
                subgx_out_the_fly = SubGraphX_Module.SubGraphX_off_the_fly(
                    test_dataset=perturbed_test_dataset, Task_name='Graph Classification', num_classes=num_classes,
                    dataset_name="Perturbed_" + str(self.dataset_name),
                    Model_Name=a_trained_GNN_Model.__class__.__name__)
                whole_data = subgx_out_the_fly(perturbed_test_dataset)

                for cls in range(num_classes):
                    for i in range(len(perturbed_test_dataset)):
                        explanations_for_perturbed_data[cls].append(whole_data[i][cls]['explanation_results'])

            # print("explanations: ", explanations)
            # print("explanations_for_perturbed_data: ", explanations_for_perturbed_data)
            # for cls in explanations.keys():
            #     l1=[]
            #     for graph in explanations[cls]:
            #         l1.append(len(graph))
            #     print(l1)
            # for cls in explanations_for_perturbed_data.keys():
            #     l1=[]
            #     for graph in explanations_for_perturbed_data[cls]:
            #         l1.append(len(graph))
            #     print(l1)
            stability_score = stability.my_stability(
                normal_saliencies_for_multiple_classes=explanations,
                perturbed_saliencies_for_multiple_classes=explanations_for_perturbed_data, top_k_features=2,
                importance_threshold=self.stability_threshold, style='Node')
            print(a_trained_GNN_Model.__class__.__name__, " Model by ",
                  explainer_name, " Stability_Score: ", stability_score)
            print("=============================================")
            print(a_trained_GNN_Model.__class__.__name__, " Model by ", explainer_name, " Fid+: ", fid_plus_score)
            print(a_trained_GNN_Model.__class__.__name__, " Model by ", explainer_name, " Fid-: ", fid_minus_score)
            print(a_trained_GNN_Model.__class__.__name__, " Model by ", explainer_name,
                  " Contrastivity_Score: ", contrastivity_score)
            print(a_trained_GNN_Model.__class__.__name__, " Model by ", explainer_name,
                  " Sparsity_Score: ", sparsity_score)
            print(a_trained_GNN_Model.__class__.__name__, " Model by ", explainer_name,
                  " Stability_Score: ", stability_score)
            print(a_trained_GNN_Model.__class__.__name__, " Model by ", explainer_name,
                  "average_explanation_time: ", average_explanation_time)

            ############################################################################################################

        elif explainer_name == "CF2":
            import sys
            py_path = './CF2 on Graph Classification/Script/'
            sys.path.insert(0,py_path)
            import cf2_on_graph_classification_final_format as CF2_Module
            CF2_Module = reload(CF2_Module)
            explanations = {}
            explanations_list = {}
            for i in range(num_classes):
                explanations[i] = []
                explanations_list[i] = []
            CF2_t1 = perf_counter()
            for i in range(num_classes):
                cf2_explanation = CF2_Module.CF2_Explaination(GNN_Model=a_trained_GNN_Model, your_dataset=test_dataset,
                                                              explainer_epochs=explainer_epoch, fix_exp=None,
                                                              input_dim=test_dataset[0].x[0].size()[0],
                                                              hid_dim=test_dataset[0].x[0].size()[0],
                                                              output_dim=num_classes)
                exp_dict = cf2_explanation.explain_nodes_gnn_stats(category=i)
                for key, value in exp_dict.items():
                    explanations[i].append(value)
                    explanations_list[i].append(value.tolist())
            CF2_timing = perf_counter()-CF2_t1
            average_explanation_time = CF2_timing / (explainer_epoch * len(test_dataset))
            print(a_trained_GNN_Model.__class__.__name__, " Model by ",  explainer_name,
                  " average_explanation_time: ", average_explanation_time)

            #  Fidelity Plus
            py_path = './Evaluation of Explainability Methods/Script/'
            sys.path.insert(0,py_path)
            import evaluation_of_xmethods_fidelity_plus as eval_xai_fid_plus
            eval_xai_fid_plus = reload(eval_xai_fid_plus)

            fid_plus_xmethod_example = eval_xai_fid_plus.evalaution_of_xmethods_fidelity_plus(
                a_trained_model=a_trained_GNN_Model, test_data=test_dataset, num_classes=num_classes)
            fid_plus_score = fid_plus_xmethod_example.my_fidelity(saliencies_for_multiple_classes=explanations,
                                                                  importance_threshold=self.fid_plus_threshold,
                                                                  style="Edge")
            print(a_trained_GNN_Model.__class__.__name__, " Model by ", explainer_name, " Fid+: ", fid_plus_score)

            #  Fidelity Minus
            py_path = './Explainability_Methods/Evaluation of Explainability Methods/Script/'
            sys.path.insert(0,py_path)
            import evaluation_of_xmethods_fidelity_minus as eval_xai_fid_minus
            eval_xai_fid_minus = reload(eval_xai_fid_minus)

            fid_minus_xmethod_example = eval_xai_fid_minus.evalaution_of_xmethods_fidelity_minus(
                a_trained_model=a_trained_GNN_Model, test_data=test_dataset)
            fid_minus_score = fid_minus_xmethod_example.my_fidelity(saliencies_for_multiple_classes=explanations,
                                                                    importance_threshold=self.fid_minus_threshold,
                                                                    style="Edge")
            print(a_trained_GNN_Model.__class__.__name__, " Model by ", explainer_name, " Fid-: ", fid_minus_score)

            #  Contrastivity
            py_path = './Evaluation of Explainability Methods/Script/'
            sys.path.insert(0,py_path)
            import evaluation_of_xmethods_contrastivity as eval_xai_contrastivity
            eval_xai_contrastivity = reload(eval_xai_contrastivity)

            contrastivity_xmethod_example = eval_xai_contrastivity.evalaution_of_xmethods_contrastivity(
                a_trained_model=a_trained_GNN_Model, test_data=test_dataset)
            contrastivity_score = contrastivity_xmethod_example.my_contrastivity(
                your_dataset=test_dataset, saliencies_for_multiple_classes=explanations_list,
                importance_threshold=self.contrastivity_threshold, contrast_coeff=1e+11)
            print(a_trained_GNN_Model.__class__.__name__, " Model by ", explainer_name,
                  " Contrastivity_Score: ", contrastivity_score)

            #  Sparsity
            py_path = './Evaluation of Explainability Methods/Script/'
            sys.path.insert(0,py_path)
            import evaluation_of_xmethods_sparsity as eval_xai_sparsity
            eval_xai_sparsity = reload(eval_xai_sparsity)

            sparsity_xmethod_example = eval_xai_sparsity.evalaution_of_xmethods_sparsity(
                a_trained_model=a_trained_GNN_Model, test_data=test_dataset)
            sparsity_score = sparsity_xmethod_example.my_sparsity(your_dataset=test_dataset,
                                                                  saliencies_for_multiple_classes=explanations_list,
                                                                  importance_threshold=self.sparsity_threshold)
            print(a_trained_GNN_Model.__class__.__name__, " Model by ", explainer_name,
                  " Sparsity_Score: ", sparsity_score)

            #  Stability
            py_path = './Evaluation of Explainability Methods/Script/'
            sys.path.insert(0,py_path)
            import evaluation_of_xmethods_stability as eval_xai_stability
            eval_xai_stability = reload(eval_xai_stability)

            stability = eval_xai_stability.evalaution_of_xmethods_stability(a_trained_model=a_trained_GNN_Model,
                                                                            test_data=test_dataset)
            perturbed_test_dataset = stability.perturb_node_features_of_dataset(
                test_dataset, self.stability_perturbation_mean, self.stability_perturbation_std)

            explanations_for_perturbed_data_tensor = {}
            explanations_for_perturbed_data_list = {}
            for i in range(num_classes):
                explanations_for_perturbed_data_tensor[i] = []
                explanations_for_perturbed_data_list[i] = []

            for i in range(num_classes):
                cf2_explanation = CF2_Module.CF2_Explaination(GNN_Model=a_trained_GNN_Model,
                                                              your_dataset=perturbed_test_dataset,
                                                              explainer_epochs=explainer_epoch, fix_exp=None,
                                                              input_dim=test_dataset[0].x[0].size()[0],
                                                              hid_dim=test_dataset[0].x[0].size()[0],
                                                              output_dim=num_classes)
                exp_dict = cf2_explanation.explain_nodes_gnn_stats(category=i)
                for key, value in exp_dict.items():
                    explanations_for_perturbed_data_tensor[i].append(value)
                    explanations_for_perturbed_data_list[i].append(value.tolist())

            stability_score = stability.my_stability(
                normal_saliencies_for_multiple_classes=explanations_list,
                perturbed_saliencies_for_multiple_classes=explanations_for_perturbed_data_list, top_k_features=2,
                importance_threshold=self.stability_threshold, style='Edge')
            print(a_trained_GNN_Model.__class__.__name__, " Model by ", explainer_name,
                  " Stability_Score: ", stability_score)

            ############################################################################################################

        elif explainer_name == "PGMExplainer":
            import sys
            py_path = '/data/cs.aau.dk/ey33jw/Explainability_Methods/PGMExplainer on Graph Classification/Script/'
            sys.path.insert(0,py_path)
            import pgmexplainer_on_graph_classification_final_format as PGMExplainer_Module
            PGMExplainer_Module = reload(PGMExplainer_Module)

            explanations = {}
            for i in range(num_classes):
                explanations[i] = []
            PGMExplainer_t1 = perf_counter()
            for i in tqdm(range(num_classes)):
                for j in range(len(test_dataset)):

                    pgmx = PGMExplainer_Module.PGM_Graph_Explainer(GNN_Model=a_trained_GNN_Model, graph=test_dataset[j],
                                                                   perturb_feature_list=[None], perturb_mode="mean",
                                                                   perturb_indicator="abs")
                    pgm_node, p_values, candidate_nodes, dependent_nodes = pgmx.explain(
                        num_samples=len(test_dataset[j].x), noise_offset_percentage=50, top_node=5,
                        p_value_threshold=0.05, class_index=i)
                    test_graph = deepcopy(test_dataset[j])
                    for k in range(len(test_graph.x)):
                        if k not in pgm_node:
                            test_graph.x[k] = torch.zeros_like(test_graph.x[k])
                    graph_list = []
                    for node in test_graph.x:
                        graph_list.append(max(node).tolist())
                    explanations[i].append(graph_list)
            PGMExplainer_timing = perf_counter() - PGMExplainer_t1
            average_explanation_time = PGMExplainer_timing / (len(test_dataset))
            print(a_trained_GNN_Model.__class__.__name__, " Model by ",  explainer_name,
                  " average_explanation_time: ", average_explanation_time)

            #  Fidelity Plus
            py_path = '/data/cs.aau.dk/ey33jw/Explainability_Methods/Evaluation of Explainability Methods/Script/'
            sys.path.insert(0,py_path)
            import evaluation_of_xmethods_fidelity_plus as eval_xai_fid_plus
            eval_xai_fid_plus = reload(eval_xai_fid_plus)

            fid_plus_xmethod_example = eval_xai_fid_plus.evalaution_of_xmethods_fidelity_plus(
                a_trained_model=a_trained_GNN_Model, test_data=test_dataset, num_classes=num_classes)
            fid_plus_score = fid_plus_xmethod_example.my_fidelity(saliencies_for_multiple_classes=explanations,
                                                                  importance_threshold=self.fid_plus_threshold,
                                                                  style="Node")
            print(a_trained_GNN_Model.__class__.__name__, " Model by ", explainer_name, " Fid+: ", fid_plus_score)

            #  Fidelity Minus
            py_path = '/data/cs.aau.dk/ey33jw/Explainability_Methods/Evaluation of Explainability Methods/Script/'
            sys.path.insert(0,py_path)
            import evaluation_of_xmethods_fidelity_minus as eval_xai_fid_minus
            eval_xai_fid_minus = reload(eval_xai_fid_minus)

            fid_minus_xmethod_example = eval_xai_fid_minus.evalaution_of_xmethods_fidelity_minus(
                a_trained_model=a_trained_GNN_Model, test_data=test_dataset)
            fid_minus_score = fid_minus_xmethod_example.my_fidelity(saliencies_for_multiple_classes=explanations,
                                                                    importance_threshold=self.fid_minus_threshold,
                                                                    style="Node")
            print(a_trained_GNN_Model.__class__.__name__, " Model by ", explainer_name, " Fid-: ", fid_minus_score)

            #  Contrastivity
            py_path = '/data/cs.aau.dk/ey33jw/Explainability_Methods/Evaluation of Explainability Methods/Script/'
            sys.path.insert(0,py_path)
            import evaluation_of_xmethods_contrastivity as eval_xai_contrastivity
            eval_xai_contrastivity = reload(eval_xai_contrastivity)

            contrastivity_xmethod_example = eval_xai_contrastivity.evalaution_of_xmethods_contrastivity(
                a_trained_model=a_trained_GNN_Model, test_data=test_dataset)
            contrastivity_score = contrastivity_xmethod_example.my_contrastivity(
                your_dataset=test_dataset, saliencies_for_multiple_classes=explanations,
                importance_threshold=self.contrastivity_threshold, contrast_coeff=1e+11)
            print(a_trained_GNN_Model.__class__.__name__, " Model by ", explainer_name,
                  " Contrastivity_Score: ", contrastivity_score)

            #  Sparsity
            py_path = '/data/cs.aau.dk/ey33jw/Explainability_Methods/Evaluation of Explainability Methods/Script/'
            sys.path.insert(0,py_path)
            import evaluation_of_xmethods_sparsity as eval_xai_sparsity
            eval_xai_sparsity = reload(eval_xai_sparsity)

            sparsity_xmethod_example = eval_xai_sparsity.evalaution_of_xmethods_sparsity(
                a_trained_model=a_trained_GNN_Model, test_data=test_dataset)
            sparsity_score = sparsity_xmethod_example.my_sparsity(your_dataset=test_dataset,
                                                                  saliencies_for_multiple_classes=explanations,
                                                                  importance_threshold=self.sparsity_threshold)
            print(a_trained_GNN_Model.__class__.__name__, " Model by ", explainer_name,
                  " Sparsity_Score: ", sparsity_score)

            #  Stability
            py_path = '/data/cs.aau.dk/ey33jw/Explainability_Methods/Evaluation of Explainability Methods/Script/'
            sys.path.insert(0,py_path)
            import evaluation_of_xmethods_stability as eval_xai_stability
            eval_xai_stability = reload(eval_xai_stability)

            stability = eval_xai_stability.evalaution_of_xmethods_stability(a_trained_model=a_trained_GNN_Model,
                                                                            test_data=test_dataset)
            perturbed_test_dataset = stability.perturb_node_features_of_dataset(test_dataset,
                                                                                self.stability_perturbation_mean,
                                                                                self.stability_perturbation_std)

            explanations_for_perturbed_data = {}
            for i in range(num_classes):
                explanations_for_perturbed_data[i] = []

            for i in range(num_classes):
                for j in range(len(perturbed_test_dataset)):

                    pgmx = PGMExplainer_Module.PGM_Graph_Explainer(GNN_Model=a_trained_GNN_Model,
                                                                   graph=perturbed_test_dataset[j],
                                                                   perturb_feature_list=[None], perturb_mode="mean",
                                                                   perturb_indicator="abs")
                    pgm_node, p_values, candidate_nodes, dependent_nodes = pgmx.explain(
                        num_samples=len(perturbed_test_dataset[j].x), noise_offset_percentage=50, top_node=5,
                        p_value_threshold=0.05, class_index=i)
                    test_graph = deepcopy(perturbed_test_dataset[j])
                    for k in range(len(test_graph.x)):
                        if k not in pgm_node:
                            test_graph.x[k] = torch.zeros_like(test_graph.x[k])
                    graph_list = []
                    for node in test_graph.x:
                        graph_list.append(max(node).tolist())
                    explanations_for_perturbed_data[i].append(graph_list)
            stability_score = stability.my_stability(
                normal_saliencies_for_multiple_classes=explanations,
                perturbed_saliencies_for_multiple_classes=explanations_for_perturbed_data, top_k_features=2,
                importance_threshold=self.sparsity_threshold, style='Node')
            print(a_trained_GNN_Model.__class__.__name__, " Model by ", explainer_name,
                  " Stability_Score: ", stability_score)

            ############################################################################################################

        elif explainer_name == "XGNN":
            import sys
            py_path = 'XGNN on Graph Classification/Script/'
            sys.path.insert(0,py_path)
            import xgnn_on_graph_classification_final_format as XGNN_Module
            XGNN_Module = reload(XGNN_Module)
            datasets_max_number_of_nodes = {'MUTAG': 28, 'NCI1': 111, 'PROTEINS': 620, 'Graph-SST5': 56, 'ENZYMES': 126,
                                            'IsCyclic': 40}
            mutag_max_number_of_nodes = datasets_max_number_of_nodes[self.dataset_name]
            explanations = {}
            for i in range(num_classes):
                explanations[i] = []
            xgnn_t1 = perf_counter()
            for cls_index in range(num_classes):
                xgnn_training = XGNN_Module.XGNN_training(GNN_Model=a_trained_GNN_Model, max_geneneration_iterations=10,
                                                          num_node_features=test_dataset[0].x[0].size()[0],
                                                          candidate_set_length=test_dataset[0].x[0].size()[0],
                                                          max_number_of_nodes=mutag_max_number_of_nodes,
                                                          random_start=True, rollout_count=10,
                                                          class_of_explanation=cls_index, hyp_for_rollout=1,
                                                          hyp_for_rules=2, dropout_rate=0.5, explainer_lr=0.01,
                                                          b1=0.9, b2=0.999, weight_decay=5e-4,
                                                          dataset_name=self.dataset_name)
                trained_graph = xgnn_training(explainer_epochs=explainer_epoch)
                explanation_graph = Data(x=trained_graph['feat'][:trained_graph['num_nodes']],
                                         edge_index=trained_graph['adj'], y=cls_index)

                py_path = 'TopDown Approach on Graph Classification/Script/'
                sys.path.insert(0, py_path)
                import topdown_approach_for_global_methods as topdown_isomorphism_scoring
                topdown_isomorphism_scoring = reload(topdown_isomorphism_scoring)

                for i in range(len(test_dataset)):
                    common_edges_finder = topdown_isomorphism_scoring.global_explanation_and_samples_intersection(
                        explanation=explanation_graph, input_graph=test_dataset[i])
                    intersection_edges = common_edges_finder()
                    explanations[cls_index].append(torch.from_numpy(np.array(list(intersection_edges.values()))))
            xgnn_timing = perf_counter()-xgnn_t1
            average_explanation_time = xgnn_timing / (explainer_epoch * len(test_dataset))
            print(a_trained_GNN_Model.__class__.__name__, " Model by ",  explainer_name,
                  " average_explanation_time: ", average_explanation_time)

            #  Fidelity Plus
            py_path = 'Evaluation of Explainability Methods/Script/'
            sys.path.insert(0, py_path)
            import evaluation_of_xmethods_fidelity_plus as eval_xai_fid_plus
            eval_xai_fid_plus = reload(eval_xai_fid_plus)

            fid_plus_xmethod_example = eval_xai_fid_plus.evalaution_of_xmethods_fidelity_plus(
                a_trained_model=a_trained_GNN_Model, test_data=test_dataset, num_classes=num_classes)
            fid_plus_score = fid_plus_xmethod_example.my_fidelity(saliencies_for_multiple_classes=explanations,
                                                                  importance_threshold=self.fid_plus_threshold,
                                                                  style="Edge")
            print(a_trained_GNN_Model.__class__.__name__, " Model by ", explainer_name, " Fid+: ", fid_plus_score)

            #  Fidelity Minus
            py_path = 'Evaluation of Explainability Methods/Script/'
            sys.path.insert(0, py_path)
            import evaluation_of_xmethods_fidelity_minus as eval_xai_fid_minus
            eval_xai_fid_minus = reload(eval_xai_fid_minus)

            fid_minus_xmethod_example = eval_xai_fid_minus.evalaution_of_xmethods_fidelity_minus(
                a_trained_model=a_trained_GNN_Model, test_data=test_dataset)
            fid_minus_score = fid_minus_xmethod_example.my_fidelity(saliencies_for_multiple_classes=explanations,
                                                                    importance_threshold=self.fid_minus_threshold,
                                                                    style="Edge")
            print(a_trained_GNN_Model.__class__.__name__, " Model by ", explainer_name, " Fid-: ", fid_minus_score)

            #  Contrastivity
            py_path = 'Evaluation of Explainability Methods/Script/'
            sys.path.insert(0, py_path)
            import evaluation_of_xmethods_contrastivity as eval_xai_contrastivity
            eval_xai_contrastivity = reload(eval_xai_contrastivity)

            contrastivity_xmethod_example = eval_xai_contrastivity.evalaution_of_xmethods_contrastivity(
                a_trained_model=a_trained_GNN_Model, test_data=test_dataset)
            contrastivity_score = contrastivity_xmethod_example.my_contrastivity(
                your_dataset=test_dataset, saliencies_for_multiple_classes=explanations,
                importance_threshold=self.contrastivity_threshold, contrast_coeff=1e+11)
            print(a_trained_GNN_Model.__class__.__name__, " Model by ", explainer_name,
                  " Contrastivity_Score: ", contrastivity_score)

            #  Sparsity
            py_path = 'Evaluation of Explainability Methods/Script/'
            sys.path.insert(0, py_path)
            import evaluation_of_xmethods_sparsity as eval_xai_sparsity
            eval_xai_sparsity = reload(eval_xai_sparsity)

            sparsity_xmethod_example = eval_xai_sparsity.evalaution_of_xmethods_sparsity(
                a_trained_model=a_trained_GNN_Model, test_data=test_dataset)
            sparsity_score = sparsity_xmethod_example.my_sparsity(your_dataset=test_dataset,
                                                                  saliencies_for_multiple_classes=explanations,
                                                                  importance_threshold=self.sparsity_threshold)
            print(a_trained_GNN_Model.__class__.__name__, " Model by ", explainer_name,
                  " Sparsity_Score: ", sparsity_score)

            #  Stability
            py_path = 'Evaluation of Explainability Methods/Script/'
            sys.path.insert(0, py_path)
            import evaluation_of_xmethods_stability as eval_xai_stability
            eval_xai_stability = reload(eval_xai_stability)

            stability = eval_xai_stability.evalaution_of_xmethods_stability(a_trained_model=a_trained_GNN_Model,
                                                                            test_data=test_dataset)
            perturbed_test_dataset = stability.perturb_node_features_of_dataset(test_dataset,
                                                                                self.stability_perturbation_mean,
                                                                                self.stability_perturbation_std)

            explanations_for_perturbed_data = {}
            for i in range(num_classes):
                explanations_for_perturbed_data[i] = []

            for cls_index in range(num_classes):
                xgnn_training = XGNN_Module.XGNN_training(GNN_Model=a_trained_GNN_Model, max_geneneration_iterations=10,
                                                          num_node_features=test_dataset[0].x[0].size()[0],
                                                          candidate_set_length=test_dataset[0].x[0].size()[0],
                                                          max_number_of_nodes=mutag_max_number_of_nodes,
                                                          random_start=True, rollout_count=10,
                                                          class_of_explanation=cls_index, hyp_for_rollout=1,
                                                          hyp_for_rules=2, dropout_rate=0.5, explainer_lr=0.01,
                                                          b1=0.9, b2=0.999, weight_decay=5e-4,
                                                          dataset_name=self.dataset_name)
                perturbed_trained_graph = xgnn_training(explainer_epochs=explainer_epoch)
                per_explanation_graph = Data(x=perturbed_trained_graph['feat'][:perturbed_trained_graph['num_nodes']],
                                             edge_index=perturbed_trained_graph['adj'], y=cls_index)
                py_path = 'Evaluation of Model-level and Global Methods on Graph Classification/Script/'
                sys.path.insert(0, py_path)
                import topdown_approach_for_global_methods as topdown_isomorphism_scoring
                topdown_isomorphism_scoring = reload(topdown_isomorphism_scoring)

                for i in range(len(perturbed_test_dataset)):
                    common_edges_finder = topdown_isomorphism_scoring.global_explanation_and_samples_intersection(
                        explanation=per_explanation_graph, input_graph=perturbed_test_dataset[i])
                    perturbed_intersection_edges = common_edges_finder()
                    explanations_for_perturbed_data[cls_index].append(
                        torch.from_numpy(np.array(list(perturbed_intersection_edges.values()))))

            stability_score = stability.my_stability(
                normal_saliencies_for_multiple_classes=explanations,
                perturbed_saliencies_for_multiple_classes=explanations_for_perturbed_data, top_k_features=2,
                importance_threshold=self.stability_threshold, style='Edge')
            print(a_trained_GNN_Model.__class__.__name__, " Model by ", explainer_name,
                  " Stability_Score: ", stability_score)

            ############################################################################################################

        elif explainer_name == "GNNInterpreter":
            import sys
            py_path = '/data/cs.aau.dk/ey33jw/Explainability_Methods/GNNInterpreter on Graph Classification/Script/'
            sys.path.insert(0, py_path)
            import gnninterpreter_on_graph_classification_final_format as GNNInterpreter_Module

            explanations_size = {
                                 "MUTAG": {"GCN_plus_GAP_Model": {0: 8, 1: 9}, "DGCNN_Model": {0: 13, 1: 12},
                                           "DIFFPOOL_Model": {0: 9, 1: 14}, "GIN_Model": {0: 9, 1: 7}},

                                 "NCI1": {"GCN_plus_GAP_Model": {0: 12, 1: 12}, "DGCNN_Model": {0: 13, 1: 13},
                                          "DIFFPOOL_Model": {0: 12, 1: 11}, "GIN_Model": {0: 19, 1: 17}},

                                 "ENZYMES": {"GCN_plus_GAP_Model": {0: 8, 1: 6, 2: 8, 3: 9, 4: 7, 5: 6},
                                             "DGCNN_Model": {0: 10, 1: 10, 2: 9, 3: 9, 4: 11, 5: 10},
                                             "DIFFPOOL_Model": {0: 27, 1: 23, 2: 27, 3: 23, 4: 26, 5: 27},
                                             "GIN_Model": {0: 7, 1: 6, 2: 8, 3: 7, 4: 6, 5: 7}},

                                 "Graph-SST5": {"GCN_plus_GAP_Model": {0: 10, 1: 9, 2: 9, 3: 9, 4: 20},
                                                "DGCNN_Model": {},
                                                "DIFFPOOL_Model": {0: 10, 1: 10, 2: 9, 3: 9, 4: 10},
                                                "GIN_Model": {0: 20, 1: 10, 2: 10, 3: 9, 4: 20}},

                                 "PROTEINS": {"GCN_plus_GAP_Model": {0: 26, 1: 26}, "DGCNN_Model": {0: 20, 1: 24},
                                              "DIFFPOOL_Model": {0: 25, 1: 26}, "GIN_Model": {0: 19, 1: 22}},

                                 "IsCyclic": {"GCN_plus_GAP_Model": {0: 5, 1: 4}, "DGCNN_Model": {0: 13, 1: 15},
                                              "DIFFPOOL_Model": {0: 14, 1: 16}, "GIN_Model": {0: 5, 1: 7}}}
            explanations = {}
            for i in range(num_classes):
                explanations[i] = []

            generations = {}
            for i in range(num_classes):
                generations[i] = []

            mean_of_node_embeddings = {}
            for i in range(num_classes):
                mean_of_node_embeddings[i] = torch.mean(torch.cat([graph.x for graph in test_dataset if graph.y == i],
                                                                  dim=0), axis=0)

            gnninterpreter_t1 = perf_counter()
            for cls_index in range(num_classes):
                losses_aggregated = GNNInterpreter_Module.losses_aggregation(
                    [
                        dict(key="continuous_generated_embeddings",
                             criterion=GNNInterpreter_Module.Embedding_Loss_by_Cosine_Similarity(
                                 target_embedding=mean_of_node_embeddings[cls_index]), weight=10),
                        dict(key="discrete_generated_embeddings",
                             criterion=GNNInterpreter_Module.Embedding_Loss_by_Cosine_Similarity(
                                 target_embedding=mean_of_node_embeddings[cls_index]), weight=10),
                        dict(key="logits_continuous",
                             criterion=GNNInterpreter_Module.Explanation_Class_Score(class_idx=cls_index,
                                                                                     mode='maximize'), weight=1),
                        dict(key="logits_continuous", criterion=GNNInterpreter_Module.MeanPenalty(), weight=0),
                        dict(key="logits_discrete",
                             criterion=GNNInterpreter_Module.Explanation_Class_Score(class_idx=cls_index,
                                                                                     mode='maximize'), weight=1),
                        dict(key="logits_discrete", criterion=GNNInterpreter_Module.MeanPenalty(), weight=0),
                        dict(key="edge_parameters", criterion=GNNInterpreter_Module.NormPenalty(order=1), weight=1),
                        dict(key="edge_parameters", criterion=GNNInterpreter_Module.NormPenalty(order=2), weight=1),
                        dict(key="node_feature_parameters", criterion=GNNInterpreter_Module.NormPenalty(order=1),
                             weight=0),
                        dict(key="node_feature_parameters", criterion=GNNInterpreter_Module.NormPenalty(order=2),
                             weight=0),
                        # dict(key="edge_feature_parameters", criterion=NormPenalty(order=1), weight=0),
                        # dict(key="edge_feature_parameters", criterion=NormPenalty(order=2), weight=0),
                        dict(key="edge_parameters_pairs_of_nodes",
                             criterion=GNNInterpreter_Module.KLDivergencePenalty(binary=True), weight=0)
                    ]
                )
                generator = GNNInterpreter_Module.Graph_Generator(
                    max_nodes=explanations_size[self.dataset_name][a_trained_GNN_Model.__class__.__name__][cls_index],
                    num_node_classes=test_dataset[0].x[0].size()[0], num_edge_classes=None, nodes=None, edges=None,
                    Graph=None, learning_node_feat=True, learning_edge_feat=False, temperature=0.15)

                generations[cls_index] = GNNInterpreter_Module.Generation_Manager_wrt_Classes(
                    generator=generator, discriminator=a_trained_GNN_Model, aggregate_losses=losses_aggregated,
                    optimizer=(o := torch.optim.SGD(generator.parameters(), lr=1)), dataset=test_dataset,
                    budget_penalty=GNNInterpreter_Module.BudgetPenalty_for_second_regularization(budget=10, order=2,
                                                                                                 beta=1),
                    targeted_probabilities={cls_index: (0.9, 1)}, batch_size_for_same_sized_graphs=1)

                continuous_generated_graph, discrete_generated_graph = generations[cls_index].train(explainer_epoch)
                # explanations[cls_index] = discrete_generated_graph
                #print("discrete_generated_graph: ", discrete_generated_graph)

                py_path = ('/data/cs.aau.dk/ey33jw/Explainability_Methods/' +
                           'TopDown Approach on Graph Classification/Script/')
                sys.path.insert(0, py_path)
                import topdown_approach_for_global_methods as topdown_isomorphism_scoring
                topdown_isomorphism_scoring = reload(topdown_isomorphism_scoring)

                for i in range(len(test_dataset)):
                    common_edges_finder = topdown_isomorphism_scoring.global_explanation_and_samples_intersection(
                        explanation=discrete_generated_graph, input_graph=test_dataset[i])
                    intersection_edges = common_edges_finder()
                    explanations[cls_index].append(torch.from_numpy(np.array(list(intersection_edges.values()))))

            gnninterpreter_timing = perf_counter()-gnninterpreter_t1
            average_explanation_time = gnninterpreter_timing/(explainer_epoch*len(test_dataset))
            print(a_trained_GNN_Model.__class__.__name__, " Model by ",  explainer_name,
                  " average_explanation_time: ", average_explanation_time)


            #  Fidelity Plus
            py_path = '/data/cs.aau.dk/ey33jw/Explainability_Methods/Evaluation of Explainability Methods/Script/'
            sys.path.insert(0, py_path)
            import evaluation_of_xmethods_fidelity_plus as eval_xai_fid_plus
            eval_xai_fid_plus = reload(eval_xai_fid_plus)

            fid_plus_xmethod_example = eval_xai_fid_plus.evalaution_of_xmethods_fidelity_plus(
                a_trained_model=a_trained_GNN_Model, test_data=test_dataset, num_classes=num_classes)
            fid_plus_score = fid_plus_xmethod_example.my_fidelity(saliencies_for_multiple_classes=explanations,
                                                                  importance_threshold=self.fid_plus_threshold,
                                                                  style="Edge")
            print(a_trained_GNN_Model.__class__.__name__, " Model by ", explainer_name, " Fid+: ", fid_plus_score)



            #  Fidelity Minus
            py_path = '/data/cs.aau.dk/ey33jw/Explainability_Methods/Evaluation of Explainability Methods/Script/'
            sys.path.insert(0, py_path)
            import evaluation_of_xmethods_fidelity_minus as eval_xai_fid_minus
            eval_xai_fid_minus = reload(eval_xai_fid_minus)

            fid_minus_xmethod_example = eval_xai_fid_minus.evalaution_of_xmethods_fidelity_minus(
                a_trained_model=a_trained_GNN_Model, test_data=test_dataset)
            fid_minus_score = fid_minus_xmethod_example.my_fidelity(saliencies_for_multiple_classes=explanations,
                                                                    importance_threshold=self.fid_minus_threshold,
                                                                    style="Edge")
            print(a_trained_GNN_Model.__class__.__name__, " Model by ", explainer_name, " Fid-: ", fid_minus_score)



            #  Contrastivity
            py_path = '/data/cs.aau.dk/ey33jw/Explainability_Methods/Evaluation of Explainability Methods/Script/'
            sys.path.insert(0, py_path)
            import evaluation_of_xmethods_contrastivity as eval_xai_contrastivity
            eval_xai_contrastivity = reload(eval_xai_contrastivity)

            contrastivity_xmethod_example = eval_xai_contrastivity.evalaution_of_xmethods_contrastivity(
                a_trained_model=a_trained_GNN_Model, test_data=test_dataset)
            contrastivity_score = contrastivity_xmethod_example.my_contrastivity(
                your_dataset=test_dataset, saliencies_for_multiple_classes=explanations,
                importance_threshold=self.contrastivity_threshold, contrast_coeff=1e+11)
            print(a_trained_GNN_Model.__class__.__name__, " Model by ", explainer_name,
                  " Contrastivity_Score: ", contrastivity_score)




            #  Sparsity
            py_path = '/data/cs.aau.dk/ey33jw/Explainability_Methods/Evaluation of Explainability Methods/Script/'
            sys.path.insert(0, py_path)
            import evaluation_of_xmethods_sparsity as eval_xai_sparsity
            eval_xai_sparsity = reload(eval_xai_sparsity)

            sparsity_xmethod_example = eval_xai_sparsity.evalaution_of_xmethods_sparsity(
                a_trained_model=a_trained_GNN_Model, test_data=test_dataset)
            sparsity_score = sparsity_xmethod_example.my_sparsity(your_dataset=test_dataset,
                                                                  saliencies_for_multiple_classes=explanations,
                                                                  importance_threshold=self.sparsity_threshold)
            print(a_trained_GNN_Model.__class__.__name__, " Model by ", explainer_name,
                  " Sparsity_Score: ", sparsity_score)




            #  Stability
            py_path = '/data/cs.aau.dk/ey33jw/Explainability_Methods/Evaluation of Explainability Methods/Script/'
            sys.path.insert(0, py_path)
            import evaluation_of_xmethods_stability as eval_xai_stability
            eval_xai_stability = reload(eval_xai_stability)

            generations_by_perturbed_data = {}
            for i in range(num_classes):
                generations_by_perturbed_data[i] = []

            stability = eval_xai_stability.evalaution_of_xmethods_stability(a_trained_model=a_trained_GNN_Model,
                                                                            test_data=test_dataset)
            perturbed_test_dataset = stability.perturb_node_features_of_dataset(test_dataset,
                                                                                self.stability_perturbation_mean,
                                                                                self.stability_perturbation_std)

            explanations_for_perturbed_data = {}
            for i in range(num_classes):
                explanations_for_perturbed_data[i] = []

            for cls_index in range(num_classes):
                losses_aggregated = GNNInterpreter_Module.losses_aggregation(
                    [
                        dict(key="continuous_generated_embeddings",
                             criterion=GNNInterpreter_Module.Embedding_Loss_by_Cosine_Similarity(
                                 target_embedding=mean_of_node_embeddings[cls_index]), weight=10),
                        dict(key="discrete_generated_embeddings",
                             criterion=GNNInterpreter_Module.Embedding_Loss_by_Cosine_Similarity(
                                 target_embedding=mean_of_node_embeddings[cls_index]), weight=10),
                        dict(key="logits_continuous",
                             criterion=GNNInterpreter_Module.Explanation_Class_Score(class_idx=cls_index,
                                                                                     mode='maximize'), weight=1),
                        dict(key="logits_continuous", criterion=GNNInterpreter_Module.MeanPenalty(), weight=0),
                        dict(key="logits_discrete",
                             criterion=GNNInterpreter_Module.Explanation_Class_Score(class_idx=cls_index,
                                                                                     mode='maximize'), weight=1),
                        dict(key="logits_discrete", criterion=GNNInterpreter_Module.MeanPenalty(), weight=0),
                        dict(key="edge_parameters", criterion=GNNInterpreter_Module.NormPenalty(order=1), weight=1),
                        dict(key="edge_parameters", criterion=GNNInterpreter_Module.NormPenalty(order=2), weight=1),
                        dict(key="node_feature_parameters", criterion=GNNInterpreter_Module.NormPenalty(order=1),
                             weight=0),
                        dict(key="node_feature_parameters", criterion=GNNInterpreter_Module.NormPenalty(order=2),
                             weight=0),
                        # dict(key="edge_feature_parameters", criterion=NormPenalty(order=1), weight=0),
                        # dict(key="edge_feature_parameters", criterion=NormPenalty(order=2), weight=0),
                        dict(key="edge_parameters_pairs_of_nodes",
                             criterion=GNNInterpreter_Module.KLDivergencePenalty(binary=True), weight=0)
                    ]
                )
                generator = GNNInterpreter_Module.Graph_Generator(
                    max_nodes=explanations_size[self.dataset_name][a_trained_GNN_Model.__class__.__name__][cls_index],
                    num_node_classes=test_dataset[0].x[0].size()[0], num_edge_classes=4, nodes=None, edges=None,
                    Graph=None, learning_node_feat=True, learning_edge_feat=False, temperature=0.15)

                generations_by_perturbed_data[cls_index] = GNNInterpreter_Module.Generation_Manager_wrt_Classes(
                    generator=generator, discriminator=a_trained_GNN_Model, aggregate_losses=losses_aggregated,
                    optimizer=(o := torch.optim.SGD(generator.parameters(), lr=1)), dataset=perturbed_test_dataset,
                    budget_penalty=GNNInterpreter_Module.BudgetPenalty_for_second_regularization(budget=10, order=2,
                                                                                                 beta=1),
                    targeted_probabilities={cls_index: (0.9, 1)}, batch_size_for_same_sized_graphs=1)

                continuous_generated_graph, discrete_generated_graph = generations_by_perturbed_data[cls_index].train(
                    explainer_epoch)

                py_path = ('/data/cs.aau.dk/ey33jw/Explainability_Methods/' +
                           'Evaluation of Model-level and Global Methods on Graph Classification/Script/')
                sys.path.insert(0, py_path)
                import topdown_approach_for_global_methods as topdown_isomorphism_scoring
                topdown_isomorphism_scoring = reload(topdown_isomorphism_scoring)

                for i in range(len(perturbed_test_dataset)):
                    common_edges_finder = topdown_isomorphism_scoring.global_explanation_and_samples_intersection(
                        explanation=discrete_generated_graph, input_graph=perturbed_test_dataset[i])
                    intersection_edges = common_edges_finder()
                    explanations_for_perturbed_data[cls_index].append(
                        torch.from_numpy(np.array(list(intersection_edges.values()))))

            stability_score = stability.my_stability(
                normal_saliencies_for_multiple_classes=explanations,
                perturbed_saliencies_for_multiple_classes=explanations_for_perturbed_data, top_k_features=2,
                importance_threshold=self.stability_threshold, style='Edge')
            print(a_trained_GNN_Model.__class__.__name__, " Model by ", explainer_name,
                  " Stability_Score: ", stability_score)


            ############################################################################################################



py_path = '/data/cs.aau.dk/ey33jw/Explainability_Methods/'
os.chdir(py_path)
current_directory = os.getcwd()
print("Current Working Directory:", current_directory)

all_parameters = {1: {"MUTAG": {"lr": 0.00419721, "dropout": 0.26150688, "weight_initializer": 1, "bias": True},
                      "NCI1": {"lr": 0.00447584, "dropout": 0.367435201, "weight_initializer": 1, "bias": True},
                      "ENZYMES": {"lr": 0.00393719, "dropout": 0.314394521, "weight_initializer": 3, "bias": True},
                      "Graph-SST5": {"lr": 0.00398845, "dropout": 0.376534632, "weight_initializer": 2, "bias": True},
                      "PROTEINS": {"lr": 0.00411772, "dropout": 0.329187893, "weight_initializer": 1, "bias": True},
                      "IsCyclic": {"lr": 0.008468306, "dropout": 0.342759509, "weight_initializer": 3, "bias": False}},

                  2: {"MUTAG": {"lr": 0.00130896, "dropout": 0.66060395, "weight_initializer": 3, "bias": True},
                      "NCI1": {"lr": 0.00154723, "dropout": 0.52860329, "weight_initializer": 3, "bias": True},
                      "ENZYMES": {"lr": 0.00146179, "dropout": 0.55382278, "weight_initializer": 3, "bias": True},
                      "Graph-SST5": {"lr": 0.00113456, "dropout": 0.52848269, "weight_initializer": 2, "bias": True},
                      "PROTEINS": {"lr": 0.00124589, "dropout": 0.52350291, "weight_initializer": 1, "bias": True},
                      "IsCyclic": {"lr": 0.00434473, "dropout": 0.67939804, "weight_initializer": 3, "bias": True}},

                  3: {"MUTAG": {"lr": 0.00060551, "dropout": 0.65602164, "weight_initializer": 2, "bias": True},
                      "NCI1": {"lr": 0.00071152, "dropout": 0.58243376, "weight_initializer": 3, "bias": True},
                      "ENZYMES": {"lr": 0.00058299, "dropout": 0.55350198, "weight_initializer": 3, "bias": True},
                      "Graph-SST5": {"lr": 0.00052245, "dropout": 0.52451704, "weight_initializer": 2, "bias": True},
                      "PROTEINS": {"lr": 0.00063418, "dropout": 0.5303986, "weight_initializer": 1, "bias": True},
                      "IsCyclic": {"lr": 0.00804156, "dropout": 0.4877806, "weight_initializer": 3, "bias": True}},

                  4: {"MUTAG": {"lr": 0.00804157, "dropout": 0.48778068, "weight_initializer": 3, "bias": True},
                      "NCI1": {"lr": 0.00758931, "dropout": 0.3658565, "weight_initializer": 3, "bias": True},
                      "ENZYMES": {"lr": 0.0068349, "dropout": 0.34389162, "weight_initializer": 3, "bias": True},
                      "Graph-SST5": {"lr": 0.00920516, "dropout": 0.39022403, "weight_initializer": 3, "bias": True},
                      "PROTEINS": {"lr": 0.00864241, "dropout": 0.32800018, "weight_initializer": 1, "bias": True},
                      "IsCyclic": {"lr": 0.00901886, "dropout": 0.46620775, "weight_initializer": 3, "bias": True}},
                  }

Explainability_name = 'Run All Methods'
Task_name = 'Graph Classification'
checkpoint_directory_Classifier = str(Explainability_name) + " on " + str(Task_name) + "/Model/model_classifier.pt"
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')


batch_size = 10
DataSet_name = "Graph-SST5"

Explainer_Name = "GraphMask"

gnn_model_index = 1
gnn_model_loading_epoch = 5000
explainer_epoch = 1000
classifier_weight_decay = 1e-6
################################################################################                            Load_Dataset

loading_dataset = load_my_dataset(dataset_name=DataSet_name, BATCH_SIZE=batch_size)
train_dataset, test_dataset, train_dataloader, test_dataloader, num_classes, entire_dataset = loading_dataset()

################################################################################                          Load_GNN_Model

loading_gnn_model = load_my_GNN_Model(gnn_model_index=gnn_model_index, dataset_name=DataSet_name,
                                      num_classes=num_classes, entire_dataset=entire_dataset,
                                      dropout_rate=all_parameters[gnn_model_index][DataSet_name]["dropout"],
                                      bias=all_parameters[gnn_model_index][DataSet_name]["bias"],
                                      weight_initializer=all_parameters[gnn_model_index][DataSet_name][
                                          "weight_initializer"],
                                      classifier_lr=all_parameters[gnn_model_index][DataSet_name]["lr"],
                                      act_fun="ReLu",
                                      gnn_model_loading_epoch=gnn_model_loading_epoch,
                                      classifier_weight_decay=classifier_weight_decay)
GNN_Model_Optimizer, GNN_Model_Loaded = loading_gnn_model()

# GCN_plus_GAP_Model - DGCNN_Model - DIFFPOOL_Model - GIN_Model
# GNNExplainer - PGExplainer - GraphMask - SubGraphX - CF2 - PGMExplainer - XGNN - GNNInterpreter
# Datasets name: MUTAG, Fake

just_call_me = one_model_one_explainer(dataset_name=DataSet_name)
just_call_me(a_trained_GNN_Model=GNN_Model_Loaded, explainer_name=Explainer_Name, test_dataset=test_dataset,
             num_classes=num_classes, explainer_epoch=explainer_epoch, explainer_learning_rate=0.0001,
             test_dataset_batched=test_dataloader)
