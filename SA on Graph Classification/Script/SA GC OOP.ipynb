{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO8rEjO9xZ1gGnNeyLvAwkI"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import os\n","import torch\n","os.environ['TORCH'] = torch.__version__\n","print(torch.__version__)\n","\n","!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n","!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n","!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git\n","!pip install captum\n","!pip install transformers==3"],"metadata":{"id":"FHtRSN8JwEkP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import files\n","uploaded = files.upload()"],"metadata":{"id":"uxCqze_vwBrz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torch.nn import Linear\n","import torch.nn.functional as F\n","from torch_geometric.nn import GCNConv\n","from torch_geometric.nn import global_mean_pool\n","import matplotlib.pyplot as plt\n","import torch\n","from torch_geometric.datasets import TUDataset\n","import numpy as np\n","from termcolor import colored\n","from torchsummary import summary\n","from torch.autograd import Variable\n","from keras import backend as K\n","from statistics import mean\n","from sklearn import metrics\n","from copy import deepcopy\n","from captum.attr import Saliency\n","from scipy.spatial.distance import hamming\n","from itertools import zip_longest\n","from time import perf_counter\n","import csv\n","import torch.nn as nn\n","import torch_geometric.nn as gnn\n","from torch import Tensor\n","from torch_geometric.typing import OptPairTensor, Adj, OptTensor, Size\n","from typing import Callable, Union, Tuple\n","from torch_sparse import SparseTensor\n","from time import perf_counter\n","import random\n","import pandas\n","from torch_geometric.loader import DataLoader\n","import sklearn\n","import gcn_2l_model\n","#import model_train_test_gc\n","#from model_train_test_gc import Model_Train_Test"],"metadata":{"id":"HGK-q-0uv_bJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"hgNYaSCUv8bF"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iLeTHGtDv0uu"},"outputs":[],"source":["class SA_GC(object):\n","  def __init__(self, task, method, graph, importance_range, start_epoch, input_dim, hid_dim, output_dim):\n","\n","    self.GCN_model = self.load_model(task, method, load_index=start_epoch, input_dim=input_dim, hid_dim=hid_dim, output_dim=output_dim)\n","    self.criterion = torch.nn.CrossEntropyLoss()\n","    self.importance_dict = {}\n","    self.new_graph = self.drop_important_nodes(self.GCN_model, graph, importance_range)\n","    \n","\n","\n","  def load_model(self, task, method, load_index, input_dim, hid_dim, output_dim):\n","\n","    if load_index != 0:\n","      GCN_model, optimizer, load_index = self.loading_config(task, method, load_index, input_dim, hid_dim, output_dim)\n","      return GCN_model\n","    else:\n","      GCN_model = gcn_2l_model.GCN_2Layer_Model(model_level='graph', dim_node=input_dim, dim_hidden=hid_dim, dim_output=output_dim)\n","      return GCN_model\n","\n","\n","  def loading_config(self, task, method, load_index, input_dim, hid_dim, output_dim):\n","    GCN_model = gcn_2l_model.GCN_2Layer_Model(model_level='graph', dim_node=input_dim, dim_hidden=hid_dim, dim_output=output_dim)\n","    optimizer = torch.optim.Adam(params = GCN_model.parameters(), lr=1e-4)\n","    checkpoint = torch.load(\"/content/drive/My Drive/Explainability Methods/\"+str(method)+\" on \" + str(task) + \"/Model/\" + str(method) + \" on \" + str(task) + \" classifier model_\" + str(load_index)+\".pt\")\n","    GCN_model.load_state_dict(checkpoint['model_state_dict'])\n","    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","    epoch = checkpoint['epoch']\n","    loss = checkpoint['loss']\n","    \n","    return GCN_model, optimizer, epoch\n","  \n","\n","  def loss_calculations(self, preds, gtruth):\n","    loss_per_epoch = self.criterion(preds, gtruth)\n","    return loss_per_epoch\n","\n","  def remove_nones(self, sample_grads):\n","    sample_grads2 = []\n","    for item in sample_grads:\n","      Each_Graph = []\n","      for item2 in item:\n","        if item2 != None:\n","          Each_Graph.append(torch.tensor(item2.clone().detach().requires_grad_(True),requires_grad=True))\n","        else:\n","          Each_Graph.append(torch.tensor(0))\n","      sample_grads2.append(Each_Graph)\n","\n","    return sample_grads2\n","\n","  def compute_grad(self, model, graph, with_respect):\n","    prediction = model(graph.x, graph.edge_index, graph.batch)\n","    if with_respect == 1 :\n","      loss = self.loss_calculations(prediction, graph.y)\n","      #print(loss)\n","    elif with_respect == 2:\n","      loss = self.loss_calculations(prediction, torch.tensor([0]))\n","      #print(loss)  \n","    elif with_respect == 3:\n","      loss = self.loss_calculations(prediction, torch.tensor([1]))\n","      #print(loss)\n","    return torch.autograd.grad(loss, list(self.GCN_model.parameters()), allow_unused=True)\n","\n","  def compute_sample_grads(self, model, test_dataset, with_respect):\n","\n","    sample_grads = [self.compute_grad(model, graph, with_respect) for graph in test_dataset]\n","    sample_grads = self.remove_nones(sample_grads)\n","    sample_grads = zip(*sample_grads)\n","    sample_grads = [torch.stack(shards) for shards in sample_grads]\n","    return sample_grads\n","\n","  \n","\n","  def compute_square_gradients(self, model, dataset):\n","    per_sample_grads_wrt_graph_label = self.compute_sample_grads(model, dataset, 1)\n","    per_sample_grads_wrt_class_zero = self.compute_sample_grads(model, dataset, 2)\n","    per_sample_grads_wrt_class_one = self.compute_sample_grads(model, dataset, 3)\n","\n","    grads_wrt_graph_label = torch.square(per_sample_grads_wrt_graph_label[0])\n","    #square_grads_wrt_graph_label = torch.square(per_sample_grads_wrt_graph_label[1])\n","    square_grads_wrt_graph_label = grads_wrt_graph_label.detach().tolist()\n","\n","    grads_wrt_class_zero = torch.square(per_sample_grads_wrt_class_zero[0])\n","    #square_grads_wrt_class_zero = torch.square(per_sample_grads_wrt_class_zero[1])\n","    square_grads_wrt_class_zero = grads_wrt_class_zero.detach().tolist()\n","\n","    grads_wrt_class_one = torch.square(per_sample_grads_wrt_class_one[0])\n","    #square_grads_wrt_class_one = torch.square(per_sample_grads_wrt_class_one[1])\n","    square_grads_wrt_class_one = grads_wrt_class_one.detach().tolist()\n","\n","    return square_grads_wrt_graph_label, square_grads_wrt_class_zero, square_grads_wrt_class_one\n","  \n","\n","  def saliency(self, dataset, gradients):\n","    Final= []\n","    for i in range(len(dataset)):\n","      Mid = []\n","      for node in dataset[i].x.detach().numpy():\n","        First = []\n","        for grad_list in gradients[i]:\n","          First.append(np.multiply(node, grad_list))\n","        Mid.append(First)\n","      Final.append(Mid)\n","    \n","    Saliency_Nodes = []\n","    for graph in Final:\n","      Node = []\n","      for node in graph:\n","        Grad = []\n","        for grad in node:\n","          Grad.append(sum(grad))\n","        Node.append(sum(Grad))\n","      #norm = [(float(i)-min(Node))/(max(Node)-min(Node)) for i in Node]\n","      norm = [(float(i))/(max(Node) + 1e-16) for i in Node]\n","      Saliency_Nodes.append(norm)\n","    return Saliency_Nodes\n","  \n","\n","\n","  def is_salient(self, index, score, importance_range):\n","    start, end = importance_range\n","    if start <= score <= end:\n","      return True\n","    else:\n","\n","      return False\n","  \n","\n","  def drop_important_nodes(self, model, graph, importance_range):\n","    square_grads_wrt_graph_label, square_grads_wrt_class_zero, square_grads_wrt_class_one = self.compute_square_gradients(model, graph)\n","    print(len(square_grads_wrt_graph_label))\n","    SA_attribution_scores = self.saliency(graph, square_grads_wrt_graph_label)\n","    occluded_GNNgraph_list = []\n","    \n","    for i in range(len(SA_attribution_scores)):\n","      sample_graph = deepcopy(graph[i])\n","      graph_dict ={}\n","      for j in range(len(sample_graph.x)):\n","        \n","        if self.is_salient(j, (SA_attribution_scores[i][j]), importance_range):\n","          #print(\"before: \", sample_graph.x[j])\n","          sample_graph.x[j][:] = 0\n","          #print(torch.zeros_like(sample_graph.x[j]))\n","          #print(\"manipulated: \",sample_graph.x[j])\n","          graph_dict[j] = True\n","        else:\n","          graph_dict[j] = False\n","      self.importance_dict[i] = graph_dict\n","      occluded_GNNgraph_list.append(sample_graph)\n","\n","\n","    return occluded_GNNgraph_list\n","\n","\n","\n","dataset = TUDataset(root='data/TUDataset', name='MUTAG')\n","\n","\n","\n","#new_output = SA_GC(graph=[graph], importance_range=(0.5, 1), start_epoch=200, input_dim = len(graph.x[0]), hid_dim = 7, output_dim = 2)\n","new_output = SA_GC(task=\"Graph Classification\", method=\"SA\", graph=dataset, importance_range=(0.5, 1), start_epoch=200, input_dim = len(dataset[0].x[0]), hid_dim = 7, output_dim = 2)\n","print(new_output.new_graph[-1].x, dataset[-1].x)\n","\n","print(new_output.importance_dict)\n"]},{"cell_type":"code","source":["dataset = TUDataset(root='data/TUDataset', name='MUTAG')\n","\n","\n","\n","#new_output = SA_GC(graph=[graph], importance_range=(0.5, 1), start_epoch=200, input_dim = len(graph.x[0]), hid_dim = 7, output_dim = 2)\n","new_output = SA_GC(task=\"Graph Classification\", method=\"SA\", graph=dataset, importance_range=(0.5, 1), start_epoch=200, input_dim = len(dataset[0].x[0]), hid_dim = 7, output_dim = 2)\n","print(new_output.new_graph[-1].x, dataset[-1].x)\n","\n","print(new_output.importance_dict)"],"metadata":{"id":"tX_OoAvFv113"},"execution_count":null,"outputs":[]}]}