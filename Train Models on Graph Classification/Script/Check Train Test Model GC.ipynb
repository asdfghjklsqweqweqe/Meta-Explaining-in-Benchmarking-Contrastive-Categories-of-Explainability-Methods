{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPDYz+pAyxiFEQilnqRCIgv"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import os\n","import torch\n","os.environ['TORCH'] = torch.__version__\n","print(torch.__version__)\n","\n","!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n","!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n","!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git\n","!pip install captum\n","!pip install transformers==3"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QrH2-HQN4LyS","executionInfo":{"status":"ok","timestamp":1673280905268,"user_tz":-60,"elapsed":27219,"user":{"displayName":"Ehsan Mobaraki","userId":"06314958514195310048"}},"outputId":"ebb28671-de81-427f-eaef-c6dc89d92d7a"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["1.13.0+cu116\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m70.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m49.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m512.4/512.4 KB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m280.2/280.2 KB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting captum\n","  Downloading captum-0.6.0-py3-none-any.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from captum) (1.21.6)\n","Requirement already satisfied: torch>=1.6 in /usr/local/lib/python3.8/dist-packages (from captum) (1.13.0+cu116)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (from captum) (3.2.2)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch>=1.6->captum) (4.4.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->captum) (1.4.4)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->captum) (3.0.9)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->captum) (2.8.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib->captum) (0.11.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.1->matplotlib->captum) (1.15.0)\n","Installing collected packages: captum\n","Successfully installed captum-0.6.0\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers==3\n","  Downloading transformers-3.0.0-py3-none-any.whl (754 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m754.6/754.6 KB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting sacremoses\n","  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m880.6/880.6 KB\u001b[0m \u001b[31m62.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers==3) (4.64.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers==3) (3.8.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from transformers==3) (1.21.6)\n","Collecting sentencepiece\n","  Downloading sentencepiece-0.1.97-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m78.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers==3) (2022.6.2)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from transformers==3) (21.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers==3) (2.25.1)\n","Collecting tokenizers==0.8.0-rc4\n","  Downloading tokenizers-0.8.0rc4-cp38-cp38-manylinux1_x86_64.whl (3.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m104.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->transformers==3) (3.0.9)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==3) (2.10)\n","Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==3) (4.0.0)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==3) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==3) (2022.12.7)\n","Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers==3) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers==3) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers==3) (1.2.0)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=587ae2dba4ad645f6f379a05786d8d713eb8a163ed1e43b0482febe0f666595d\n","  Stored in directory: /root/.cache/pip/wheels/82/ab/9b/c15899bf659ba74f623ac776e861cf2eb8608c1825ddec66a4\n","Successfully built sacremoses\n","Installing collected packages: tokenizers, sentencepiece, sacremoses, transformers\n","Successfully installed sacremoses-0.0.53 sentencepiece-0.1.97 tokenizers-0.8.0rc4 transformers-3.0.0\n"]}]},{"cell_type":"code","source":["from google.colab import files\n","uploaded = files.upload()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":69},"id":"qr3MLpF14Odr","executionInfo":{"status":"ok","timestamp":1673280923734,"user_tz":-60,"elapsed":18477,"user":{"displayName":"Ehsan Mobaraki","userId":"06314958514195310048"}},"outputId":"5c286f83-de8f-4a13-dd0f-b7770cb53c14"},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-4d0f31bd-912a-46bc-b3e6-0706dca553f7\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-4d0f31bd-912a-46bc-b3e6-0706dca553f7\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving gcn_2l_model.py to gcn_2l_model.py\n"]}]},{"cell_type":"code","execution_count":35,"metadata":{"id":"Xq2LdLT74AJD","executionInfo":{"status":"ok","timestamp":1673291750797,"user_tz":-60,"elapsed":7,"user":{"displayName":"Ehsan Mobaraki","userId":"06314958514195310048"}}},"outputs":[],"source":["from torch.nn import Linear\n","import torch.nn.functional as F\n","from torch_geometric.nn import GCNConv\n","from torch_geometric.nn import global_mean_pool\n","import matplotlib.pyplot as plt\n","import torch\n","from torch_geometric.datasets import TUDataset\n","import numpy as np\n","from termcolor import colored\n","from torchsummary import summary\n","from torch.autograd import Variable\n","from keras import backend as K\n","from statistics import mean\n","from sklearn import metrics\n","from copy import deepcopy\n","from captum.attr import Saliency\n","from scipy.spatial.distance import hamming\n","from itertools import zip_longest\n","from time import perf_counter\n","import csv\n","import scipy\n","import torch.nn as nn\n","import torch_geometric.nn as gnn\n","from torch import Tensor\n","from torch_geometric.typing import OptPairTensor, Adj, OptTensor, Size\n","from typing import Callable, Union, Tuple\n","from torch_sparse import SparseTensor\n","from time import perf_counter\n","import random\n","import pandas\n","from torch_geometric.loader import DataLoader\n","import sklearn\n","import gcn_2l_model\n","#import model_train_test_gc\n","#from model_train_test_gc import Model_Train_Test"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NlewiZuc4RPi","executionInfo":{"status":"ok","timestamp":1673280950678,"user_tz":-60,"elapsed":19080,"user":{"displayName":"Ehsan Mobaraki","userId":"06314958514195310048"}},"outputId":"ce58a704-c547-4220-9755-dd0fdf811c63"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["## **SA GC Object Oriented Programming**"],"metadata":{"id":"YF5Di03NtgVe"}},{"cell_type":"code","source":["class SA_GC(object):\n","  def __init__(self, task, method, graph, importance_range, start_epoch, input_dim, hid_dim, output_dim):\n","\n","    self.GCN_model = self.load_model(task, method, load_index=start_epoch, input_dim=input_dim, hid_dim=hid_dim, output_dim=output_dim)\n","    self.criterion = torch.nn.CrossEntropyLoss()\n","    self.importance_dict = {}\n","    self.new_graph, self.saliency_maps = self.drop_important_nodes(self.GCN_model, graph, importance_range)\n","    \n","\n","\n","  def load_model(self, task, method, load_index, input_dim, hid_dim, output_dim):\n","\n","    if load_index != 0:\n","      GCN_model, optimizer, load_index = self.loading_config(task, method, load_index, input_dim, hid_dim, output_dim)\n","      return GCN_model\n","    else:\n","      GCN_model = gcn_2l_model.GCN_2Layer_Model(model_level='graph', dim_node=input_dim, dim_hidden=hid_dim, dim_output=output_dim)\n","      return GCN_model\n","\n","\n","  def loading_config(self, task, method, load_index, input_dim, hid_dim, output_dim):\n","    GCN_model = gcn_2l_model.GCN_2Layer_Model(model_level='graph', dim_node=input_dim, dim_hidden=hid_dim, dim_output=output_dim)\n","    optimizer = torch.optim.Adam(params = GCN_model.parameters(), lr=0.001)\n","    checkpoint = torch.load(\"/content/drive/My Drive/Explainability Methods/\"+str(method)+\" on \" + str(task) + \"/Model/\" + str(method) + \" on \" + str(task) + \" classifier model_\" + str(load_index)+\".pt\")\n","    GCN_model.load_state_dict(checkpoint['model_state_dict'])\n","    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","    epoch = checkpoint['epoch']\n","    loss = checkpoint['loss']\n","    \n","    return GCN_model, optimizer, epoch\n","  \n","\n","  def loss_calculations(self, preds, gtruth):\n","    loss_per_epoch = self.criterion(preds, gtruth)\n","    return loss_per_epoch\n","\n","  def remove_nones(self, sample_grads):\n","    sample_grads2 = []\n","    for item in sample_grads:\n","      Each_Graph = []\n","      for item2 in item:\n","        if item2 != None:\n","          Each_Graph.append(torch.tensor(item2.clone().detach().requires_grad_(True),requires_grad=True))\n","        else:\n","          Each_Graph.append(torch.tensor(0))\n","      sample_grads2.append(Each_Graph)\n","\n","    return sample_grads2\n","\n","  def compute_grad(self, model, graph, with_respect):\n","    post_conv1, post_conv2, out_readout, prediction = model(graph.x, graph.edge_index, graph.batch)\n","    if with_respect == 1 :\n","      loss = self.loss_calculations(prediction, graph.y)\n","      #print(loss)\n","    elif with_respect == 2:\n","      loss = self.loss_calculations(prediction, torch.tensor([0]))\n","      #print(loss)  \n","    elif with_respect == 3:\n","      loss = self.loss_calculations(prediction, torch.tensor([1]))\n","      #print(loss)\n","    return torch.autograd.grad(loss, list(self.GCN_model.parameters()), allow_unused=True)\n","\n","  def compute_sample_grads(self, model, test_dataset, with_respect):\n","\n","    sample_grads = [self.compute_grad(model, graph, with_respect) for graph in test_dataset]\n","    sample_grads = self.remove_nones(sample_grads)\n","    sample_grads = zip(*sample_grads)\n","    sample_grads = [torch.stack(shards) for shards in sample_grads]\n","    return sample_grads\n","\n","  \n","\n","  def compute_square_gradients(self, model, dataset):\n","    per_sample_grads_wrt_graph_label = self.compute_sample_grads(model, dataset, 1)\n","    per_sample_grads_wrt_class_zero = self.compute_sample_grads(model, dataset, 2)\n","    per_sample_grads_wrt_class_one = self.compute_sample_grads(model, dataset, 3)\n","\n","    grads_wrt_graph_label = torch.square(per_sample_grads_wrt_graph_label[0])\n","    #square_grads_wrt_graph_label = torch.square(per_sample_grads_wrt_graph_label[1])\n","    square_grads_wrt_graph_label = grads_wrt_graph_label.detach().tolist()\n","\n","    grads_wrt_class_zero = torch.square(per_sample_grads_wrt_class_zero[0])\n","    #square_grads_wrt_class_zero = torch.square(per_sample_grads_wrt_class_zero[1])\n","    square_grads_wrt_class_zero = grads_wrt_class_zero.detach().tolist()\n","\n","    grads_wrt_class_one = torch.square(per_sample_grads_wrt_class_one[0])\n","    #square_grads_wrt_class_one = torch.square(per_sample_grads_wrt_class_one[1])\n","    square_grads_wrt_class_one = grads_wrt_class_one.detach().tolist()\n","\n","    return square_grads_wrt_graph_label, square_grads_wrt_class_zero, square_grads_wrt_class_one\n","  \n","\n","  def saliency(self, dataset, gradients):\n","    Final= []\n","    for i in range(len(dataset)):\n","      Mid = []\n","      for node in dataset[i].x.detach().numpy():\n","        First = []\n","        for grad_list in gradients[i]:\n","          First.append(np.multiply(node, grad_list))\n","        Mid.append(First)\n","      Final.append(Mid)\n","    \n","    Saliency_Nodes = []\n","    for graph in Final:\n","      Node = []\n","      for node in graph:\n","        Grad = []\n","        for grad in node:\n","          Grad.append(sum(grad))\n","        Node.append(sum(Grad))\n","      #norm = [(float(i)-min(Node))/(max(Node)-min(Node)) for i in Node]\n","      norm = [(float(i))/(max(Node) + 1e-16) for i in Node]\n","      Saliency_Nodes.append(norm)\n","    return Saliency_Nodes\n","  \n","\n","\n","  def is_salient(self, index, score, importance_range):\n","    start, end = importance_range\n","    if start <= score <= end:\n","      return True\n","    else:\n","\n","      return False\n","  \n","\n","  def drop_important_nodes(self, model, graph, importance_range):\n","    square_grads_wrt_graph_label, square_grads_wrt_class_zero, square_grads_wrt_class_one = self.compute_square_gradients(model, graph)\n","    print(len(square_grads_wrt_graph_label))\n","    SA_attribution_scores = self.saliency(graph, square_grads_wrt_graph_label)\n","    occluded_GNNgraph_list = []\n","    \n","    for i in range(len(SA_attribution_scores)):\n","      sample_graph = deepcopy(graph[i])\n","      graph_dict ={}\n","      for j in range(len(sample_graph.x)):\n","        \n","        if self.is_salient(j, (SA_attribution_scores[i][j]), importance_range):\n","          #print(\"before: \", sample_graph.x[j])\n","          sample_graph.x[j][:] = 0\n","          #print(torch.zeros_like(sample_graph.x[j]))\n","          #print(\"manipulated: \",sample_graph.x[j])\n","          graph_dict[j] = True\n","        else:\n","          graph_dict[j] = False\n","      self.importance_dict[i] = graph_dict\n","      occluded_GNNgraph_list.append(sample_graph)\n","\n","\n","    return occluded_GNNgraph_list, SA_attribution_scores\n","\n","\n","\n","dataset = TUDataset(root='data/TUDataset', name='MUTAG')\n","\n","\n","\n","#new_output = SA_GC(graph=[graph], importance_range=(0.5, 1), start_epoch=200, input_dim = len(graph.x[0]), hid_dim = 7, output_dim = 2)\n","new_output = SA_GC(task=\"Graph Classification\", method=\"SA\", graph=[dataset[0]], importance_range=(0.5, 1), start_epoch=200, input_dim = len(dataset[0].x[0]), hid_dim = 7, output_dim = 2)\n","print(new_output.new_graph[-1].x, dataset[-1].x)\n","print(new_output.saliency_maps)\n","print(new_output.importance_dict)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8IHWy2qJVf4M","executionInfo":{"status":"ok","timestamp":1673292506692,"user_tz":-60,"elapsed":4,"user":{"displayName":"Ehsan Mobaraki","userId":"06314958514195310048"}},"outputId":"eeb69caa-cdb7-4d82-8797-d2d3606fb8d0"},"execution_count":49,"outputs":[{"output_type":"stream","name":"stdout","text":["1\n","tensor([[0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 1., 0., 0., 0., 0.],\n","        [0., 0., 1., 0., 0., 0., 0.]]) tensor([[1., 0., 0., 0., 0., 0., 0.],\n","        [1., 0., 0., 0., 0., 0., 0.],\n","        [1., 0., 0., 0., 0., 0., 0.],\n","        [1., 0., 0., 0., 0., 0., 0.],\n","        [0., 1., 0., 0., 0., 0., 0.],\n","        [1., 0., 0., 0., 0., 0., 0.],\n","        [1., 0., 0., 0., 0., 0., 0.],\n","        [1., 0., 0., 0., 0., 0., 0.],\n","        [1., 0., 0., 0., 0., 0., 0.],\n","        [1., 0., 0., 0., 0., 0., 0.],\n","        [1., 0., 0., 0., 0., 0., 0.],\n","        [1., 0., 0., 0., 0., 0., 0.],\n","        [1., 0., 0., 0., 0., 0., 0.],\n","        [0., 1., 0., 0., 0., 0., 0.],\n","        [0., 0., 1., 0., 0., 0., 0.],\n","        [0., 0., 1., 0., 0., 0., 0.]])\n","[[0.9999999999999992, 0.9999999999999992, 0.9999999999999992, 0.9999999999999992, 0.9999999999999992, 0.9999999999999992, 0.9999999999999992, 0.9999999999999992, 0.9999999999999992, 0.9999999999999992, 0.9999999999999992, 0.9999999999999992, 0.9999999999999992, 0.9999999999999992, 0.5865093941244874, 0.2956994337010833, 0.2956994337010833]]\n","{0: {0: True, 1: True, 2: True, 3: True, 4: True, 5: True, 6: True, 7: True, 8: True, 9: True, 10: True, 11: True, 12: True, 13: True, 14: True, 15: False, 16: False}}\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-49-decf5eda2cf6>:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  Each_Graph.append(torch.tensor(item2.clone().detach().requires_grad_(True),requires_grad=True))\n"]}]},{"cell_type":"code","source":["dataset = TUDataset(root='data/TUDataset', name='MUTAG')\n","\n","\n","\n","#new_output = SA_GC(graph=[graph], importance_range=(0.5, 1), start_epoch=200, input_dim = len(graph.x[0]), hid_dim = 7, output_dim = 2)\n","new_output = SA_GC(task=\"Graph Classification\", method=\"SA\", graph=dataset, importance_range=(0.5, 1), start_epoch=200, input_dim = len(dataset[0].x[0]), hid_dim = 7, output_dim = 2)\n","print(new_output.new_graph[-1].x, dataset[-1].x)\n","\n","print(new_output.importance_dict)"],"metadata":{"id":"Ny29FwlariAo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **GuidedBP GC Object Oriented Programming**"],"metadata":{"id":"r65AOawJwyZw"}},{"cell_type":"code","source":["class GuidedBP_GC(object):\n","  def __init__(self, task, method, graph, importance_range, load_index, input_dim, hid_dim, output_dim):\n","\n","    self.GCN_model = self.load_model(task, method, load_index=load_index, input_dim=input_dim, hid_dim=hid_dim, output_dim=output_dim)\n","    self.criterion = torch.nn.CrossEntropyLoss()\n","    self.importance_dict = {}\n","    self.new_graph, self.saliency_maps = self.drop_important_nodes(self.GCN_model, graph, importance_range)\n","    \n","\n","\n","  def load_model(self, task, method, load_index, input_dim, hid_dim, output_dim):\n","\n","    if load_index != 0:\n","      GCN_model, optimizer, load_index = self.loading_config(task, method, load_index, input_dim, hid_dim, output_dim)\n","      return GCN_model\n","    else:\n","      GCN_model = gcn_2l_model.GCN_2Layer_Model(model_level='graph', dim_node=input_dim, dim_hidden=hid_dim, dim_output=output_dim)\n","      return GCN_model\n","\n","\n","  def loading_config(self, task, method, load_index, input_dim, hid_dim, output_dim):\n","    GCN_model = gcn_2l_model.GCN_2Layer_Model(model_level='graph', dim_node=input_dim, dim_hidden=hid_dim, dim_output=output_dim)\n","    optimizer = torch.optim.Adam(params = GCN_model.parameters(), lr=1e-4)\n","    checkpoint = torch.load(\"/content/drive/My Drive/Explainability Methods/\"+str(method)+\" on \" + str(task) + \"/Model/\" + str(method) + \" on \" + str(task) + \" classifier model_\" + str(load_index)+\".pt\")\n","    GCN_model.load_state_dict(checkpoint['model_state_dict'])\n","    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","    epoch = checkpoint['epoch']\n","    loss = checkpoint['loss']\n","    \n","    return GCN_model, optimizer, epoch\n","  \n","\n","  def loss_calculations(self, preds, gtruth):\n","    loss_per_epoch = self.criterion(preds, gtruth)\n","    return loss_per_epoch\n","\n","  def remove_nones(self, sample_grads):\n","    sample_grads2 = []\n","    for item in sample_grads:\n","      Each_Graph = []\n","      for item2 in item:\n","        if item2 != None:\n","          Each_Graph.append(torch.tensor(item2.clone().detach().requires_grad_(True),requires_grad=True))\n","        else:\n","          Each_Graph.append(torch.tensor(0))\n","      sample_grads2.append(Each_Graph)\n","\n","    return sample_grads2\n","\n","  def compute_grad(self, model, graph, with_respect):\n","    post_conv1, post_conv2, out_readout, prediction = model(graph.x, graph.edge_index, graph.batch)\n","    if with_respect == 1 :\n","      loss = self.loss_calculations(prediction, graph.y)\n","      #print(loss)\n","    elif with_respect == 2:\n","      loss = self.loss_calculations(prediction, torch.tensor([0]))\n","      #print(loss)  \n","    elif with_respect == 3:\n","      loss = self.loss_calculations(prediction, torch.tensor([1]))\n","      #print(loss)\n","    return torch.autograd.grad(loss, list(self.GCN_model.parameters()), allow_unused=True)\n","\n","  def compute_sample_grads(self, model, test_dataset, with_respect):\n","\n","    sample_grads = [self.compute_grad(model, graph, with_respect) for graph in test_dataset]\n","    sample_grads = self.remove_nones(sample_grads)\n","    sample_grads = zip(*sample_grads)\n","    sample_grads = [torch.stack(shards) for shards in sample_grads]\n","    return sample_grads\n","\n","  \n","\n","  def compute_guided_gradients(self, your_model, dataset):\n","    per_sample_grads_wrt_graph_label = self.compute_sample_grads(your_model, dataset, 1)\n","    per_sample_grads_wrt_class_zero = self.compute_sample_grads(your_model, dataset, 2)\n","    per_sample_grads_wrt_class_one = self.compute_sample_grads(your_model, dataset, 3)\n","\n","    grads_wrt_graph_label = per_sample_grads_wrt_graph_label[0]\n","    guided_grads_wrt_graph_label = torch.maximum(torch.zeros_like(grads_wrt_graph_label), grads_wrt_graph_label)\n","    guided_grads_wrt_graph_label = guided_grads_wrt_graph_label.detach().tolist()\n","\n","    grads_wrt_class_zero = per_sample_grads_wrt_class_zero[0]\n","    guided_grads_wrt_class_zero = torch.maximum(torch.zeros_like(grads_wrt_class_zero), grads_wrt_class_zero)\n","    guided_grads_wrt_class_zero = guided_grads_wrt_class_zero.detach().tolist()\n","\n","    grads_wrt_class_one = per_sample_grads_wrt_class_one[0]\n","    guided_grads_wrt_class_one = torch.maximum(torch.zeros_like(grads_wrt_class_one), grads_wrt_class_one)\n","    guided_grads_wrt_class_one = guided_grads_wrt_class_one.detach().tolist()\n","\n","    return guided_grads_wrt_graph_label, guided_grads_wrt_class_zero, guided_grads_wrt_class_one\n","  \n","\n","  def saliency(self, dataset, gradients):\n","    Final= []\n","    for i in range(len(dataset)):\n","      Mid = []\n","      for node in dataset[i].x.detach().numpy():\n","        First = []\n","        for grad_list in gradients[i]:\n","          First.append(np.multiply(node, grad_list))\n","        Mid.append(First)\n","      Final.append(Mid)\n","    \n","    Saliency_Nodes = []\n","    for graph in Final:\n","      Node = []\n","      for node in graph:\n","        Grad = []\n","        for grad in node:\n","          Grad.append(sum(grad))\n","        Node.append(sum(Grad))\n","      #norm = [(float(i)-min(Node))/(max(Node)-min(Node)) for i in Node]\n","      norm = [(float(i))/(max(Node) + 1e-16) for i in Node]\n","      Saliency_Nodes.append(norm)\n","    return Saliency_Nodes\n","  \n","\n","\n","  def is_salient(self, index, score, importance_range):\n","    start, end = importance_range\n","    if start <= score <= end:\n","      return True\n","    else:\n","\n","      return False\n","  \n","\n","  def drop_important_nodes(self, model, graph, importance_range):\n","    square_grads_wrt_graph_label, square_grads_wrt_class_zero, square_grads_wrt_class_one = self.compute_guided_gradients(model, graph)\n","    print(len(square_grads_wrt_graph_label))\n","    GuidedBP_attribution_scores = self.saliency(graph, square_grads_wrt_graph_label)\n","    occluded_GNNgraph_list = []\n","    \n","    for i in range(len(GuidedBP_attribution_scores)):\n","      sample_graph = deepcopy(graph[i])\n","      graph_dict ={}\n","      for j in range(len(sample_graph.x)):\n","        \n","        if self.is_salient(j, (GuidedBP_attribution_scores[i][j]), importance_range):\n","          #print(\"before: \", sample_graph.x[j])\n","          sample_graph.x[j][:] = 0\n","          #print(torch.zeros_like(sample_graph.x[j]))\n","          #print(\"manipulated: \",sample_graph.x[j])\n","          graph_dict[j] = True\n","        else:\n","          graph_dict[j] = False\n","      self.importance_dict[i] = graph_dict\n","      occluded_GNNgraph_list.append(sample_graph)\n","\n","\n","    return occluded_GNNgraph_list, GuidedBP_attribution_scores\n","\n","\n","\n","dataset = TUDataset(root='data/TUDataset', name='MUTAG')\n","\n","\n","\n","#new_output = SA_GC(graph=[graph], importance_range=(0.5, 1), start_epoch=200, input_dim = len(graph.x[0]), hid_dim = 7, output_dim = 2)\n","new_output = GuidedBP_GC(task=\"Graph Classification\", method=\"GuidedBP\", graph=[dataset[0]], importance_range=(0.5, 1), load_index=200, input_dim = len(dataset[0].x[0]), hid_dim = 7, output_dim = 2)\n","print(new_output.new_graph[-1].x, dataset[-1].x)\n","print(new_output.saliency_maps)\n","print(new_output.importance_dict)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Q-o3JcF6ujhD","executionInfo":{"status":"ok","timestamp":1673292442772,"user_tz":-60,"elapsed":443,"user":{"displayName":"Ehsan Mobaraki","userId":"06314958514195310048"}},"outputId":"dc9714e2-e944-405d-841a-1ece48cda424"},"execution_count":47,"outputs":[{"output_type":"stream","name":"stdout","text":["1\n","tensor([[0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 1., 0., 0., 0., 0.],\n","        [0., 0., 1., 0., 0., 0., 0.]]) tensor([[1., 0., 0., 0., 0., 0., 0.],\n","        [1., 0., 0., 0., 0., 0., 0.],\n","        [1., 0., 0., 0., 0., 0., 0.],\n","        [1., 0., 0., 0., 0., 0., 0.],\n","        [0., 1., 0., 0., 0., 0., 0.],\n","        [1., 0., 0., 0., 0., 0., 0.],\n","        [1., 0., 0., 0., 0., 0., 0.],\n","        [1., 0., 0., 0., 0., 0., 0.],\n","        [1., 0., 0., 0., 0., 0., 0.],\n","        [1., 0., 0., 0., 0., 0., 0.],\n","        [1., 0., 0., 0., 0., 0., 0.],\n","        [1., 0., 0., 0., 0., 0., 0.],\n","        [1., 0., 0., 0., 0., 0., 0.],\n","        [0., 1., 0., 0., 0., 0., 0.],\n","        [0., 0., 1., 0., 0., 0., 0.],\n","        [0., 0., 1., 0., 0., 0., 0.]])\n","[[0.9999999999999998, 0.9999999999999998, 0.9999999999999998, 0.9999999999999998, 0.9999999999999998, 0.9999999999999998, 0.9999999999999998, 0.9999999999999998, 0.9999999999999998, 0.9999999999999998, 0.9999999999999998, 0.9999999999999998, 0.9999999999999998, 0.9999999999999998, 0.7847041535631566, 0.0, 0.0]]\n","{0: {0: True, 1: True, 2: True, 3: True, 4: True, 5: True, 6: True, 7: True, 8: True, 9: True, 10: True, 11: True, 12: True, 13: True, 14: True, 15: False, 16: False}}\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-47-da95ae09e176>:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  Each_Graph.append(torch.tensor(item2.clone().detach().requires_grad_(True),requires_grad=True))\n"]}]},{"cell_type":"markdown","source":["## **CAM GC Object Oriented Programming**"],"metadata":{"id":"Bz-tYZfdyuiC"}},{"cell_type":"code","source":["class CAM_GC(object):\n","  def __init__(self, task, method, graph, importance_range, load_index, input_dim, hid_dim, output_dim):\n","\n","    self.GCN_model = self.load_model(task, method, load_index=load_index, input_dim=input_dim, hid_dim=hid_dim, output_dim=output_dim)\n","    self.criterion = torch.nn.CrossEntropyLoss()\n","    self.importance_dict = {}\n","    self.new_graph, self.saliency_maps = self.drop_important_nodes(self.GCN_model, graph, importance_range)\n","    \n","\n","\n","  def load_model(self, task, method, load_index, input_dim, hid_dim, output_dim):\n","\n","    if load_index != 0:\n","      GCN_model, optimizer, load_index = self.loading_config(task, method, load_index, input_dim, hid_dim, output_dim)\n","      return GCN_model\n","    else:\n","      GCN_model = gcn_2l_model.GCN_2Layer_Model(model_level='graph', dim_node=input_dim, dim_hidden=hid_dim, dim_output=output_dim)\n","      return GCN_model\n","\n","\n","  def loading_config(self, task, method, load_index, input_dim, hid_dim, output_dim):\n","    GCN_model = gcn_2l_model.GCN_2Layer_Model(model_level='graph', dim_node=input_dim, dim_hidden=hid_dim, dim_output=output_dim)\n","    optimizer = torch.optim.Adam(params=GCN_model.parameters(), lr=0.001)\n","    checkpoint = torch.load(\"/content/drive/My Drive/Explainability Methods/\"+str(method)+\" on \" + str(task) + \"/Model/\" + str(method) + \" on \" + str(task) + \" classifier model_\" + str(load_index)+\".pt\")\n","    GCN_model.load_state_dict(checkpoint['model_state_dict'])\n","    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","    epoch = checkpoint['epoch']\n","    loss = checkpoint['loss']\n","    \n","    return GCN_model, optimizer, epoch\n","  \n","\n","  def loss_calculations(self, preds, gtruth):\n","    loss_per_epoch = self.criterion(preds, gtruth)\n","    return loss_per_epoch\n","\n","  def weights_of_model(self, model):\n","    Dense_Layer_Weights = model.ffn.weight\n","    #Dense_Layer_Biases = model.lin.bias.data\n","\n","    #GConv3_Layer_Weights = model.conv3.lin.weight.data\n","    GConv2_Layer_Weights = model.gconv2.weight\n","    GConv1_Layer_Weights = model.gconv1.weight\n","\n","    Dense_Layer_Weights = Dense_Layer_Weights.detach().tolist()\n","    #GConv3_Layer_Weights = GConv3_Layer_Weights.detach().tolist()\n","    GConv2_Layer_Weights = GConv2_Layer_Weights.detach().tolist()\n","    GConv1_Layer_Weights = GConv1_Layer_Weights.detach().tolist()\n","\n","    #return GConv1_Layer_Weights, GConv2_Layer_Weights, GConv3_Layer_Weights, Dense_Layer_Weights\n","    return GConv1_Layer_Weights, GConv2_Layer_Weights, Dense_Layer_Weights\n","  \n","  \n","  def CAM_FeatureMAP_production(self, model, your_dataset):\n","    index_of_winner_labels = []\n","    FeatureMaps_of_the_Last_Conv = []\n","    output_of_the_GAP = []\n","    Final_predictions = []\n","\n","    model.eval()\n","    for batched_data in your_dataset:\n","      print(batched_data.x)\n","      CAM_Test_One_Before_Last_Conv, CAM_Test_Last_Conv, CAM_Test_GAP, CAM_output = model(batched_data.x, batched_data.edge_index, batched_data.batch)\n","      #index_of_winner_label = CAM_output.argmax(dim=1) \n","      logits = F.log_softmax(CAM_output, dim=1)\n","      prob = F.softmax(logits, dim=1)\n","      index_of_winner_label = prob.argmax(dim=1) \n","\n","      index_of_winner_labels.append(index_of_winner_label.detach().tolist()) \n","      Final_predictions.append(prob.detach().tolist())\n","      output_of_the_GAP.append(CAM_Test_GAP.detach().tolist())\n","      FeatureMaps_of_the_Last_Conv.append(CAM_Test_Last_Conv.detach().tolist())\n","    return FeatureMaps_of_the_Last_Conv, output_of_the_GAP, Final_predictions, index_of_winner_labels\n","\n","\n","\n","  def weight_wrt_class_and_performance(self, index_of_winner_labels, Dense_Layer_Weights):\n","    Weights_of_the_Predicted_Class = []\n","    for i in range(len(index_of_winner_labels)):\n","      Weights_of_the_Predicted_Class.append(Dense_Layer_Weights[index_of_winner_labels[i][0]])\n","    \n","    Weights_of_the_Class_0 = []\n","    for i in range(len(index_of_winner_labels)):\n","      Weights_of_the_Class_0.append(Dense_Layer_Weights[0])\n","\n","    Weights_of_the_Class_1 = []\n","    for i in range(len(index_of_winner_labels)):\n","      Weights_of_the_Class_1.append(Dense_Layer_Weights[1])\n","\n","    return Weights_of_the_Predicted_Class, Weights_of_the_Class_0, Weights_of_the_Class_1\n","\n","\n","\n","  def CAM_Attribution_Scores(self, model, your_dataset):\n","    GConv1_Layer_Weights, GConv2_Layer_Weights, Dense_Layer_Weights = self.weights_of_model(model)\n","    FeatureMaps_of_the_Last_Conv, output_of_the_GAP, Final_predictions, index_of_winner_labels = self.CAM_FeatureMAP_production(model, your_dataset)\n","    Weights_of_the_Predicted_Class, Weights_of_the_Class_0, Weights_of_the_Class_1 = self.weight_wrt_class_and_performance(index_of_winner_labels, Dense_Layer_Weights)\n","    Weights_and_Maps_Multiplication_on_Nodes_of_each_graph = []\n","    Normalized_Attributions = []\n","\n","    for i in range(len(FeatureMaps_of_the_Last_Conv)):\n","      Each_Graph = []\n","      for j in range(len(FeatureMaps_of_the_Last_Conv[i])):\n","        Each_Graph.append(np.multiply(Weights_of_the_Predicted_Class[i], FeatureMaps_of_the_Last_Conv[i][j]))\n","      Weights_and_Maps_Multiplication_on_Nodes_of_each_graph.append(Each_Graph)\n","\n","    for i in range(len(FeatureMaps_of_the_Last_Conv)):\n","      Each_Graph = []\n","      for j in range(len(FeatureMaps_of_the_Last_Conv[i])):\n","        Each_Graph.append(sum(Weights_and_Maps_Multiplication_on_Nodes_of_each_graph[i][j]))\n","      norm = [(float(i))/(max(Each_Graph) + 1e-16) for i in Each_Graph] \n","      Normalized_Attributions.append(norm)\n","    return Normalized_Attributions\n","\n","  def is_salient(self, score, importance_range):\n","    start, end = importance_range\n","    if start <= score <= end:\n","      return True\n","    else:\n","      return False\n","\n","\n","  def drop_important_nodes(self, your_model, your_dataset, importance_range):\n","    CAM_attribution_scores = self.CAM_Attribution_Scores(your_model, your_dataset)\n","    occluded_GNNgraph_list = []\n","    \n","    for i in range(len(CAM_attribution_scores)):\n","      sample_graph = deepcopy(your_dataset[i])\n","      graph_dict ={}\n","      for j in range(len(sample_graph.x)):\n","        \n","        if self.is_salient((CAM_attribution_scores[i][j]), importance_range):\n","          #print(\"before: \", sample_graph.x[j])\n","          sample_graph.x[j][:] = 0\n","          #print(torch.zeros_like(sample_graph.x[j]))\n","          #print(\"manipulated: \",sample_graph.x[j])\n","          graph_dict[j] = True\n","        else:\n","          graph_dict[j] = False\n","      self.importance_dict[i] = graph_dict\n","      occluded_GNNgraph_list.append(sample_graph)\n","    return occluded_GNNgraph_list, CAM_attribution_scores\n","\n","\n","\n","  \n","\n","\n","dataset = TUDataset(root='data/TUDataset', name='MUTAG')\n","\n","\n","\n","#new_output = SA_GC(graph=[graph], importance_range=(0.5, 1), start_epoch=200, input_dim = len(graph.x[0]), hid_dim = 7, output_dim = 2)\n","new_output = CAM_GC(task=\"Graph Classification\", method=\"CAM\", graph=[dataset[0]], importance_range=(0.5, 1), load_index=200, input_dim = len(dataset[0].x[0]), hid_dim = 7, output_dim = 2)\n","print(new_output.new_graph[-1].x, dataset[-1].x)\n","print(new_output.saliency_maps)\n","print(new_output.importance_dict)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2TxEORgqyyea","executionInfo":{"status":"ok","timestamp":1673292316042,"user_tz":-60,"elapsed":243,"user":{"displayName":"Ehsan Mobaraki","userId":"06314958514195310048"}},"outputId":"60010846-e78d-499e-bbe9-fe95e113216e"},"execution_count":44,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[1., 0., 0., 0., 0., 0., 0.],\n","        [1., 0., 0., 0., 0., 0., 0.],\n","        [1., 0., 0., 0., 0., 0., 0.],\n","        [1., 0., 0., 0., 0., 0., 0.],\n","        [1., 0., 0., 0., 0., 0., 0.],\n","        [1., 0., 0., 0., 0., 0., 0.],\n","        [1., 0., 0., 0., 0., 0., 0.],\n","        [1., 0., 0., 0., 0., 0., 0.],\n","        [1., 0., 0., 0., 0., 0., 0.],\n","        [1., 0., 0., 0., 0., 0., 0.],\n","        [1., 0., 0., 0., 0., 0., 0.],\n","        [1., 0., 0., 0., 0., 0., 0.],\n","        [1., 0., 0., 0., 0., 0., 0.],\n","        [1., 0., 0., 0., 0., 0., 0.],\n","        [0., 1., 0., 0., 0., 0., 0.],\n","        [0., 0., 1., 0., 0., 0., 0.],\n","        [0., 0., 1., 0., 0., 0., 0.]])\n","tensor([[0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0., 0.],\n","        [1., 0., 0., 0., 0., 0., 0.],\n","        [1., 0., 0., 0., 0., 0., 0.],\n","        [1., 0., 0., 0., 0., 0., 0.],\n","        [0., 1., 0., 0., 0., 0., 0.],\n","        [0., 0., 1., 0., 0., 0., 0.],\n","        [0., 0., 1., 0., 0., 0., 0.]]) tensor([[1., 0., 0., 0., 0., 0., 0.],\n","        [1., 0., 0., 0., 0., 0., 0.],\n","        [1., 0., 0., 0., 0., 0., 0.],\n","        [1., 0., 0., 0., 0., 0., 0.],\n","        [0., 1., 0., 0., 0., 0., 0.],\n","        [1., 0., 0., 0., 0., 0., 0.],\n","        [1., 0., 0., 0., 0., 0., 0.],\n","        [1., 0., 0., 0., 0., 0., 0.],\n","        [1., 0., 0., 0., 0., 0., 0.],\n","        [1., 0., 0., 0., 0., 0., 0.],\n","        [1., 0., 0., 0., 0., 0., 0.],\n","        [1., 0., 0., 0., 0., 0., 0.],\n","        [1., 0., 0., 0., 0., 0., 0.],\n","        [0., 1., 0., 0., 0., 0., 0.],\n","        [0., 0., 1., 0., 0., 0., 0.],\n","        [0., 0., 1., 0., 0., 0., 0.]])\n","[[0.9141845468895651, 0.9141845468895651, 0.8856599154782825, 0.9879956687868259, 1.0, 0.8967161345006492, 0.8821370855832975, 0.8821370855832975, 0.9876205970119251, 0.9879956479482059, 0.871056110268959, 0.45662583823479325, 0.3459664749199423, 0.4384727992346883, -0.8777768015402347, -0.5366097238163876, -0.5366096958876769]]\n","{0: {0: True, 1: True, 2: True, 3: True, 4: True, 5: True, 6: True, 7: True, 8: True, 9: True, 10: True, 11: False, 12: False, 13: False, 14: False, 15: False, 16: False}}\n"]}]},{"cell_type":"markdown","source":["## **Grad-CAM GC Object Oriented Programming**"],"metadata":{"id":"jCZqmViY1SnV"}},{"cell_type":"code","source":["class Grad_CAM_GC(object):\n","  def __init__(self, task, method, graph, importance_range, load_index, input_dim, hid_dim, output_dim):\n","\n","    self.GCN_model = self.load_model(task, method, load_index=load_index, input_dim=input_dim, hid_dim=hid_dim, output_dim=output_dim)\n","    self.criterion = torch.nn.CrossEntropyLoss()\n","    self.importance_dict = {}\n","    self.new_graph, self.saliency_maps = self.drop_important_nodes(self.GCN_model, graph, importance_range)\n","    \n","\n","\n","  def load_model(self, task, method, load_index, input_dim, hid_dim, output_dim):\n","\n","    if load_index != 0:\n","      GCN_model, optimizer, load_index = self.loading_config(task, method, load_index, input_dim, hid_dim, output_dim)\n","      return GCN_model\n","    else:\n","      GCN_model = gcn_2l_model.GCN_2Layer_Model(model_level='graph', dim_node=input_dim, dim_hidden=hid_dim, dim_output=output_dim)\n","      return GCN_model\n","\n","\n","  def loading_config(self, task, method, load_index, input_dim, hid_dim, output_dim):\n","    GCN_model = gcn_2l_model.GCN_2Layer_Model(model_level='graph', dim_node=input_dim, dim_hidden=hid_dim, dim_output=output_dim)\n","    optimizer = torch.optim.Adam(params=GCN_model.parameters(), lr=0.001)\n","    checkpoint = torch.load(\"/content/drive/My Drive/Explainability Methods/\"+str(method)+\" on \" + str(task) + \"/Model/\" + str(method) + \" on \" + str(task) + \" classifier model_\" + str(load_index)+\".pt\")\n","    GCN_model.load_state_dict(checkpoint['model_state_dict'])\n","    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","    epoch = checkpoint['epoch']\n","    loss = checkpoint['loss']\n","    \n","    return GCN_model, optimizer, epoch\n","  \n","\n","  def loss_calculations(self, preds, gtruth):\n","    loss_per_epoch = self.criterion(preds, gtruth)\n","    return loss_per_epoch\n","\n","  def compute_grad(self, model, graph, with_respect):\n","    Grad_CAM_Test_One_Before_Last_Conv, Grad_CAM_Test_Last_Conv, Grad_CAM_Test_GAP, Grad_CAM_Test_out = model(graph.x, graph.edge_index, graph.batch)\n","    #print(prediction)\n","    if with_respect == 1 :\n","      loss = self.loss_calculations(Grad_CAM_Test_out, graph.y)\n","      #print(loss)\n","      #print(\"done\")\n","    elif with_respect == 2:\n","      loss = self.loss_calculations(Grad_CAM_Test_out, torch.tensor([0]))\n","      #print(loss)  \n","    elif with_respect == 3:\n","      loss = self.loss_calculations(Grad_CAM_Test_out, torch.tensor([1]))\n","      #print(loss)\n","    return torch.autograd.grad(loss, list(model.parameters()),allow_unused=True)\n","\n","  \n","  def remove_nones(self, sample_grads):\n","    #print(type(sample_grads[0]))\n","    sample_grads2 = []\n","    for item in sample_grads:\n","      Each_Graph = []\n","      for item2 in item:\n","        if item2 != None:\n","          Each_Graph.append(torch.tensor(item2.clone().detach().requires_grad_(True),requires_grad=True))\n","        else:\n","          Each_Graph.append(torch.tensor(0))\n","      sample_grads2.append(Each_Graph)\n","      #print(\"separate                         \")\n","          #item2 = torch.tensor([0])\n","    #print(np.shape(sample_grads[0]))\n","    #print(np.shape(sample_grads2[0]))\n","    return sample_grads2\n","\n","\n","  def compute_sample_grads(self, model, test_dataset, with_respect):\n","\n","    sample_grads = [self.compute_grad(model, graph, with_respect) for graph in test_dataset]\n","    #print(np.shape(sample_grads[0]))\n","    #print(sample_grads[20])\n","    sample_grads = self.remove_nones(sample_grads)\n","    sample_grads = zip(*sample_grads)\n","    sample_grads = [torch.stack(shards) for shards in sample_grads]\n","    #sample_grads = [print(shards) for shards in sample_grads]\n","    return sample_grads\n","\n","\n","  def compute_grad_cam_gradients(self, your_model, dataset):\n","    per_sample_grads_wrt_graph_label = self.compute_sample_grads(your_model, dataset, 1)\n","    per_sample_grads_wrt_class_zero = self.compute_sample_grads(your_model, dataset, 2)\n","    per_sample_grads_wrt_class_one = self.compute_sample_grads(your_model, dataset, 3)\n","\n","    grads_wrt_graph_label = per_sample_grads_wrt_graph_label[2].detach().tolist()\n","    grads_wrt_class_zero = per_sample_grads_wrt_class_zero[2].detach().tolist()\n","    grads_wrt_class_one = per_sample_grads_wrt_class_one[2].detach().tolist()\n","\n","\n","    return grads_wrt_graph_label, grads_wrt_class_zero, grads_wrt_class_one\n","\n","  def is_salient(self, score, importance_range):\n","    start, end = importance_range\n","    if start <= score <= end:\n","      return True\n","    else:\n","      return False\n","\n","\n","  def Grad_CAM_FeatureMAP_production(self, your_model, test_loader):\n","    index_of_winner_labels = []\n","    FeatureMaps_of_the_Last_Conv = []\n","    output_of_the_GAP = []\n","    Final_predictions = []\n","\n","    your_model.eval()\n","    for batched_data in test_loader:  \n","      Grad_CAM_Test_One_Before_Last_Conv, Grad_CAM_Test_Last_Conv, Grad_CAM_Test_GAP, Grad_CAM_Test_out = your_model(batched_data.x, batched_data.edge_index, batched_data.batch)\n","      #index_of_winner_label = CAM_output.argmax(dim=1) \n","      logits = F.log_softmax(Grad_CAM_Test_out, dim=1)\n","      prob = F.softmax(logits, dim=1)\n","      index_of_winner_label = prob.argmax(dim=1) \n","\n","      index_of_winner_labels.append(index_of_winner_label.detach().tolist()) \n","      Final_predictions.append(prob.detach().tolist())\n","\n","      FeatureMaps_of_the_Last_Conv.append(Grad_CAM_Test_Last_Conv.detach().tolist())\n","    return FeatureMaps_of_the_Last_Conv, Final_predictions, index_of_winner_labels\n","\n","\n","  def Grad_CAM_Attribution_Scores(self, your_model, your_dataset):\n","    grads_wrt_graph_label, grads_wrt_class_zero, grads_wrt_class_one = self.compute_grad_cam_gradients(your_model, your_dataset)\n","    FeatureMaps_of_the_Last_Conv, Final_predictions, index_of_winner_labels = self.Grad_CAM_FeatureMAP_production(your_model, your_dataset)\n","    Grads_and_Maps_Multiplication_on_Nodes_of_each_graph = []\n","    Normalized_Attributions = []\n","\n","    for i in range(len(FeatureMaps_of_the_Last_Conv)):\n","      Each_Graph = []\n","      for j in range(len(FeatureMaps_of_the_Last_Conv[i])):\n","        Each_Node = []\n","        for k in range(len(grads_wrt_graph_label[i])):\n","          Each_Node.append(np.multiply(grads_wrt_graph_label[i][k], FeatureMaps_of_the_Last_Conv[i][j]))\n","        Each_Graph.append(sum(np.mean(Each_Node, 0)))\n","      Grads_and_Maps_Multiplication_on_Nodes_of_each_graph.append(Each_Graph)\n","    #print(np.shape(Grads_and_Maps_Multiplication_on_Nodes_of_each_graph[0]))\n","    for i in range(len(FeatureMaps_of_the_Last_Conv)):\n","      Each_Graph = []\n","      for j in range(len(FeatureMaps_of_the_Last_Conv[i])):\n","        Each_Graph.append(Grads_and_Maps_Multiplication_on_Nodes_of_each_graph[i][j])\n","      norm = [(float(i))/(max(Each_Graph) + 1e-16) for i in Each_Graph] \n","      Normalized_Attributions.append(norm)\n","    return Normalized_Attributions\n","\n","\n","  def drop_important_nodes(self, your_model, your_dataset, importance_range):\n","    Grad_CAM_attribution_scores = self.Grad_CAM_Attribution_Scores(your_model, your_dataset)\n","    occluded_GNNgraph_list = []\n","    \n","    for i in range(len(Grad_CAM_attribution_scores)):\n","      sample_graph = deepcopy(your_dataset[i])\n","      graph_dict ={}\n","      for j in range(len(sample_graph.x)):\n","        \n","        if self.is_salient((Grad_CAM_attribution_scores[i][j]), importance_range):\n","          #print(\"before: \", sample_graph.x[j])\n","          sample_graph.x[j][:] = 0\n","          #print(torch.zeros_like(sample_graph.x[j]))\n","          #print(\"manipulated: \",sample_graph.x[j])\n","          graph_dict[j] = True\n","        else:\n","          graph_dict[j] = False\n","      self.importance_dict[i] = graph_dict\n","      occluded_GNNgraph_list.append(sample_graph)\n","    return occluded_GNNgraph_list, Grad_CAM_attribution_scores#scipy.special.softmax(Grad_CAM_attribution_scores).tolist()\n","\n","\n","  \n","\n","\n","dataset = TUDataset(root='data/TUDataset', name='MUTAG')\n","\n","\n","\n","#new_output = SA_GC(graph=[graph], importance_range=(0.5, 1), start_epoch=200, input_dim = len(graph.x[0]), hid_dim = 7, output_dim = 2)\n","new_output = Grad_CAM_GC(task=\"Graph Classification\", method=\"Grad-CAM\", graph=[dataset[0]], importance_range=(0.5, 1), load_index=200, input_dim = len(dataset[0].x[0]), hid_dim = 7, output_dim = 2)\n","print(new_output.new_graph[-1].x, dataset[-1].x)\n","print(new_output.saliency_maps)\n","print(new_output.importance_dict)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cNQjMV1Y1TEh","executionInfo":{"status":"ok","timestamp":1673292114329,"user_tz":-60,"elapsed":254,"user":{"displayName":"Ehsan Mobaraki","userId":"06314958514195310048"}},"outputId":"ba831d11-d793-4658-b31b-870871bb3ba5"},"execution_count":41,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[1., 0., 0., 0., 0., 0., 0.],\n","        [1., 0., 0., 0., 0., 0., 0.],\n","        [1., 0., 0., 0., 0., 0., 0.],\n","        [1., 0., 0., 0., 0., 0., 0.],\n","        [1., 0., 0., 0., 0., 0., 0.],\n","        [1., 0., 0., 0., 0., 0., 0.],\n","        [1., 0., 0., 0., 0., 0., 0.],\n","        [1., 0., 0., 0., 0., 0., 0.],\n","        [1., 0., 0., 0., 0., 0., 0.],\n","        [1., 0., 0., 0., 0., 0., 0.],\n","        [1., 0., 0., 0., 0., 0., 0.],\n","        [1., 0., 0., 0., 0., 0., 0.],\n","        [1., 0., 0., 0., 0., 0., 0.],\n","        [1., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0., 0.]]) tensor([[1., 0., 0., 0., 0., 0., 0.],\n","        [1., 0., 0., 0., 0., 0., 0.],\n","        [1., 0., 0., 0., 0., 0., 0.],\n","        [1., 0., 0., 0., 0., 0., 0.],\n","        [0., 1., 0., 0., 0., 0., 0.],\n","        [1., 0., 0., 0., 0., 0., 0.],\n","        [1., 0., 0., 0., 0., 0., 0.],\n","        [1., 0., 0., 0., 0., 0., 0.],\n","        [1., 0., 0., 0., 0., 0., 0.],\n","        [1., 0., 0., 0., 0., 0., 0.],\n","        [1., 0., 0., 0., 0., 0., 0.],\n","        [1., 0., 0., 0., 0., 0., 0.],\n","        [1., 0., 0., 0., 0., 0., 0.],\n","        [0., 1., 0., 0., 0., 0., 0.],\n","        [0., 0., 1., 0., 0., 0., 0.],\n","        [0., 0., 1., 0., 0., 0., 0.]])\n","[[0.02831478931615405, 0.02831478931615405, 0.029280909583929052, 0.026210086131583432, 0.02577668528732541, 0.02897023726308663, 0.029387290786581928, 0.029387290786581928, 0.026098582789116972, 0.026210086131583432, 0.029702439230094514, 0.03961890457243535, 0.05506788372388112, 0.04054020215570043, 0.20454890329733585, 0.176285459814228, 0.176285459814228]]\n","{0: {0: False, 1: False, 2: False, 3: False, 4: False, 5: False, 6: False, 7: False, 8: False, 9: False, 10: False, 11: False, 12: False, 13: False, 14: True, 15: True, 16: True}}\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-41-b30f62c64275>:60: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  Each_Graph.append(torch.tensor(item2.clone().detach().requires_grad_(True),requires_grad=True))\n"]}]},{"cell_type":"markdown","source":["## **LRP GC Object Oriented Programming**"],"metadata":{"id":"wL1qRchbUKoJ"}},{"cell_type":"code","source":["class LRP_GC(object):\n","  def __init__(self, task, method, graph, importance_range, load_index, input_dim, hid_dim, output_dim):\n","\n","    self.GCN_model = self.load_model(task, method, load_index=load_index, input_dim=input_dim, hid_dim=hid_dim, output_dim=output_dim)\n","    self.criterion = torch.nn.CrossEntropyLoss()\n","    self.importance_dict = {}\n","    self.new_graph, self.saliency_maps = self.drop_important_nodes(self.GCN_model, graph, importance_range)\n","    \n","\n","\n","  def load_model(self, task, method, load_index, input_dim, hid_dim, output_dim):\n","\n","    if load_index != 0:\n","      GCN_model, optimizer, load_index = self.loading_config(task, method, load_index, input_dim, hid_dim, output_dim)\n","      return GCN_model\n","    else:\n","      GCN_model = gcn_2l_model.GCN_2Layer_Model(model_level='graph', dim_node=input_dim, dim_hidden=hid_dim, dim_output=output_dim)\n","      return GCN_model\n","\n","\n","  def loading_config(self, task, method, load_index, input_dim, hid_dim, output_dim):\n","    GCN_model = gcn_2l_model.GCN_2Layer_Model(model_level='graph', dim_node=input_dim, dim_hidden=hid_dim, dim_output=output_dim)\n","    optimizer = torch.optim.Adam(params=GCN_model.parameters(), lr=0.001)\n","    checkpoint = torch.load(\"/content/drive/My Drive/Explainability Methods/\"+str(method)+\" on \" + str(task) + \"/Model/\" + str(method) + \" on \" + str(task) + \" classifier model_\" + str(load_index)+\".pt\")\n","    GCN_model.load_state_dict(checkpoint['model_state_dict'])\n","    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","    epoch = checkpoint['epoch']\n","    loss = checkpoint['loss']\n","    \n","    return GCN_model, optimizer, epoch\n","  \n","\n","  def loss_calculations(self, preds, gtruth):\n","    loss_per_epoch = self.criterion(preds, gtruth)\n","    return loss_per_epoch\n","  \n","  def accumulate_weights(self, model_for_you):\n","\n","    gconv1_weight = model_for_you.gconv1.weight.detach().tolist()\n","\n","    gconv2_weight = model_for_you.gconv2.weight.detach().tolist()\n","\n","    ffn_weight = model_for_you.ffn.weight.detach().tolist()\n","\n","    return gconv1_weight, gconv2_weight, ffn_weight\n","\n","  def GCN_Model_LRP(self, your_model, test_dataset):\n","    FFN_activations = []\n","    GConv2_activations = [] \n","    GConv1_activations = [] \n","\n","    your_model.eval()\n","    for batch_of_graphs in test_dataset:\n","      post_conv1_test_activations, post_conv2_test_activations, lrp_gap, GCN_Model_test_activations = your_model(batch_of_graphs.x, batch_of_graphs.edge_index, batch_of_graphs.batch)\n","      GConv1_activations.append(post_conv1_test_activations.detach().tolist())\n","      GConv2_activations.append(post_conv2_test_activations.detach().tolist())\n","      FFN_activations.append(GCN_Model_test_activations.detach().tolist())  \n","      \n","    return GConv1_activations, GConv2_activations, FFN_activations \n","\n","  def my_relu(self, input):\n","    return np.maximum(0, input)\n","\n","  def Compute_R_J(self, epsilon, preceding_layer_activations, exceding_layer_weights, next_layer_R_k, is_it_last):\n","    Denominators = []\n","    for i in range(len(preceding_layer_activations)):\n","      SUM_Denominator = 0\n","      for j in range(len(preceding_layer_activations[i])):\n","        for k in range(len(exceding_layer_weights[j])):\n","          SUM_Denominator = SUM_Denominator + preceding_layer_activations[i][j] * self.my_relu(exceding_layer_weights[j][k])\n","        Denominators.append(epsilon + SUM_Denominator)\n","    #print(\"Denominator is done: \", Denominators)\n","    #print(\"Denominator Length: \", len(Denominators))\n","    \n","    if is_it_last:\n","      Numerators = []\n","      for i in range(len(preceding_layer_activations)):\n","        Node_Numerator = []\n","        for j in range(len(preceding_layer_activations[i])):\n","          SUM_Nominator = 0\n","          for k in range(len(exceding_layer_weights[j])):\n","            SUM_Nominator = SUM_Nominator + preceding_layer_activations[i][j] * self.my_relu(exceding_layer_weights[j][k]) * next_layer_R_k[k]\n","          Node_Numerator.append(SUM_Nominator)\n","        Numerators.append(Node_Numerator)\n","    else:\n","      Numerators = []\n","      for i in range(len(preceding_layer_activations)):\n","        Node_Numerator = []\n","        for j in range(len(preceding_layer_activations[i])):\n","          SUM_Nominator = 0\n","          for k in range(len(exceding_layer_weights[j])):\n","            SUM_Nominator = SUM_Nominator + preceding_layer_activations[i][j] * self.my_relu(exceding_layer_weights[j][k]) * next_layer_R_k[i][k]\n","          Node_Numerator.append(SUM_Nominator)\n","        Numerators.append(Node_Numerator)\n","    \n","    R_Js_Graph = []\n","    for i in range(len(Numerators)):\n","      R_Js_Graph.append([x / Denominators[i] for x in Node_Numerator])\n","\n","    return R_Js_Graph\n","  \n","  def Compute_R_K(self, FFN_activations, wrt):\n","    #print(FFN_activations)\n","    FFN_activations = FFN_activations[0]\n","    if wrt == 2:  #.     Graph Label\n","      last_layer_R_k = [0] * len(FFN_activations)\n","      last_layer_R_k[FFN_activations.index(max(FFN_activations))] = FFN_activations[FFN_activations.index(max(FFN_activations))]\n","      #print(last_layer_R_k)\n","      return last_layer_R_k\n","    elif wrt == 0: #.    Class 0\n","      last_layer_R_k = [0] * len(FFN_activations)\n","      last_layer_R_k[0] = FFN_activations[0]\n","      #print(last_layer_R_k)\n","      return last_layer_R_k\n","    elif wrt == 1: #.    Class 1\n","      last_layer_R_k = [0] * len(FFN_activations)\n","      last_layer_R_k[1] = FFN_activations[1]\n","      #print(last_layer_R_k)\n","      return last_layer_R_k\n","\n","  def One_Graph_LRP(self, epsilon, wrt, graph_sample, weights, activations):\n","    GConv1_weight = weights[0]\n","    GConv1_weight_T = weights[1]\n","\n","    GConv2_weight = weights[2]\n","    GConv2_weight_T = weights[3]\n","\n","    FFN_weight = weights[4]\n","    FFN_weight_T = weights[5]\n","\n","    GConv1_activations = activations[0]\n","    GConv2_activations = activations[1]\n","    FFN_activations = activations[2]\n","\n","    last_layer_R_k = self.Compute_R_K(FFN_activations, wrt)\n","    #print(\"R_K: \",last_layer_R_k)\n","    R_J_hidden2 = self.Compute_R_J(epsilon, GConv2_activations, FFN_weight_T, last_layer_R_k, True)\n","    #print(R_J_hidden2)\n","    #print(len(R_J_hidden2))\n","    R_J_hidden1 = self.Compute_R_J(epsilon, GConv1_activations, GConv2_weight, R_J_hidden2, False)\n","    #print(R_J_hidden1)\n","    #print(len(R_J_hidden1))\n","    R_J_input = self.Compute_R_J(epsilon, graph_sample, GConv1_weight, R_J_hidden1, False)\n","    #print(R_J_input)\n","    #print(len(R_J_input))  \n","    \n","    \n","    return R_J_input\n","\n","\n","  def Normalize_LRPs(self, your_model, dataset):\n","    GConv1_activations, GConv2_activations, FFN_activations  = self.GCN_Model_LRP(your_model, dataset)\n","    GConv1_weight, GConv2_weight, FFN_weight = self.accumulate_weights(your_model)\n","    \n","    GConv1_weight_T = np.array(GConv1_weight).transpose()\n","    GConv1_weight_T = GConv1_weight_T.tolist()\n","\n","    GConv2_weight_T = np.array(GConv2_weight).transpose()\n","    GConv2_weight_T = GConv2_weight_T.tolist()\n","\n","    FFN_weight_T = np.array(FFN_weight).transpose()\n","    FFN_weight_T = FFN_weight_T.tolist()\n","\n","    epsilon = 1e-16\n","    LRPs_Testset = []\n","    for i in range(len(dataset)):\n","      post_conv1_test_activations, post_conv2_test_activations, lrp_gap, GCN_Model_test_activations = your_model(dataset[i].x, dataset[i].edge_index, dataset[i].batch)\n","      wrt = GCN_Model_test_activations.argmax(dim=1).detach().tolist()[0] \n","      #print(wrt)\n","      LRPs_Testset.append(self.One_Graph_LRP(epsilon, wrt, dataset[i].x.detach().tolist(), [GConv1_weight, GConv1_weight_T, GConv2_weight, GConv2_weight_T, FFN_weight, FFN_weight_T], [GConv1_activations[i], GConv2_activations[i], FFN_activations[i]]))\n","    \n","    Normalized_LRPs = []\n","    for i in range(len(LRPs_Testset)):\n","      Each_Graph = []\n","      for j in range(len(LRPs_Testset[i])):\n","        Each_Graph.append(sum(LRPs_Testset[i][j]))\n","      norm = [(float(i))/(max(Each_Graph) + 1e-16) for i in Each_Graph] \n","      Normalized_LRPs.append(norm)\n","    print(LRPs_Testset[0])\n","    print(Normalized_LRPs[0])\n","    return Normalized_LRPs\n","\n","  def is_salient(self, score, importance_range):\n","    start = importance_range[0]\n","    end = importance_range[1]\n","    #print(start, score, end)\n","    if float(start) <= float(score) <= float(end):\n","      return True\n","    else:\n","      return False\n","\n","\n","  def drop_important_nodes(self, your_model, your_dataset, importance_range):\n","    LRP_attribution_scores = self.Normalize_LRPs(your_model, your_dataset)\n","    occluded_GNNgraph_list = []\n","    \n","    for i in range(len(LRP_attribution_scores)):\n","      sample_graph = deepcopy(your_dataset[i])\n","      graph_dict ={}\n","      for j in range(len(sample_graph.x)):\n","        \n","        if self.is_salient((LRP_attribution_scores[i][j]), importance_range):\n","          #print(\"before: \", sample_graph.x[j])\n","          sample_graph.x[j][:] = 0\n","          #print(torch.zeros_like(sample_graph.x[j]))\n","          #print(\"manipulated: \",sample_graph.x[j])\n","          graph_dict[j] = True\n","        else:\n","          graph_dict[j] = False\n","      self.importance_dict[i] = graph_dict\n","      occluded_GNNgraph_list.append(sample_graph)\n","    return occluded_GNNgraph_list, LRP_attribution_scores#scipy.special.softmax(LRP_attribution_scores).tolist()\n","\n","\n","\n","\n","dataset = TUDataset(root='data/TUDataset', name='MUTAG')\n","\n","\n","\n","new_output = LRP_GC(task=\"Graph Classification\", method=\"LRP\", graph=[dataset[0]], importance_range=(0.5, 1), load_index=200, input_dim = len(dataset[0].x[0]), hid_dim = 7, output_dim = 2)\n","print(new_output.new_graph[-1].x, dataset[-1].x)\n","print(new_output.saliency_maps)\n","print(new_output.importance_dict)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hwdsyAATUS4N","executionInfo":{"status":"ok","timestamp":1673291885959,"user_tz":-60,"elapsed":902,"user":{"displayName":"Ehsan Mobaraki","userId":"06314958514195310048"}},"outputId":"3593edbc-d16b-41b7-9f74-609a0856b523"},"execution_count":37,"outputs":[{"output_type":"stream","name":"stdout","text":["[[0.0, 0.0, -0.0627912565780123, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, -0.0627912565780123, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, -0.0627912565780123, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, -0.0627912565780123, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, -0.0627912565780123, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, -0.0627912565780123, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, -0.0627912565780123, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, -0.0627912565780123, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, -0.0627912565780123, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, -0.0627912565780123, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, -0.0627912565780123, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, -0.0627912565780123, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, -0.0627912565780123, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, -0.0627912565780123, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, -0.0627912565780123, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, -0.0627912565780123, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, -0.0627912565780123, 0.0, 0.0, 0.0, 0.0]]\n","[1.0000000000000016, 1.0000000000000016, 1.0000000000000016, 1.0000000000000016, 1.0000000000000016, 1.0000000000000016, 1.0000000000000016, 1.0000000000000016, 1.0000000000000016, 1.0000000000000016, 1.0000000000000016, 1.0000000000000016, 1.0000000000000016, 1.0000000000000016, 1.0000000000000016, 1.0000000000000016, 1.0000000000000016]\n","tensor([[1., 0., 0., 0., 0., 0., 0.],\n","        [1., 0., 0., 0., 0., 0., 0.],\n","        [1., 0., 0., 0., 0., 0., 0.],\n","        [1., 0., 0., 0., 0., 0., 0.],\n","        [1., 0., 0., 0., 0., 0., 0.],\n","        [1., 0., 0., 0., 0., 0., 0.],\n","        [1., 0., 0., 0., 0., 0., 0.],\n","        [1., 0., 0., 0., 0., 0., 0.],\n","        [1., 0., 0., 0., 0., 0., 0.],\n","        [1., 0., 0., 0., 0., 0., 0.],\n","        [1., 0., 0., 0., 0., 0., 0.],\n","        [1., 0., 0., 0., 0., 0., 0.],\n","        [1., 0., 0., 0., 0., 0., 0.],\n","        [1., 0., 0., 0., 0., 0., 0.],\n","        [0., 1., 0., 0., 0., 0., 0.],\n","        [0., 0., 1., 0., 0., 0., 0.],\n","        [0., 0., 1., 0., 0., 0., 0.]]) tensor([[1., 0., 0., 0., 0., 0., 0.],\n","        [1., 0., 0., 0., 0., 0., 0.],\n","        [1., 0., 0., 0., 0., 0., 0.],\n","        [1., 0., 0., 0., 0., 0., 0.],\n","        [0., 1., 0., 0., 0., 0., 0.],\n","        [1., 0., 0., 0., 0., 0., 0.],\n","        [1., 0., 0., 0., 0., 0., 0.],\n","        [1., 0., 0., 0., 0., 0., 0.],\n","        [1., 0., 0., 0., 0., 0., 0.],\n","        [1., 0., 0., 0., 0., 0., 0.],\n","        [1., 0., 0., 0., 0., 0., 0.],\n","        [1., 0., 0., 0., 0., 0., 0.],\n","        [1., 0., 0., 0., 0., 0., 0.],\n","        [0., 1., 0., 0., 0., 0., 0.],\n","        [0., 0., 1., 0., 0., 0., 0.],\n","        [0., 0., 1., 0., 0., 0., 0.]])\n","[[0.0588235294117647, 0.0588235294117647, 0.0588235294117647, 0.0588235294117647, 0.0588235294117647, 0.0588235294117647, 0.0588235294117647, 0.0588235294117647, 0.0588235294117647, 0.0588235294117647, 0.0588235294117647, 0.0588235294117647, 0.0588235294117647, 0.0588235294117647, 0.0588235294117647, 0.0588235294117647, 0.0588235294117647]]\n","{0: {0: False, 1: False, 2: False, 3: False, 4: False, 5: False, 6: False, 7: False, 8: False, 9: False, 10: False, 11: False, 12: False, 13: False, 14: False, 15: False, 16: False}}\n"]}]},{"cell_type":"markdown","source":["## **ExcitationBP GC Object Oriented Programming**"],"metadata":{"id":"bIuYAkOEBgSu"}},{"cell_type":"code","source":["class ExcitationBP_GC(object):\n","  def __init__(self, task, method, graph, importance_range, load_index, input_dim, hid_dim, output_dim):\n","\n","    self.GCN_model = self.load_model(task, method, load_index=load_index, input_dim=input_dim, hid_dim=hid_dim, output_dim=output_dim)\n","    self.criterion = torch.nn.CrossEntropyLoss()\n","    self.importance_dict = {}\n","    self.new_graph, self.saliency_maps = self.drop_important_nodes(self.GCN_model, graph, importance_range)\n","    \n","\n","\n","  def load_model(self, task, method, load_index, input_dim, hid_dim, output_dim):\n","\n","    if load_index != 0:\n","      GCN_model, optimizer, load_index = self.loading_config(task, method, load_index, input_dim, hid_dim, output_dim)\n","      return GCN_model\n","    else:\n","      GCN_model = gcn_2l_model.GCN_2Layer_Model(model_level='graph', dim_node=input_dim, dim_hidden=hid_dim, dim_output=output_dim)\n","      return GCN_model\n","\n","\n","  def loading_config(self, task, method, load_index, input_dim, hid_dim, output_dim):\n","    GCN_model = gcn_2l_model.GCN_2Layer_Model(model_level='graph', dim_node=input_dim, dim_hidden=hid_dim, dim_output=output_dim)\n","    optimizer = torch.optim.Adam(params=GCN_model.parameters(), lr=0.001)\n","    checkpoint = torch.load(\"/content/drive/My Drive/Explainability Methods/\"+str(method)+\" on \" + str(task) + \"/Model/\" + str(method) + \" on \" + str(task) + \" classifier model_\" + str(load_index)+\".pt\")\n","    GCN_model.load_state_dict(checkpoint['model_state_dict'])\n","    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","    epoch = checkpoint['epoch']\n","    loss = checkpoint['loss']\n","    \n","    return GCN_model, optimizer, epoch\n","  \n","\n","  def loss_calculations(self, preds, gtruth):\n","    loss_per_epoch = self.criterion(preds, gtruth)\n","    return loss_per_epoch\n","\n","  def my_relu(self,input):\n","    return np.maximum(0, input)\n","\n","  def accumulate_weights(self, model_for_you):\n","    gconv1_weight = model_for_you.gconv1.weight.detach().tolist()\n","\n","    gconv2_weight = model_for_you.gconv2.weight.detach().tolist()\n","\n","    ffn_weight = model_for_you.ffn.weight.detach().tolist()\n","\n","    return gconv1_weight, gconv2_weight, ffn_weight\n","    \n","  def Division_by_Zero(self, epsilon, act_hat):\n","    for i in range(len(act_hat)):\n","      for j in range(len(act_hat[i])):\n","        if act_hat[i][j] == 0:\n","          act_hat[i][j] = act_hat[i][j] + epsilon\n","        else:\n","          act_hat[i][j] = act_hat[i][j]\n","    return act_hat\n","\n","\n","  def Compute_Pobabilities(self, last_layer, epsilon, preceding_layer_activations, exceding_layer_weights, exceding_layer_prob):\n","\n","\n","    # 1 Weights and Activations\n","    weights_and_activations_Graph = []\n","    for i in range(len(exceding_layer_weights)):\n","      weights_and_activations_Node = []\n","      for j in range(len(preceding_layer_activations)):\n","        weights_and_activations_Node.append(sum(np.multiply(self.my_relu(exceding_layer_weights[i]), preceding_layer_activations[j])))\n","      weights_and_activations_Graph.append(weights_and_activations_Node)\n","    #print(np.shape(weights_and_activations_Graph), weights_and_activations_Graph)\n","    #print(\"1 Multiplication of Weights and Activations: \", np.shape(weights_and_activations_Graph))\n","    weights_and_activations_Graph = self.Division_by_Zero(epsilon, weights_and_activations_Graph)\n","    weights_and_activations_Graph = np.array(weights_and_activations_Graph).transpose()\n","    weights_and_activations_Graph = weights_and_activations_Graph.tolist()\n","    #print(\"mul: \",np.shape(weights_and_activations_Graph), weights_and_activations_Graph)\n","    # 2 Point-Wise division\n","    #print(exceding_layer_prob)\n","    if last_layer:\n","      division_result_Graph = []\n","      for i in range(len(exceding_layer_prob)):\n","        division_result = [float(exceding_layer_prob[i]/ weights_and_activations_Graph[j][i]) for j in range(len(weights_and_activations_Graph))]\n","        division_result_Graph.append(division_result)\n","      #print(\"2 Division Results: \", np.shape(division_result_Graph))\n","      division_result_Graph = np.array(division_result_Graph).transpose()\n","      division_result_Graph = division_result_Graph.tolist()\n","      #print(np.shape(division_result_Graph), division_result_Graph)\n","    else:\n","      division_result_Graph = []\n","      for i in range(len(exceding_layer_prob)):\n","        division_result = [float(exceding_layer_prob[i][j]/ weights_and_activations_Graph[i][j]) for j in range(len(weights_and_activations_Graph[i]))]\n","        division_result_Graph.append(division_result)\n","      #print(\"2 Division Results: \", np.shape(division_result_Graph))\n","      #print(np.shape(division_result_Graph), division_result_Graph)\n","\n","    # 3 Multiplication by Weights\n","    exceding_layer_weights = np.array(exceding_layer_weights).transpose()\n","    exceding_layer_weights = exceding_layer_weights.tolist()\n","\n","    weights_third_step_Graph = []\n","    for i in range(len(division_result_Graph)):\n","      weights_third_step = []\n","      for j in range(len(exceding_layer_weights)):\n","        weights_third_step.append(sum([x*y for x,y in zip(self.my_relu(exceding_layer_weights[j]), division_result_Graph[i])]))\n","      weights_third_step_Graph.append(weights_third_step)\n","    #print(\"3 Third Step: \", np.shape(weights_third_step_Graph))\n","    #print(weights_third_step_Graph)\n","\n","    # 4 Forth Step\n","    final_probability_Graph = []\n","    for i in range(len(preceding_layer_activations)):\n","      final_probability_vector = [preceding_layer_activations[i][j] * weights_third_step_Graph[i][j] for j in range(len(preceding_layer_activations[i]))]\n","      final_probability_Graph.append(final_probability_vector)\n","    #print(np.shape(final_probability_Graph))\n","    #print(final_probability_Graph)\n","\n","\n","    \n","    return final_probability_Graph\n","\n","  def is_salient(self, score, importance_range):\n","    start = importance_range[0]\n","    end = importance_range[1]\n","    #print(start, score, end)\n","    if float(start) <= float(score) <= float(end):\n","      return True\n","    else:\n","      return False\n","\n","  def normalize_labels_to_probabilistics(self, labels):\n","    labels = torch.tensor(labels)\n","    logits = F.log_softmax(labels, dim=1)\n","    prob = F.softmax(logits, dim=1)\n","    prob = prob.detach().tolist()\n","    norms = prob[0]\n","    #print(norms)\n","    return norms\n","\n","  def Compute_P_last_layer(self, FFN_activations, wrt):\n","    prob = self.normalize_labels_to_probabilistics(FFN_activations)\n","    \n","    if wrt == 2:\n","      last_layer_R_k = [0] * len(prob)\n","      last_layer_R_k[prob.index(max(prob))] = max(prob)#1\n","      return last_layer_R_k\n","\n","    elif wrt == 1:\n","      last_layer_R_k = [0] * len(prob)\n","      last_layer_R_k[1] = 1#prob[1]\n","      return last_layer_R_k\n","\n","    elif wrt == 0:\n","      last_layer_R_k = [0] * len(prob)\n","      last_layer_R_k[0] = 1#prob[0]\n","      return last_layer_R_k\n","\n","\n","  def One_Graph_EB(self, wrt, epsilon, input_sample, weights, activations):\n","    GConv1_weight = weights[0]\n","    GConv1_weight_T = weights[1]\n","\n","    GConv2_weight = weights[2]\n","    GConv2_weight_T = weights[3]\n","\n","    FFN_weight = weights[4]\n","    FFN_weight_T = weights[5]\n","\n","    GConv1_activations = activations[0]\n","    GConv2_activations = activations[1]\n","    FFN_activations = activations[2]\n","\n","    #print(FFN_activations)\n","    last_layer_prob = self.Compute_P_last_layer(FFN_activations, wrt)\n","    #print(\"FFN: \", last_layer_prob)\n","\n","    hidden2_probability_vector = self.Compute_Pobabilities(True, epsilon, GConv2_activations, FFN_weight, last_layer_prob)\n","    #print(\"Second GCN Probability: \",hidden2_probability_vector)\n","    #print(\"hid2: \", np.shape(hidden2_probability_vector)) \n","\n","    hidden1_probability_vector = self.Compute_Pobabilities(False, epsilon, GConv1_activations, GConv2_weight_T, hidden2_probability_vector)\n","    #print(\"First GCN Probability: \",hidden2_probability_vector)\n","    #print(\"hid1: \", np.shape(hidden1_probability_vector))\n","    \n","\n","\n","    input_probability_vector = self.Compute_Pobabilities(False, epsilon, input_sample, GConv1_weight_T, hidden1_probability_vector)\n","    #print(\"Input Probability: \",input_probability_vector)\n","    #print(\"input: \", np.shape(input_probability_vector))\n","    \n","    \n","    return input_probability_vector\n","\n","\n","\n","  def transpose_weights(self, GConv1_weight, GConv2_weight, FFN_weight):\n","    GConv1_weight_T = np.array(GConv1_weight).transpose()\n","    GConv1_weight_T = GConv1_weight_T.tolist()\n","\n","    GConv2_weight_T = np.array(GConv2_weight).transpose()\n","    GConv2_weight_T = GConv2_weight_T.tolist()\n","\n","    FFN_weight_T = np.array(FFN_weight).transpose()\n","    FFN_weight_T = FFN_weight_T.tolist()\n","\n","    return GConv1_weight_T, GConv2_weight_T, FFN_weight_T\n","\n","  def Get_ExcitationBPs(self, your_model, dataset):\n","    GConv1_weight, GConv2_weight, FFN_weight = self.accumulate_weights(your_model)\n","    GConv1_weight_T, GConv2_weight_T, FFN_weight_T = self.transpose_weights(GConv1_weight, GConv2_weight, FFN_weight)\n","\n","    GConv1_weight = GConv1_weight.copy()\n","    GConv2_weight = GConv2_weight.copy()\n","    FFN_weight = FFN_weight.copy()\n","\n","    EBs_Testset = []\n","    epsilon = 1e-16\n","    #wrt = 0\n","    for i, graph in enumerate(dataset):\n","      post_conv1_test_activations, post_conv2_test_activations, lrp_gap, GCN_Model_test_activations = your_model(graph.x, graph.edge_index, graph.batch)\n","      wrt = GCN_Model_test_activations.argmax(dim=1).detach().tolist()[0]\n","\n","      GCN_Model_test_activations = GCN_Model_test_activations.detach().tolist()\n","      post_conv2_test_activations = post_conv2_test_activations.detach().tolist()\n","      post_conv1_test_activations = post_conv1_test_activations.detach().tolist()\n","\n","      EBs_Graph = self.One_Graph_EB(wrt, epsilon, dataset[i].x.detach().tolist(), [GConv1_weight, GConv1_weight_T, GConv2_weight, GConv2_weight_T, FFN_weight, FFN_weight_T], [post_conv1_test_activations, post_conv2_test_activations, GCN_Model_test_activations])\n","      graphs = []\n","      for j in range(len(EBs_Graph)):\n","        graphs.append(sum(EBs_Graph[j]))\n","      norm = [(float(i))/(max(graphs) - min(graphs) + 1e-16) for i in graphs] \n","      EBs_Testset.append(norm)\n","\n","    return EBs_Testset\n","\n","\n","\n","  def drop_important_nodes(self, your_model, your_dataset, importance_range):\n","    EBP_attribution_scores = self.Get_ExcitationBPs(your_model, your_dataset)\n","    occluded_GNNgraph_list = []\n","    \n","    for i in range(len(EBP_attribution_scores)):\n","      sample_graph = deepcopy(your_dataset[i])\n","      graph_dict ={}\n","      for j in range(len(sample_graph.x)):\n","        \n","        if self.is_salient((EBP_attribution_scores[i][j]), importance_range):\n","          #print(\"before: \", sample_graph.x[j])\n","          sample_graph.x[j][:] = 0\n","          #print(torch.zeros_like(sample_graph.x[j]))\n","          #print(\"manipulated: \",sample_graph.x[j])\n","          graph_dict[j] = True\n","        else:\n","          graph_dict[j] = False\n","      self.importance_dict[i] = graph_dict\n","      occluded_GNNgraph_list.append(sample_graph)\n","    return occluded_GNNgraph_list, EBP_attribution_scores#scipy.special.softmax(EBP_attribution_scores).tolist()\n","\n","\n","\n","\n","dataset = TUDataset(root='data/TUDataset', name='MUTAG')\n","\n","\n","\n","new_output = ExcitationBP_GC(task=\"Graph Classification\", method=\"ExcitationBP\", graph=[dataset[0]], importance_range=(0.5, 1), load_index=200, input_dim = len(dataset[0].x[0]), hid_dim = 7, output_dim = 2)\n","print(new_output.new_graph[-1].x, dataset[-1].x)\n","print(new_output.saliency_maps)\n","print(new_output.importance_dict)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fB5mI9LDBgsw","executionInfo":{"status":"ok","timestamp":1673291805694,"user_tz":-60,"elapsed":254,"user":{"displayName":"Ehsan Mobaraki","userId":"06314958514195310048"}},"outputId":"796e82ec-d0f2-4b70-d1e1-de041b7af6c6"},"execution_count":36,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0., 0.],\n","        [0., 1., 0., 0., 0., 0., 0.],\n","        [0., 0., 1., 0., 0., 0., 0.],\n","        [0., 0., 1., 0., 0., 0., 0.]]) tensor([[1., 0., 0., 0., 0., 0., 0.],\n","        [1., 0., 0., 0., 0., 0., 0.],\n","        [1., 0., 0., 0., 0., 0., 0.],\n","        [1., 0., 0., 0., 0., 0., 0.],\n","        [0., 1., 0., 0., 0., 0., 0.],\n","        [1., 0., 0., 0., 0., 0., 0.],\n","        [1., 0., 0., 0., 0., 0., 0.],\n","        [1., 0., 0., 0., 0., 0., 0.],\n","        [1., 0., 0., 0., 0., 0., 0.],\n","        [1., 0., 0., 0., 0., 0., 0.],\n","        [1., 0., 0., 0., 0., 0., 0.],\n","        [1., 0., 0., 0., 0., 0., 0.],\n","        [1., 0., 0., 0., 0., 0., 0.],\n","        [0., 1., 0., 0., 0., 0., 0.],\n","        [0., 0., 1., 0., 0., 0., 0.],\n","        [0., 0., 1., 0., 0., 0., 0.]])\n","[[0.0645537826350231, 0.06455378263502312, 0.0645537826350231, 0.0645537826350231, 0.0645537826350231, 0.0645537826350231, 0.0645537826350231, 0.0645537826350231, 0.0645537826350231, 0.0645537826350231, 0.0645537826350231, 0.0645537826350231, 0.06455378263502312, 0.0645537826350231, 0.023748009481275062, 0.036249516814200716, 0.036249516814200716]]\n","{0: {0: True, 1: True, 2: True, 3: True, 4: True, 5: True, 6: True, 7: True, 8: True, 9: True, 10: True, 11: True, 12: True, 13: True, 14: False, 15: False, 16: False}}\n"]}]},{"cell_type":"code","source":["import scipy\n","from scipy import special\n","scipy.special.softmax([1,2])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q7dNwbj78WDK","executionInfo":{"status":"ok","timestamp":1673291663397,"user_tz":-60,"elapsed":3,"user":{"displayName":"Ehsan Mobaraki","userId":"06314958514195310048"}},"outputId":"0e4f3f19-bfe3-406a-dc87-2824f67a1557"},"execution_count":34,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0.26894142, 0.73105858])"]},"metadata":{},"execution_count":34}]}]}