# -*- coding: utf-8 -*-
"""Train_GNNs_by_Optimal_Hyperparameters.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ccdpAS4lop68bKTZngpa8G0T2JBxKG2G
"""

# Install required packages.
import os

#!pip install torch==1.7.0
import torch
os.environ['TORCH'] = torch.__version__
print(torch.__version__)

import argparse
import os
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
import numpy as np
from math import sqrt
import math
import sys

import torch as th
import torch
import torch_geometric
import torch.nn as nn
from torch_geometric.datasets import TUDataset
from torch.nn.parameter import Parameter
from torch_geometric.nn import GCNConv
import torch.nn.functional as F
from torch.nn import Linear, LayerNorm
from sklearn import metrics
from statistics import mean
from scipy.spatial.distance import hamming
import statistics
import pandas
from time import perf_counter
from IPython.core.display import deepcopy
from torch_geometric.nn import MessagePassing
import copy
from torch.nn import ReLU, Sequential
from torch import sigmoid
from itertools import chain
from time import perf_counter
import csv
import pickle




from torch_geometric.nn import GCNConv, global_mean_pool
from torch_geometric.loader import DataLoader
import torch_geometric.nn as gnn

class Train_GNNs_by_Optimal_Hyperparameters:
    def __init__(self, GNN_Model_index, EPOCHS, dataset_index, winit_index, lr, dropout_rate, bias,
                 batch_size, loss_function_index, weight_decay, act_fun, Model_Saving_Cycle, LOSS_Visualization_Cycle,
                 default_path):
        self.default_path = default_path
        os.chdir(self.default_path)

        self.LOSS_Visualization_Cycle = LOSS_Visualization_Cycle
        self.Model_Saving_Cycle = Model_Saving_Cycle

        self.GNN_Models_Name = dict([(1, "GCN_plus_GAP_Model"), (2, "DGCNN_Model"), (3, "DIFFPOOL_Model"),
                                     (4, "GIN_Model")])
        self.Datsets_Name_Dict = {1: "MUTAG", 2: "NCI1", 3: "Graph-SST5", 4: "ENZYMES", 5: "PROTEINS", 6: "IsCyclic"}
        self.Weight_Initializer = {1: "XAVIER", 2: "Kaiming", 3: "Uniform"}
        self.Loss_Function = {1: "CrossEntropy", 2: "MAE", 3: "MSE"}

        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

        self.GNN_Model_Name = self.GNN_Models_Name[GNN_Model_index]
        self.dataset_name = self.Datsets_Name_Dict[dataset_index]
        self.weight_initializer = self.Weight_Initializer[winit_index]
        self.loss_function = self.Loss_Function[loss_function_index]

        self.EPOCHS = EPOCHS
        self.classifier_weight_decay = weight_decay
        self.classifier_lr = lr
        self.dropout_rate = dropout_rate
        self.bias = bias
        self.batch_size = batch_size
        self.act_fun = act_fun

        self.GNN_Model_training_Acc_per_epoch = []
        self.GNN_Model_training_time_per_epoch = []

        self.train_dataloaded, self.test_dataloaded = self.my_dataset()

        sys.path.insert(0, self.default_path+'/Models/Script/Layers')



        ###################################################################################################                    GCN
        if self.GNN_Model_Name == "GCN_plus_GAP_Model":
            import GCN_Layer as gcn_layer
            import GlobalAveragePooling as globalaveragepooling
            import IdenticalPooling as identicalpooling

            sys.path.insert(0, default_path+'/Models/Script')
            import GCN_plus_GAP as gcn_plus_gap_model
            node_feat_size = self.entire_dataset[0].x.size()[-1]
            gcn_input_dim = {'MUTAG': node_feat_size, 'NCI1': node_feat_size, 'ENZYMES': 16, 'Graph-SST5': 64,
                             "PROTEINS": 16, 'IsCyclic': node_feat_size}
            self.GNN_Model = gcn_plus_gap_model.GCN_plus_GAP_Model(model_level="graph", num_classes=self.num_classes,
                                                                   GNN_layers=[node_feat_size,
                                                                               gcn_input_dim[self.dataset_name]],
                                                                   Weight_Initializer=self.weight_initializer,
                                                                   Bias=self.bias, act_fun=act_fun,
                                                                   dropout_rate=dropout_rate).to(self.device)

        ###################################################################################################                  DGCNN
        if self.GNN_Model_Name == "DGCNN_Model":
            import DGCNN_Layer as dgcnn_layer
            import DGCNN_GNN_Layers as dgcnn_gnn_layers
            import DGCNN_SortPooling_Layer as sortpooling_layer
            import DGCNN_MLP as dgcnn_mlp

            sys.path.insert(0, default_path+'/Models/Script')
            import DGCNN as dgcnn_model

            k_dgcnn = {'MUTAG': 17, 'NCI1': 32, 'ENZYMES': 32, 'Graph-SST5': 19, 'PROTEINS': 26, 'IsCyclic': 20}
            node_feat_size = self.entire_dataset[0].x.size()[-1]
            dgcnn_input_dim = {'MUTAG': [32, 32, 32, 32], 'NCI1': [32, 32, 32, 32], 'ENZYMES': [32, 32, 32, 32],
                               'Graph-SST5': [64, 64, 64, 64], 'PROTEINS': [32, 32, 32, 32],
                               'IsCyclic': [32, 32, 32, 32]}

            self.GNN_Model = dgcnn_model.DGCNN_Model(GNN_layers=dgcnn_input_dim[self.dataset_name], Bias=self.bias,
                                                     num_classes=self.num_classes, mlp_act_fun="ReLu",
                                                     dgcnn_act_fun="tanh", Weight_Initializer=self.weight_initializer,
                                                     mlp_dropout_rate=dropout_rate, dgcnn_k=k_dgcnn[self.dataset_name],
                                                     node_feat_size=node_feat_size, hid_channels=[16, 32],
                                                     conv1d_kernels=[2, 5], ffn_layer_size=128,
                                                     strides=[2, 1]).to(self.device)
        ###################################################################################################               DIFFPOOL
        if self.GNN_Model_Name == "DIFFPOOL_Model":
            import Batched_GraphSage_Layer as batched_graphsage_layer
            import Batched_DIFFPOOL_Assignment as batched_diffpool_assignment
            import Batched_DIFFPOOL_Embedding as batched_diffpool_embedding
            import Batched_DIFFPOOL_Layer as batched_diffpool_layer
            sys.path.insert(0, default_path+'/Models/Script')
            import DIFFPOOL as diffpool_model
            node_feat_size = self.entire_dataset[0].x.size()[-1]
            self.GNN_Model = diffpool_model.DIFFPOOL_Model(embedding_input_dim=node_feat_size,
                                                           embedding_num_block_layers=1, embedding_hid_dim=64,
                                                           new_feature_size=64, assignment_input_dim=node_feat_size,
                                                           assignment_num_block_layers=1, assignment_hid_dim=64,
                                                           max_number_of_nodes=256, prediction_hid_layers=[50],
                                                           concat_neighborhood=False, num_classes=self.num_classes,
                                                           Weight_Initializer=self.weight_initializer, Bias=self.bias,
                                                           act_fun=act_fun, dropout_rate=dropout_rate,
                                                           normalize_graphsage=False, aggregation="mean",
                                                           concat_diffpools_outputs=True,num_pooling=1,
                                                           pooling="mean").to(self.device)
        ###################################################################################################                    GIN
        if self.GNN_Model_Name == "GIN_Model":
            import GIN_MLP_Layers as gin_mlp_layers
            sys.path.insert(0, default_path+'/Models/Script')
            import GIN as gin_model
            node_feat_size = self.entire_dataset[0].x.size()[-1]

            gin_input_dim = {'MUTAG': node_feat_size, 'NCI1': node_feat_size, 'ENZYMES': 16, 'Graph-SST5': 64,
                             'PROTEINS': 16, 'IsCyclic': node_feat_size}
            self.GNN_Model = gin_model.GIN_Model(num_mlp_layers=4, Bias=self.bias, num_slp_layers=2,
                                                 mlp_act_fun=act_fun, mlp_input_dim=node_feat_size,
                                                 mlp_hid_dim=gin_input_dim[self.dataset_name], joint_embeddings=False,
                                                 mlp_output_dim=self.num_classes, dropout_rate=dropout_rate,
                                                 Weight_Initializer=self.weight_initializer).to(self.device)

        ###################################################################################################              Optimizer
        self.GNN_Model_Optimizer = torch.optim.Adam(self.GNN_Model.parameters(), lr=self.classifier_lr,
                                                    weight_decay=self.classifier_weight_decay)
        ###################################################################################################          Loss Function
        if self.loss_function == "CrossEntropy":
            self.criterion = torch.nn.CrossEntropyLoss().to(self.device)
        elif self.loss_function == "MAE":
            self.criterion = torch.nn.L1Loss().to(self.device)
        elif self.loss_function == "MSE":
            self.criterion = torch.nn.MSELoss().to(self.device)

    def my_dataset(self):

        if self.dataset_name == "Graph-SST5":
            from dig.xgraph.dataset import SentiGraphDataset
            py_path = '/data/cs.aau.dk/ey33jw/'
            os.chdir(py_path)
            current_directory = os.getcwd()
            self.entire_dataset = SentiGraphDataset(root='./Datasets_for_Explainability_Methods/', name='Graph-SST5')
            py_path = '/data/cs.aau.dk/ey33jw/Explainability_Methods/'
            os.chdir(py_path)
            # current_directory = os.getcwd()
        elif self.dataset_name == "IsCyclic":
            with open("/data/cs.aau.dk/ey33jw/Datasets_for_Explainability_Methods/IsCyclic/iscyclic_graphs.pkl", 'rb') as f:
                self.entire_dataset = pickle.load(f)
        else:
            self.entire_dataset = TUDataset(root='data/TUDataset', name=self.dataset_name)

        labels = []
        for graph in self.entire_dataset:
            labels.append(int(graph.y))
        y_true = np.array(labels)
        unique_classes = np.unique(y_true)
        self.num_classes = len(unique_classes)

        df = pandas.read_csv("/data/cs.aau.dk/ey33jw/Datasets_for_Explainability_Methods/" +
                             "Train and Test Indexes on Graph Classification/Experimental Results/" +
                             "train_test_indexes_" + str(self.dataset_name) + ".csv")

        read_training_list_indexes__ = df['Train Indexes']
        read_test_list_indexes__ = df['Test Indexes']
        read_test_list_indexes__ = read_test_list_indexes__.dropna()
        read_test_list_indexes = []
        read_training_list_indexes = []
        for element in read_test_list_indexes__:
            read_test_list_indexes.append(int(element))
        for element in read_training_list_indexes__:
            read_training_list_indexes.append(int(element))

        self.train_dataset = []
        self.test_dataset = []
        for index in read_training_list_indexes:
            self.train_dataset.append(self.entire_dataset[index])
        for index in read_test_list_indexes:
            self.test_dataset.append(self.entire_dataset[index])
        os.chdir(self.default_path)

        train_dataloader = DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=False)
        test_dataloader = DataLoader(self.test_dataset, batch_size=self.batch_size, shuffle=False)
        return train_dataloader, test_dataloader

    def loss_calculations(self, preds, gtruth):
        loss_per_epoch = self.criterion(preds, gtruth)
        return loss_per_epoch

    def visualize_losses(self, GNN_Model_losses, epoch_history):
        GNN_Model_losses_list = torch.stack(GNN_Model_losses).cpu().detach().numpy()

        fig = plt.figure(figsize=(27, 20))

        ax = plt.subplot2grid((3, 1), (0, 0), colspan=1)
        plt.xlabel('Epochs')
        plt.ylabel('Loss')
        plt.title(" Loss in Epoch: " + str(epoch_history))

        ax.plot(GNN_Model_losses_list, color='r')

        plt.savefig("/data/cs.aau.dk/ey33jw/Train_GNNs_by_Optimal_HyperParameters/Plots/" +
                    self.GNN_Model_Name + "_" + self.dataset_name + '_Loss_vs_Epoch_{:04d}.png'.format(epoch_history))
        # plt.show()
        plt.close()

    def train_step(self):
        GNN_Model_loss_batch = []
        Pred_Labels = []
        Real_Labels = []

        self.GNN_Model.train()
        self.GNN_Model.zero_grad()
        for batch_of_graphs in self.train_dataloaded:
            batch_of_graphs = batch_of_graphs.to(self.device)
            if self.GNN_Model.__class__.__name__ == "GCN_plus_GAP_Model":
                Output_of_Hidden_Layers, pooling_layer_output, ffn_output, soft = self.GNN_Model(batch_of_graphs, None)
                batch_loss = self.loss_calculations(soft, batch_of_graphs.y)
                Pred_Labels.extend(soft.argmax(dim=1).detach().tolist())

            elif self.GNN_Model.__class__.__name__ == "DGCNN_Model":
                final_GNN_layer_output, sortpooled_embedings, output_conv1d_1, maxpooled_output_conv1d_1, output_conv1d_2, to_dense, output_h1, dropout_output_h1, output_h2, softmaxed_h2 = self.GNN_Model(batch_of_graphs, None)
                batch_loss = self.loss_calculations(softmaxed_h2, batch_of_graphs.y)
                Pred_Labels.extend(softmaxed_h2.argmax(dim=1).detach().tolist())

            elif self.GNN_Model.__class__.__name__ == "DIFFPOOL_Model":
                concatination_list_of_poolings, prediction_output_before_soft, prediction_output = self.GNN_Model(batch_of_graphs, None)
                Pred_Labels.extend(prediction_output.argmax(dim=1).detach().tolist())
                batch_loss = self.loss_calculations(prediction_output, batch_of_graphs.y)

            elif self.GNN_Model.__class__.__name__ == "GIN_Model":
                mlps_output_embeds, mlp_outputs_globalSUMpooled, lin1_output, lin1_output_dropouted, lin2_output, lin2_output_softmaxed = self.GNN_Model(batch_of_graphs, None)
                batch_loss = self.loss_calculations(lin2_output_softmaxed, batch_of_graphs.y)
                Pred_Labels.extend(lin2_output_softmaxed.argmax(dim=1).detach().tolist())
            else:
                raise Exception("We cover GCN_plus_GAP, DGCNN, DIFFPOOL, and GIN.")

            Real_Labels.extend(batch_of_graphs.y.detach().tolist())
            GNN_Model_loss_batch.append(batch_loss)

            batch_loss.backward()
            self.GNN_Model_Optimizer.step()

        return torch.mean(torch.tensor(GNN_Model_loss_batch)), metrics.accuracy_score(Real_Labels, Pred_Labels)

    def start_training(self):
        GNN_Model_training_loss_per_epoch = []

        for epoch in range(self.EPOCHS):
            t1 = perf_counter()
            GNN_Model_training_loss, training_acc = self.train_step()
            self.GNN_Model_training_time_per_epoch.append(perf_counter()-t1)
            print(f'Epoch: {epoch+1:03d}, Model Loss: {GNN_Model_training_loss:.4f}, Accuracy: {training_acc:.2f}')

            GNN_Model_training_loss_per_epoch.append(GNN_Model_training_loss)
            self.GNN_Model_training_Acc_per_epoch.append(training_acc)

            if (epoch + 1) % self.LOSS_Visualization_Cycle == 0 and epoch > 0:
                self.visualize_losses(GNN_Model_training_loss_per_epoch, epoch + 1)
            if (epoch + 1) % self.Model_Saving_Cycle == 0 and epoch > 0:
                torch.save({'epoch': epoch+1, 'model_state_dict': self.GNN_Model.state_dict(),
                            'optimizer_state_dict': self.GNN_Model_Optimizer.state_dict(),
                            'loss': GNN_Model_training_loss_per_epoch},
                           "/data/cs.aau.dk/ey33jw/Train_GNNs_by_Optimal_HyperParameters/" + self.GNN_Model_Name + "/" +
                           self.GNN_Model_Name + "_" + self.dataset_name + "_" + str(epoch + 1)+".pt")

    def gnn_model_test(self):

        GNN_Preds = []
        Real_Labels = []

        if self.GNN_Model_Name == "GCN_plus_GAP_Model":
            self.GNN_Model.eval()

            for batch_of_graphs in self.test_dataset:
                batch_of_graphs = batch_of_graphs.to(self.device)
                Output_of_Hidden_Layers_test, pooling_layer_output_test, ffn_output_test, gcn_soft_test = self.GNN_Model(batch_of_graphs, None)
                GNN_Model_test_pred = gcn_soft_test.argmax(dim=1)
                GNN_Preds.extend(GNN_Model_test_pred.tolist())
                Real_Labels.extend(batch_of_graphs.y.tolist())

            return self.Evaluations(np.array(Real_Labels), np.array(GNN_Preds))

        elif self.GNN_Model_Name == "DGCNN_Model":
            self.GNN_Model.eval()

            for batch_of_graphs in self.test_dataset:
                batch_of_graphs = batch_of_graphs.to(self.device)
                final_GNN_layer_output, sortpooled_embedings, output_conv1d_1, maxpooled_output_conv1d_1, output_conv1d_2, to_dense, ffn_1, dropout_ffn_1, ffn_2, dgcnn_softmaxed_ffn_2_test = self.GNN_Model(batch_of_graphs, None)
                GNN_Model_test_pred = dgcnn_softmaxed_ffn_2_test.argmax(dim=1)
                GNN_Preds.extend(GNN_Model_test_pred.tolist())
                Real_Labels.extend(batch_of_graphs.y.tolist())

            return self.Evaluations(np.array(Real_Labels), np.array(GNN_Preds))


        elif self.GNN_Model_Name == "DIFFPOOL_Model":
            self.GNN_Model.eval()

            for batch_of_graphs in self.test_dataset:
                batch_of_graphs = batch_of_graphs.to(self.device)
                concatination_list_of_poolings, prediction_output_without_soft, prediction_output = self.GNN_Model(batch_of_graphs, None)
                GNN_Model_test_pred = prediction_output.argmax(dim=1)
                GNN_Preds.extend(GNN_Model_test_pred.tolist())
                Real_Labels.extend(batch_of_graphs.y.tolist())

            return self.Evaluations(np.array(Real_Labels), np.array(GNN_Preds))

        elif self.GNN_Model_Name == "GIN_Model":
            self.GNN_Model.eval()

            for batch_of_graphs in self.test_dataset:
                batch_of_graphs = batch_of_graphs.to(self.device)
                mlps_output_embeds, mlp_outputs_globalSUMpooled, lin1_output, lin1_output_dropouted, lin2_output, lin2_output_softmaxed = self.GNN_Model(batch_of_graphs, None)
                GNN_Model_test_pred = lin2_output_softmaxed.argmax(dim=1)
                GNN_Preds.extend(GNN_Model_test_pred.tolist())
                Real_Labels.extend(batch_of_graphs.y.tolist())

            return self.Evaluations(np.array(Real_Labels), np.array(GNN_Preds))

    def multiclass_tensor_converter(self, mylabels):
        one_hot_labels = F.one_hot(mylabels, num_classes=self.num_classes).to(torch.float32)
        return one_hot_labels

    def Evaluations(self, y, pred):
        print("pred: ", pred)
        print("y: ", y)

        new_pred = self.multiclass_tensor_converter(torch.from_numpy(pred))

        auc_roc_scores = []
        auc_prc_scores = []
        precision_scores = []
        recall_scores = []
        for i in range(self.num_classes):
            aucroc = metrics.roc_auc_score(y_true=y==i, y_score=new_pred[:, i])
            auc_roc_scores.append(aucroc)

            precision, recall, thresholds = metrics.precision_recall_curve(y_true= y == i,  probas_pred=new_pred[:, i])
            auc_prc_scores.append(metrics.auc(recall, precision))

            precision_scores.append(precision)
            recall_scores.append(recall)

        AUC_ROC = mean(auc_roc_scores)
        print("AUC_ROC: ", AUC_ROC)

        AUC_PRC = mean(auc_prc_scores)
        print("AUC_PRC: ", AUC_PRC)

        precision_scores = metrics.precision_score(y_true=y, y_pred=pred, average=None, zero_division=0)
        print("Precision: ", precision_scores)

        ACC_Quantitative = metrics.accuracy_score(y, pred.round(), normalize=False)
        print("ACC_Quantitative: ", ACC_Quantitative)
        ACC_Probabilistic = metrics.accuracy_score(y, pred.round(), normalize=True)
        print("ACC_Probabilistic: ", ACC_Probabilistic)

        return AUC_ROC, AUC_PRC, ACC_Quantitative, ACC_Probabilistic
    def __call__(self):
        self.start_training()
        self.gnn_model_test()

# Datsets_Name_Dict = {1: "MUTAG", 2: "NCI1", 3: "Graph-SST5", 4: "ENZYMES", 5: "PROTEINS", 6: "IsCyclic"}
# Weight_Initializer = {1:"XAVIER", 2:"Kaiming", 3:"Uniform"}
# Loss Function = {1:"CrossEntropy", 2:"MAE", 3:"MSE"}
train_the_GNN_Model = Train_GNNs_by_Optimal_Hyperparameters(GNN_Model_index=4, EPOCHS=5000, dataset_index=6, bias=True,
                                                            winit_index=3, lr=0.00901886658315339, dropout_rate=0.466207753210106,
                                                            batch_size=64, loss_function_index=1, weight_decay=1e-6,
                                                            act_fun="ReLu", Model_Saving_Cycle=1000,
                                                            LOSS_Visualization_Cycle=500,
                                                            default_path='/data/cs.aau.dk/ey33jw/Explainability_Methods')
train_the_GNN_Model()



# train_the_GNN_Model = Train_GNNs_by_Optimal_Hyperparameters(GNN_Model_index=4, EPOCHS=5000, dataset_index=1, bias=True,
#                                                             winit_index=2, lr=0.0001, dropout_rate=0.3,
#                                                             batch_size=64, loss_function_index=1, weight_decay=1e-6,
#                                                             act_fun="ReLu", Model_Saving_Cycle=1000,
#                                                             LOSS_Visualization_Cycle=500,
#                                                             default_path='/data/cs.aau.dk/ey33jw/Explainability_Methods')